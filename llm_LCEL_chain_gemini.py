# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

print("ğŸ”— Gemini API LCEL Chain êµ¬ì„± ì˜ˆì œ")
print("="*60)

# === 1. ê¸°ë³¸ LCEL Chain ===
print("\nğŸ“ 1. ê¸°ë³¸ LCEL Chain")
print("-" * 30)

# ì»´í¬ë„ŒíŠ¸ ì¤€ë¹„
prompt = ChatPromptTemplate.from_template("{text}")
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")
output_parser = StrOutputParser()

# LCELë¡œ ì²´ì¸ êµ¬ì„±
basic_chain = prompt | llm | output_parser

# ì²´ì¸ ì‹¤í–‰
result = basic_chain.invoke({"text": "ì•ˆë…•í•˜ì„¸ìš”!"})
print(f"ì…ë ¥: ì•ˆë…•í•˜ì„¸ìš”!")
print(f"ì¶œë ¥: {result}")

# === 2. ë³µí•© í”„ë¡¬í”„íŠ¸ Chain ===
print("\nğŸ“ 2. ë³µí•© í”„ë¡¬í”„íŠ¸ Chain")
print("-" * 30)

# ë” ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
complex_prompt = ChatPromptTemplate.from_template(
    """ì£¼ì œ: {topic}
ë‚œì´ë„: {level}
í˜•ì‹: {format}

ìœ„ ì¡°ê±´ì— ë§ì¶° ì„¤ëª…í•´ì£¼ì„¸ìš”."""
)

# ë³µí•© ì²´ì¸ êµ¬ì„±
complex_chain = complex_prompt | llm | output_parser

# ì²´ì¸ ì‹¤í–‰
complex_result = complex_chain.invoke({
    "topic": "ë¨¸ì‹ ëŸ¬ë‹",
    "level": "ì´ˆë³´ì",
    "format": "ë‹¨ê³„ë³„ ì„¤ëª…"
})

print(f"ê²°ê³¼: {complex_result[:200]}...")

# === 3. ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ í¬í•¨ëœ Chain ===
print("\nğŸ“ 3. ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ í¬í•¨ëœ Chain")
print("-" * 30)

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ì‚¬ìš©ì ë©”ì‹œì§€
system_prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ {role} ì „ë¬¸ê°€ì…ë‹ˆë‹¤. {style} ìŠ¤íƒ€ì¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."),
    ("human", "{question}")
])

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì²´ì¸
system_chain = system_prompt | llm | output_parser

# ì‹¤í–‰
system_result = system_chain.invoke({
    "role": "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°",
    "style": "ì¹œì ˆí•˜ê³  ìƒì„¸í•œ",
    "question": "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
})

print(f"ê²°ê³¼: {system_result[:200]}...")

# === 4. ë‹¤ë‹¨ê³„ ì²˜ë¦¬ Chain ===
print("\nğŸ“ 4. ë‹¤ë‹¨ê³„ ì²˜ë¦¬ Chain")
print("-" * 30)

# ì²« ë²ˆì§¸ ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
keyword_prompt = ChatPromptTemplate.from_template(
    "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 3ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ë‹µë³€í•˜ì„¸ìš”: {text}"
)

# ë‘ ë²ˆì§¸ ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ ì„¤ëª…
explanation_prompt = ChatPromptTemplate.from_template(
    "ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì— ëŒ€í•´ ì¢…í•©ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”: {keywords}"
)

# ë‹¤ë‹¨ê³„ ì²´ì¸ êµ¬ì„±
multi_stage_chain = (
    {"text": RunnablePassthrough()} |
    keyword_prompt |
    llm |
    output_parser |
    {"keywords": RunnablePassthrough()} |
    explanation_prompt |
    llm |
    output_parser
)

# ì‹¤í–‰
multi_result = multi_stage_chain.invoke("ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ í˜„ëŒ€ ê¸°ìˆ ì˜ í•µì‹¬ì´ë©°, ë°ì´í„° ê³¼í•™ê³¼ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤.")
print(f"ìµœì¢… ê²°ê³¼: {multi_result[:200]}...")

# === 5. ë³‘ë ¬ ì²˜ë¦¬ Chain ===
print("\nğŸ“ 5. ë³‘ë ¬ ì²˜ë¦¬ Chain")
print("-" * 30)

from langchain_core.runnables import RunnableParallel

# ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ì²´ì¸ë“¤
translation_prompt = ChatPromptTemplate.from_template("ë‹¤ìŒì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”: {text}")
summary_prompt = ChatPromptTemplate.from_template("ë‹¤ìŒì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”: {text}")
sentiment_prompt = ChatPromptTemplate.from_template("ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”: {text}")

# ë³‘ë ¬ ì²´ì¸ êµ¬ì„±
parallel_chain = RunnableParallel({
    "translation": translation_prompt | llm | output_parser,
    "summary": summary_prompt | llm | output_parser,
    "sentiment": sentiment_prompt | llm | output_parser
})

# ë³‘ë ¬ ì‹¤í–‰
parallel_result = parallel_chain.invoke({
    "text": "ì˜¤ëŠ˜ì€ ì •ë§ ì¢‹ì€ ë‚ ì”¨ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•˜ê²Œ ë˜ì–´ ë§¤ìš° ê¸°ëŒ€ë©ë‹ˆë‹¤."
})

print("ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼:")
for key, value in parallel_result.items():
    print(f"- {key}: {value}")

# === 6. ì¡°ê±´ë¶€ ì²˜ë¦¬ Chain ===
print("\nğŸ“ 6. ì¡°ê±´ë¶€ ì²˜ë¦¬ Chain")
print("-" * 30)

# í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ì¡°ê±´ë¶€ ì²˜ë¦¬
def choose_prompt(input_dict):
    text = input_dict["text"]
    if len(text) > 100:
        return ChatPromptTemplate.from_template("ë‹¤ìŒ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”: {text}")
    else:
        return ChatPromptTemplate.from_template("ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í™•ì¥í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”: {text}")

# ì¡°ê±´ë¶€ ì²´ì¸
conditional_chain = (
    RunnableLambda(lambda x: {"text": x["text"], "prompt": choose_prompt(x)}) |
    RunnableLambda(lambda x: x["prompt"].format(text=x["text"])) |
    llm |
    output_parser
)

# ì§§ì€ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
short_text = "AIëŠ” ë¯¸ë˜ë‹¤."
conditional_result1 = conditional_chain.invoke({"text": short_text})
print(f"ì§§ì€ í…ìŠ¤íŠ¸ ê²°ê³¼: {conditional_result1[:150]}...")

# ê¸´ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
long_text = "ì¸ê³µì§€ëŠ¥ì€ í˜„ëŒ€ ì‚¬íšŒì˜ ë§ì€ ë¶„ì•¼ì—ì„œ í˜ì‹ ì„ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹, ìì—°ì–´ì²˜ë¦¬ ë“±ì˜ ê¸°ìˆ ì„ í†µí•´ ìš°ë¦¬ëŠ” ì´ì „ì— ë¶ˆê°€ëŠ¥í–ˆë˜ ë§ì€ ì¼ë“¤ì„ í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤."
conditional_result2 = conditional_chain.invoke({"text": long_text})
print(f"ê¸´ í…ìŠ¤íŠ¸ ê²°ê³¼: {conditional_result2[:150]}...")

# === 7. ì»¤ìŠ¤í…€ í•¨ìˆ˜ì™€ Chain ê²°í•© ===
print("\nğŸ“ 7. ì»¤ìŠ¤í…€ í•¨ìˆ˜ì™€ Chain ê²°í•©")
print("-" * 30)

# ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_text(text):
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    processed = text.strip().upper()
    return f"[ì „ì²˜ë¦¬ë¨] {processed}"

# ì»¤ìŠ¤í…€ í›„ì²˜ë¦¬ í•¨ìˆ˜
def postprocess_result(result):
    """ê²°ê³¼ í›„ì²˜ë¦¬ í•¨ìˆ˜"""
    return f"ğŸ¤– Gemini ë‹µë³€: {result}\nğŸ“Š ë‹µë³€ ê¸¸ì´: {len(result)}ì"

# ì „ì²˜ë¦¬ + LLM + í›„ì²˜ë¦¬ ì²´ì¸
custom_chain = (
    RunnableLambda(preprocess_text) |
    ChatPromptTemplate.from_template("ë‹¤ìŒ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”: {text}") |
    llm |
    output_parser |
    RunnableLambda(postprocess_result)
)

# ì‹¤í–‰
custom_result = custom_chain.invoke("langchainì€ ë¬´ì—‡ì¸ê°€ìš”?")
print(custom_result)

# === 8. ëŒ€í™”í˜• Chain í…ŒìŠ¤íŠ¸ ===
print("\nğŸ“ 8. ëŒ€í™”í˜• Chain í…ŒìŠ¤íŠ¸")
print("-" * 30)
print("ì§ì ‘ ì²´ì¸ì„ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”! (ì¢…ë£Œ: 'quit')")

# ë‹¤ì–‘í•œ ì²´ì¸ ì˜µì…˜
chain_options = {
    "1": {"name": "ê¸°ë³¸ ì§ˆë¬¸ë‹µë³€", "chain": basic_chain},
    "2": {"name": "ì „ë¬¸ê°€ ë‹µë³€", "chain": system_chain},
    "3": {"name": "ë³‘ë ¬ ë¶„ì„", "chain": parallel_chain}
}

while True:
    print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì²´ì¸:")
    for key, value in chain_options.items():
        print(f"{key}. {value['name']}")
    
    choice = input("\nì²´ì¸ì„ ì„ íƒí•˜ì„¸ìš” (1-3, ë˜ëŠ” 'quit'): ").strip()
    
    if choice.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
        break
    
    if choice not in chain_options:
        print("ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        continue
    
    user_input = input("ì…ë ¥ í…ìŠ¤íŠ¸: ").strip()
    if not user_input:
        continue
    
    try:
        selected_chain = chain_options[choice]["chain"]
        
        if choice == "2":  # ì „ë¬¸ê°€ ë‹µë³€ ì²´ì¸
            result = selected_chain.invoke({
                "role": "AI ì „ë¬¸ê°€",
                "style": "ì¹œê·¼í•œ",
                "question": user_input
            })
        else:  # ê¸°ë³¸ ì²´ì¸ë“¤
            result = selected_chain.invoke({"text": user_input})
        
        print(f"\nâœ… ê²°ê³¼:")
        if isinstance(result, dict):
            for key, value in result.items():
                print(f"- {key}: {value}")
        else:
            print(result)
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")

print("\nğŸ‰ LCEL Chain ì˜ˆì œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")