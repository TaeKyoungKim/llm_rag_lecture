"""
SimpleRAG with LangGraph
FAISSSearchEngineì„ ì„í¬íŠ¸í•˜ì—¬ LangGraphë¡œ êµ¬í˜„í•œ RAG ì‹œìŠ¤í…œ
"""

import os
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional, TypedDict, Annotated
import numpy as np

# LangGraph ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import Document, HumanMessage, SystemMessage
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# FAISS ê²€ìƒ‰ ì—”ì§„ ì„í¬íŠ¸
from faiss_search_engine import FAISSSearchEngine

# ê·¸ë˜í”„ ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches
    from matplotlib.patches import FancyBboxPatch, ConnectionPatch
    import networkx as nx
    GRAPH_AVAILABLE = True
except ImportError:
    GRAPH_AVAILABLE = False
    print("âš ï¸ ê·¸ë˜í”„ ì‹œê°í™”ë¥¼ ìœ„í•´ matplotlibê³¼ networkxë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: uv add matplotlib networkx")

from dotenv import load_dotenv
load_dotenv()

# ìƒíƒœ ì •ì˜
class RAGState(TypedDict):
    """RAG ì‹œìŠ¤í…œì˜ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤"""
    query: str
    search_results: List[Tuple[Document, float]]
    context: str
    answer: str
    sources: List[Dict[str, Any]]
    error: Optional[str]
    metadata: Dict[str, Any]

class SimpleRAGWithLangGraph:
    """
    LangGraphë¥¼ ì‚¬ìš©í•œ SimpleRAG ì‹œìŠ¤í…œ
    FAISSSearchEngineì„ ì„í¬íŠ¸í•˜ì—¬ ê²€ìƒ‰ê³¼ ë‹µë³€ ìƒì„±ì„ ë¶„ë¦¬ëœ ë…¸ë“œë¡œ êµ¬ì„±
    """
    
    def __init__(self, embedding_type: str = "huggingface", prompt_style: str = "simple"):
        """
        SimpleRAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        Args:
            embedding_type: "huggingface" ë˜ëŠ” "gemini"
            prompt_style: "simple", "detailed", "academic" ì¤‘ ì„ íƒ
        """
        print("ğŸ¤– SimpleRAG with LangGraph ì´ˆê¸°í™”")
        
        # ì„¤ì • ì €ì¥
        self.embedding_type = embedding_type
        self.prompt_style = prompt_style
        
        # FAISS ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        self.search_engine = FAISSSearchEngine(embedding_type=embedding_type)
        
        # LLM ì´ˆê¸°í™”
        self._initialize_llm()
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”
        self._initialize_prompts()
        
        # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        self.workflow = self._create_workflow()
        
        print("   âœ… SimpleRAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_llm(self):
        """LLM ëª¨ë¸ ì´ˆê¸°í™”"""
        print("   ğŸ”„ LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        try:
            # Gemini API í‚¤ í™•ì¸
            api_key = os.getenv("GOOGLE_API_KEY")
            if not api_key:
                raise Exception("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            print("   ğŸ”‘ Gemini API í‚¤ í™•ì¸ë¨")
            
            # Gemini 2.5 Flash ëª¨ë¸ ì´ˆê¸°í™”
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-2.0-flash-exp",
                google_api_key=api_key,
                temperature=0.7,
                max_output_tokens=2048,
                convert_system_message_to_human=True
            )
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            test_response = self.llm.invoke("ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.")
            if test_response and test_response.content:
                print("   âœ… Gemini 2.5 Flash LLM ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            else:
                raise Exception("LLM í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                
        except Exception as e:
            error_msg = str(e)
            print(f"   âŒ LLM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {error_msg}")
            
            # ì˜¤ë¥˜ ìœ í˜•ë³„ ì•ˆë‚´ ë©”ì‹œì§€
            if "invalid_grant" in error_msg or "Bad Request" in error_msg:
                print("   ğŸ’¡ API í‚¤ ì¸ì¦ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
                print("      - GOOGLE_API_KEYê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
                print("      - API í‚¤ê°€ Gemini APIì— ëŒ€í•œ ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸")
                print("      - API í‚¤ê°€ ë§Œë£Œë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸")
            elif "timeout" in error_msg.lower():
                print("   ğŸ’¡ ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒì…ë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            elif "quota" in error_msg.lower():
                print("   ğŸ’¡ API í• ë‹¹ëŸ‰ ì´ˆê³¼ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
            else:
                print("   ğŸ’¡ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ì…ë‹ˆë‹¤. API í‚¤ì™€ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
            raise
    
    def _initialize_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”"""
        print(f"   ğŸ”„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™” ì¤‘... (ìŠ¤íƒ€ì¼: {self.prompt_style})")
        
        # í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ë³„ í…œí”Œë¦¿ ì •ì˜
        prompt_templates = {
            "simple": ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ê¸°ìˆ ì  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ë‚´ìš©ë§Œ í¬í•¨í•˜ì„¸ìš”
3. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
4. ë¶ˆí•„ìš”í•œ í˜•ì‹ì  ë¬¸êµ¬ëŠ” ìƒëµí•˜ì„¸ìš”"""),
                ("human", """ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€í•´ì£¼ì„¸ìš”.""")
            ]),
            
            "detailed": ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ê¸°ìˆ ì  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
3. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”
4. í•„ìš”ì‹œ ì˜ˆì‹œë‚˜ êµ¬ì²´ì ì¸ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”
5. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
6. ë‹µë³€ì˜ ì‹œì‘ì— "ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."ë¼ëŠ” ë¬¸êµ¬ë¥¼ í¬í•¨í•˜ì„¸ìš”
7. ë‹µë³€ì˜ ëì— "ì´ìƒì´ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ë‚´ìš©ì…ë‹ˆë‹¤."ë¼ëŠ” ë¬¸êµ¬ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”
8. ë§Œì•½ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
9. ë‹µë³€ì€ êµ¬ì¡°í™”í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš” (ì˜ˆ: ì£¼ìš” ê°œë…, íŠ¹ì§•, í™œìš©ë²• ë“±)"""),
                ("human", """ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.""")
            ]),
            
            "academic": ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ê¸ˆìœµê³µí•™ ë° ê¸°ìˆ ì  ë¶„ì„ ë¶„ì•¼ì˜ í•™ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìˆ ì ì´ê³  ì²´ê³„ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìˆ ì  ê´€ì ì—ì„œ ë‹µë³€í•˜ì„¸ìš”
2. ê°œë…ì  ì •ì˜, ìˆ˜í•™ì  ì›ë¦¬, ì‹¤ë¬´ ì ìš© ìˆœì„œë¡œ êµ¬ì„±í•˜ì„¸ìš”
3. ì „ë¬¸ ìš©ì–´ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ë˜, ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”
4. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
5. ë‹µë³€ì˜ ì‹œì‘ì— "í•™ìˆ ì  ê´€ì ì—ì„œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."ë¼ëŠ” ë¬¸êµ¬ë¥¼ í¬í•¨í•˜ì„¸ìš”
6. ë‹µë³€ì˜ ëì— "ì´ìƒì´ í•™ìˆ ì  ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤."ë¼ëŠ” ë¬¸êµ¬ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”"""),
                ("human", """ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ í•™ìˆ ì  ê´€ì ì—ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

í•™ìˆ ì ì´ê³  ì²´ê³„ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.""")
            ])
        }
        
        # ì„ íƒëœ ìŠ¤íƒ€ì¼ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
        if self.prompt_style in prompt_templates:
            self.prompt_template = prompt_templates[self.prompt_style]
        else:
            print(f"   âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ '{self.prompt_style}'. ê¸°ë³¸ ìŠ¤íƒ€ì¼(simple)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.prompt_template = prompt_templates["simple"]
        
        # LLM ì²´ì¸ ìƒì„±
        self.llm_chain = self.prompt_template | self.llm | StrOutputParser()
        
        print(f"   âœ… ChatPromptTemplate ì´ˆê¸°í™” ì™„ë£Œ (ìŠ¤íƒ€ì¼: {self.prompt_style})")
    
    def _create_workflow(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        print("   ğŸ”„ LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì¤‘...")
        
        # ìƒíƒœ ê·¸ë˜í”„ ìƒì„±
        workflow = StateGraph(RAGState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("search", self._search_node)
        workflow.add_node("generate_answer", self._generate_answer_node)
        workflow.add_node("handle_error", self._handle_error_node)
        
        # ì—£ì§€ ì„¤ì •
        workflow.set_entry_point("search")
        workflow.add_edge("search", "generate_answer")
        workflow.add_edge("generate_answer", END)
        workflow.add_edge("handle_error", END)
        
        # ì¡°ê±´ë¶€ ì—£ì§€ ì„¤ì •
        workflow.add_conditional_edges(
            "search",
            self._should_generate_answer,
            {
                "continue": "generate_answer",
                "error": "handle_error"
            }
        )
        
        # ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼
        compiled_workflow = workflow.compile(checkpointer=MemorySaver())
        
        print("   âœ… LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì™„ë£Œ")
        return compiled_workflow
    
    def visualize_workflow(self, save_path: str = "simple_rag_workflow.png"):
        """ì›Œí¬í”Œë¡œìš° ì‹œê°í™”"""
        if not GRAPH_AVAILABLE:
            print("âŒ ê·¸ë˜í”„ ì‹œê°í™”ë¥¼ ìœ„í•´ matplotlibê³¼ networkxë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
            print("   uv add matplotlib networkx")
            return
        
        try:
            print("ğŸ¨ ì›Œí¬í”Œë¡œìš° ì‹œê°í™” ìƒì„± ì¤‘...")
            
            # ê·¸ë˜í”„ ìƒì„±
            G = nx.DiGraph()
            
            # ë…¸ë“œ ì¶”ê°€
            nodes = [
                ("ì…ë ¥ ì¿¼ë¦¬", {"pos": (0, 2), "color": "#E3F2FD", "type": "input"}),
                ("ê²€ìƒ‰ ë…¸ë“œ", {"pos": (2, 2), "color": "#FFF3E0", "type": "process"}),
                ("ì¡°ê±´ë¶€ ë¶„ê¸°", {"pos": (4, 2), "color": "#F3E5F5", "type": "decision"}),
                ("ë‹µë³€ ìƒì„± ë…¸ë“œ", {"pos": (6, 3), "color": "#E8F5E8", "type": "process"}),
                ("ì˜¤ë¥˜ ì²˜ë¦¬ ë…¸ë“œ", {"pos": (6, 1), "color": "#FFEBEE", "type": "error"}),
                ("ìµœì¢… ë‹µë³€", {"pos": (8, 2), "color": "#E0F2F1", "type": "output"})
            ]
            
            for node, attrs in nodes:
                G.add_node(node, **attrs)
            
            # ì—£ì§€ ì¶”ê°€
            edges = [
                ("ì…ë ¥ ì¿¼ë¦¬", "ê²€ìƒ‰ ë…¸ë“œ", "ì¿¼ë¦¬ ì „ë‹¬"),
                ("ê²€ìƒ‰ ë…¸ë“œ", "ì¡°ê±´ë¶€ ë¶„ê¸°", "ê²€ìƒ‰ ê²°ê³¼"),
                ("ì¡°ê±´ë¶€ ë¶„ê¸°", "ë‹µë³€ ìƒì„± ë…¸ë“œ", "ì„±ê³µ"),
                ("ì¡°ê±´ë¶€ ë¶„ê¸°", "ì˜¤ë¥˜ ì²˜ë¦¬ ë…¸ë“œ", "ì‹¤íŒ¨"),
                ("ë‹µë³€ ìƒì„± ë…¸ë“œ", "ìµœì¢… ë‹µë³€", "ìƒì„±ëœ ë‹µë³€"),
                ("ì˜¤ë¥˜ ì²˜ë¦¬ ë…¸ë“œ", "ìµœì¢… ë‹µë³€", "ì˜¤ë¥˜ ë©”ì‹œì§€")
            ]
            
            for edge in edges:
                G.add_edge(edge[0], edge[1], label=edge[2])
            
            # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
            plt.figure(figsize=(14, 8))
            ax = plt.gca()
            
            # ë…¸ë“œ ìœ„ì¹˜
            pos = nx.get_node_attributes(G, 'pos')
            
            # ë…¸ë“œ ê·¸ë¦¬ê¸°
            for node, (x, y) in pos.items():
                node_attrs = G.nodes[node]
                color = node_attrs['color']
                node_type = node_attrs['type']
                
                # ë…¸ë“œ ëª¨ì–‘ ê²°ì •
                if node_type == "input":
                    shape = "s"  # ì‚¬ê°í˜•
                    size = 3000
                elif node_type == "output":
                    shape = "s"  # ì‚¬ê°í˜•
                    size = 3000
                elif node_type == "decision":
                    shape = "d"  # ë‹¤ì´ì•„ëª¬ë“œ
                    size = 4000
                elif node_type == "error":
                    shape = "o"  # ì›
                    size = 2500
                else:
                    shape = "o"  # ì›
                    size = 3000
                
                nx.draw_networkx_nodes(G, pos, nodelist=[node], 
                                     node_color=color, node_size=size, 
                                     node_shape=shape, ax=ax)
            
            # ì—£ì§€ ê·¸ë¦¬ê¸°
            nx.draw_networkx_edges(G, pos, edge_color='gray', 
                                 arrows=True, arrowsize=20, 
                                 arrowstyle='->', ax=ax)
            
            # ë…¸ë“œ ë¼ë²¨
            nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')
            
            # ì—£ì§€ ë¼ë²¨
            edge_labels = nx.get_edge_attributes(G, 'label')
            nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=8)
            
            # ì œëª© ë° ì„¤ì •
            plt.title("SimpleRAG LangGraph ì›Œí¬í”Œë¡œìš°", fontsize=16, fontweight='bold', pad=20)
            plt.axis('off')
            
            # ë²”ë¡€ ì¶”ê°€
            legend_elements = [
                plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='#E3F2FD', 
                          markersize=15, label='ì…ë ¥/ì¶œë ¥'),
                plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#FFF3E0', 
                          markersize=15, label='ì²˜ë¦¬ ë…¸ë“œ'),
                plt.Line2D([0], [0], marker='d', color='w', markerfacecolor='#F3E5F5', 
                          markersize=15, label='ì¡°ê±´ë¶€ ë¶„ê¸°'),
                plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#FFEBEE', 
                          markersize=15, label='ì˜¤ë¥˜ ì²˜ë¦¬')
            ]
            plt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(0, 1))
            
            # ì‹œìŠ¤í…œ ì •ë³´ í…ìŠ¤íŠ¸ ì¶”ê°€
            info_text = f"""
ì‹œìŠ¤í…œ ì •ë³´:
â€¢ ì„ë² ë”© ëª¨ë¸: {self.embedding_type}
â€¢ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {self.prompt_style}
â€¢ ì›Œí¬í”Œë¡œìš° ì—”ì§„: LangGraph
â€¢ ë…¸ë“œ êµ¬ì„±: search â†’ generate_answer
            """
            plt.figtext(0.02, 0.02, info_text, fontsize=9, 
                       bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
            
            # ì €ì¥
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.show()
            
            print(f"   âœ… ì›Œí¬í”Œë¡œìš° ì‹œê°í™” ì €ì¥: {save_path}")
            
        except Exception as e:
            print(f"   âŒ ì›Œí¬í”Œë¡œìš° ì‹œê°í™” ì‹¤íŒ¨: {str(e)}")
    
    def _search_node(self, state: RAGState) -> RAGState:
        """ê²€ìƒ‰ ë…¸ë“œ - FAISS ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            print(f"ğŸ” ê²€ìƒ‰ ë…¸ë“œ ì‹¤í–‰: '{state['query']}'")
            
            # FAISS ê²€ìƒ‰ ìˆ˜í–‰
            search_results = self.search_engine.search(state['query'], k=5)
            
            if search_results:
                # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                context_parts = []
                sources = []
                
                for i, (doc, score) in enumerate(search_results, 1):
                    context_parts.append(f"[ë¬¸ì„œ {i}] {doc.page_content}")
                    sources.append({
                        'page': doc.metadata.get('page', 'N/A'),
                        'chunk_id': doc.metadata.get('chunk_id', 'N/A'),
                        'score': float(score),
                        'content_preview': doc.page_content[:200] + "..."
                    })
                
                context = "\n\n".join(context_parts)
                
                return {
                    **state,
                    'search_results': search_results,
                    'context': context,
                    'sources': sources,
                    'error': None
                }
            else:
                return {
                    **state,
                    'search_results': [],
                    'context': "",
                    'sources': [],
                    'error': "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
                
        except Exception as e:
            return {
                **state,
                'error': f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }
    
    def _generate_answer_node(self, state: RAGState) -> RAGState:
        """ë‹µë³€ ìƒì„± ë…¸ë“œ - LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        try:
            print("ğŸ¤– ë‹µë³€ ìƒì„± ë…¸ë“œ ì‹¤í–‰")
            
            if not state['context']:
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
                response = self.llm_chain.invoke({
                    "context": state['context'],
                    "question": state['query']
                })
                
                answer = str(response)
            
            return {
                **state,
                'answer': answer,
                'metadata': {
                    'embedding_type': self.embedding_type,
                    'llm_model': 'gemini-2.5-flash',
                    'prompt_style': self.prompt_style,
                    'timestamp': datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            return {
                **state,
                'error': f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                'answer': "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }
    
    def _handle_error_node(self, state: RAGState) -> RAGState:
        """ì˜¤ë¥˜ ì²˜ë¦¬ ë…¸ë“œ"""
        print(f"âŒ ì˜¤ë¥˜ ì²˜ë¦¬ ë…¸ë“œ ì‹¤í–‰: {state.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        
        return {
            **state,
            'answer': f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {state.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}",
            'context': "",
            'sources': []
        }
    
    def _should_generate_answer(self, state: RAGState) -> str:
        """ê²€ìƒ‰ í›„ ë‹µë³€ ìƒì„± ì—¬ë¶€ ê²°ì •"""
        if state.get('error'):
            return "error"
        return "continue"
    
    def load_index(self) -> bool:
        """FAISS ì¸ë±ìŠ¤ ë¡œë“œ"""
        return self.search_engine.load_index()
    
    def process_query(self, query: str) -> Dict[str, Any]:
        """
        ì¿¼ë¦¬ ì²˜ë¦¬ - LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            print(f"\nğŸ¤– SimpleRAG ì²˜ë¦¬ ì‹œì‘: '{query}'")
            print("=" * 60)
            
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = RAGState(
                query=query,
                search_results=[],
                context="",
                answer="",
                sources=[],
                error=None,
                metadata={}
            )
            
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            result = self.workflow.invoke(initial_state)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ¤– ë‹µë³€:")
            print("-" * 40)
            print(result['answer'])
            
            if result['sources']:
                print(f"\nğŸ“„ ì°¸ê³  ë¬¸ì„œ ({len(result['sources'])}ê°œ):")
                for i, source in enumerate(result['sources'], 1):
                    print(f"   {i}. í˜ì´ì§€ {source['page']} (ìœ ì‚¬ë„: {source['score']:.4f})")
            
            return result
            
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {
                'query': query,
                'answer': f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'context': "",
                'sources': [],
                'error': str(e)
            }
    
    def save_results(self, result: Dict[str, Any]):
        """ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"simple_rag_results_{timestamp}.json"
        
        try:
            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            save_result = {
                'query': result.get('query', ''),
                'answer': result.get('answer', ''),
                'embedding_type': result.get('metadata', {}).get('embedding_type', ''),
                'llm_model': result.get('metadata', {}).get('llm_model', ''),
                'prompt_style': result.get('metadata', {}).get('prompt_style', ''),
                'timestamp': result.get('metadata', {}).get('timestamp', ''),
                'sources': result.get('sources', []),
                'context_preview': result.get('context', '')[:1000] + "..." if result.get('context') else ""
            }
            
            output_path = Path("DocumentsLoader/simple_rag_results") / filename
            output_path.parent.mkdir(exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(save_result, f, ensure_ascii=False, indent=2)
            
            print(f"   âœ… ê²°ê³¼ ì €ì¥: {output_path}")
            
        except Exception as e:
            print(f"   âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def interactive_mode(self):
        """ëŒ€í™”í˜• ëª¨ë“œ"""
        print("\nğŸ¤– SimpleRAG ëŒ€í™”í˜• ëª¨ë“œ")
        print("=" * 60)
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:")
        print("  - ì§ˆë¬¸ë§Œ ì…ë ¥: ì§ì ‘ ì§ˆë¬¸ (ì˜ˆ: 'RSIë€ ë¬´ì—‡ì¸ê°€ìš”?', 'ë³¼ë¦°ì €ë°´ë“œ ì‚¬ìš©ë²•')")
        print("  - 'ask <ì§ˆë¬¸>': ëª…ì‹œì  ì§ˆë¬¸ ëª…ë ¹")
        print("  - 'info': ì‹œìŠ¤í…œ ì •ë³´")
        print("  - 'graph': ì›Œí¬í”Œë¡œìš° ì‹œê°í™”")
        print("  - 'quit': ì¢…ë£Œ")
        print(f"\ní˜„ì¬ ì„¤ì •:")
        print(f"  â€¢ ì„ë² ë”©: {self.embedding_type}")
        print(f"  â€¢ LLM: Gemini 2.5 Flash")
        print(f"  â€¢ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {self.prompt_style}")
        print(f"  â€¢ ì›Œí¬í”Œë¡œìš°: LangGraph")
        print()
        
        while True:
            try:
                command = input("ì§ˆë¬¸ ì…ë ¥: ").strip()
                
                if command.lower() == 'quit':
                    break
                elif command.lower() == 'info':
                    print(f"\nğŸ“Š ì‹œìŠ¤í…œ ì •ë³´:")
                    print(f"  â€¢ ì„ë² ë”© ëª¨ë¸: {self.embedding_type}")
                    print(f"  â€¢ LLM ëª¨ë¸: Gemini 2.5 Flash")
                    print(f"  â€¢ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {self.prompt_style}")
                    print(f"  â€¢ ì›Œí¬í”Œë¡œìš° ì—”ì§„: LangGraph")
                    print(f"  â€¢ ë…¸ë“œ êµ¬ì„±: search â†’ generate_answer")
                elif command.lower() == 'graph':
                    self.visualize_workflow()
                elif command.startswith('ask '):
                    question = command[4:].strip()
                    if question:
                        result = self.process_query(question)
                        self.save_results(result)
                    else:
                        print("   âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                elif command:
                    result = self.process_query(command)
                    self.save_results(result)
                else:
                    print("   âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                
                print()
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ SimpleRAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
    embedding_type = "huggingface"
    prompt_style = "simple"
    
    if len(sys.argv) > 1:
        embedding_type = sys.argv[1].lower()
    
    if len(sys.argv) > 2:
        prompt_style = sys.argv[2].lower()
    
    # ìœ íš¨ì„± ê²€ì‚¬
    if embedding_type not in ["huggingface", "gemini"]:
        print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© íƒ€ì…ì…ë‹ˆë‹¤. 'huggingface' ë˜ëŠ” 'gemini'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        return
    
    if prompt_style not in ["simple", "detailed", "academic"]:
        print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ì…ë‹ˆë‹¤. 'simple', 'detailed', 'academic' ì¤‘ ì„ íƒí•˜ì„¸ìš”.")
        return
    
    print(f"ğŸ¤– SimpleRAG with LangGraph")
    print(f"   â€¢ ì„ë² ë”© ëª¨ë¸: {embedding_type}")
    print(f"   â€¢ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {prompt_style}")
    print(f"   â€¢ ì›Œí¬í”Œë¡œìš° ì—”ì§„: LangGraph")
    
    # SimpleRAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    simple_rag = SimpleRAGWithLangGraph(embedding_type=embedding_type, prompt_style=prompt_style)
    
    # ì¸ë±ìŠ¤ ë¡œë“œ
    if simple_rag.load_index():
        print("\nğŸ‰ SimpleRAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
        
        # ì›Œí¬í”Œë¡œìš° ì‹œê°í™” ìƒì„±
        simple_rag.visualize_workflow()
        
        # ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰
        simple_rag.interactive_mode()
        
    else:
        print("âŒ SimpleRAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë¨¼ì € 'faiss_index_builder.py'ë¥¼ ì‹¤í–‰í•˜ì—¬ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main() 