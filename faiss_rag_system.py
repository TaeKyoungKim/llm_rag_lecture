"""
FAISS RAG ì‹œìŠ¤í…œ
FAISS ê²€ìƒ‰ ê²°ê³¼ë¥¼ Gemini 2.5 Flash ëª¨ë¸ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import os
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import numpy as np

# FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬
import faiss

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.schema import Document, HumanMessage, SystemMessage
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.chains import LLMChain
from langchain.schema.output_parser import StrOutputParser

from dotenv import load_dotenv
load_dotenv()

class FAISSRAGSystem:
    """
    FAISS RAG ì‹œìŠ¤í…œ í´ë˜ìŠ¤
    FAISS ê²€ìƒ‰ ê²°ê³¼ë¥¼ Gemini 2.5 Flash ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±
    """
    
    def __init__(self, embedding_type: str = "huggingface", prompt_style: str = "detailed"):
        """
        RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        Args:
            embedding_type: "huggingface" ë˜ëŠ” "gemini"
            prompt_style: "detailed", "simple", "academic" ì¤‘ ì„ íƒ
        """
        print("ğŸ¤– FAISS RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.index_dir = Path("DocumentsLoader/educational_faiss_index")
        
        # ì„ë² ë”© íƒ€ì… ì„¤ì •
        self.embedding_type = embedding_type
        
        # í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ì„¤ì •
        self.prompt_style = prompt_style
        
        # HNSW ì„¤ì •
        self.hnsw_config = {
            'M': 16,
            'efConstruction': 100,
            'efSearch': 50,
            'metric': faiss.METRIC_INNER_PRODUCT
        }
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_embeddings()
        
        # LLM ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_llm()
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.faiss_index = None
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”
        self._initialize_prompts()
        
        print("   âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        print("   ğŸ”„ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        if self.embedding_type == "gemini":
            try:
                # Gemini API í‚¤ í™•ì¸
                api_key = os.getenv("GOOGLE_API_KEY")
                if not api_key:
                    print("   âš ï¸ GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    print("   ğŸ’¡ HuggingFace ì„ë² ë”©ìœ¼ë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤.")
                    self._load_huggingface_embeddings()
                    return
                
                print("   ğŸ”‘ Gemini API í‚¤ í™•ì¸ë¨")
                
                # Gemini ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
                self.embeddings = GoogleGenerativeAIEmbeddings(
                    model="models/embedding-001",
                    google_api_key=api_key,
                    task_type="retrieval_query",
                    title="Technical Analysis Document"
                )
                
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ ì—°ê²° í™•ì¸
                test_embedding = self.embeddings.embed_query("test")
                if test_embedding and len(test_embedding) > 0:
                    print("   âœ… Gemini ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                else:
                    raise Exception("ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                    
            except Exception as e:
                error_msg = str(e)
                print(f"   âš ï¸ Gemini ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {error_msg}")
                
                # ì˜¤ë¥˜ ìœ í˜•ë³„ ì•ˆë‚´ ë©”ì‹œì§€
                if "invalid_grant" in error_msg or "Bad Request" in error_msg:
                    print("   ğŸ’¡ API í‚¤ ì¸ì¦ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
                    print("      - GOOGLE_API_KEYê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
                    print("      - API í‚¤ê°€ Gemini APIì— ëŒ€í•œ ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸")
                    print("      - API í‚¤ê°€ ë§Œë£Œë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸")
                elif "timeout" in error_msg.lower():
                    print("   ğŸ’¡ ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒì…ë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                elif "quota" in error_msg.lower():
                    print("   ğŸ’¡ API í• ë‹¹ëŸ‰ ì´ˆê³¼ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
                else:
                    print("   ğŸ’¡ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ì…ë‹ˆë‹¤. HuggingFace ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                
                print("   ğŸ”„ HuggingFace ì„ë² ë”©ìœ¼ë¡œ ìë™ ì „í™˜ ì¤‘...")
                self._load_huggingface_embeddings()
        else:
            self._load_huggingface_embeddings()
    
    def _load_huggingface_embeddings(self):
        """HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        try:
            print("   ğŸ”„ HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            # ì—¬ëŸ¬ ëª¨ë¸ ì˜µì…˜ ì œê³µ
            model_options = [
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                "sentence-transformers/all-MiniLM-L6-v2",
                "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
            ]
            
            for model_name in model_options:
                try:
                    print(f"   ğŸ” ëª¨ë¸ ì‹œë„ ì¤‘: {model_name}")
                    self.embeddings = HuggingFaceEmbeddings(
                        model_name=model_name,
                        model_kwargs={'device': 'cpu'},
                        encode_kwargs={'normalize_embeddings': True}
                    )
                    
                    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
                    test_embedding = self.embeddings.embed_query("test")
                    if test_embedding and len(test_embedding) > 0:
                        print(f"   âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
                        self.embedding_type = "huggingface"
                        return
                    else:
                        raise Exception("ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                        
                except Exception as e:
                    print(f"   âš ï¸ {model_name} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                    continue
            
            # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš°
            raise Exception("ì‚¬ìš© ê°€ëŠ¥í•œ HuggingFace ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"   âŒ HuggingFace ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            print("   ğŸ’¡ ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ì „í™˜í•˜ê±°ë‚˜ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            raise
    
    def _initialize_llm(self):
        """LLM ëª¨ë¸ ì´ˆê¸°í™”"""
        print("   ğŸ”„ LLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        try:
            # Gemini API í‚¤ í™•ì¸
            api_key = os.getenv("GOOGLE_API_KEY")
            if not api_key:
                raise Exception("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            print("   ğŸ”‘ Gemini API í‚¤ í™•ì¸ë¨")
            
            # Gemini 2.5 Flash ëª¨ë¸ ì´ˆê¸°í™”
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-2.0-flash-exp",
                google_api_key=api_key,
                temperature=0.7,
                max_output_tokens=2048,
                convert_system_message_to_human=True
            )
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            test_response = self.llm.invoke("ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.")
            if test_response and test_response.content:
                print("   âœ… Gemini 2.5 Flash LLM ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            else:
                raise Exception("LLM í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                
        except Exception as e:
            error_msg = str(e)
            print(f"   âŒ LLM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {error_msg}")
            
            # ì˜¤ë¥˜ ìœ í˜•ë³„ ì•ˆë‚´ ë©”ì‹œì§€
            if "invalid_grant" in error_msg or "Bad Request" in error_msg:
                print("   ğŸ’¡ API í‚¤ ì¸ì¦ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
                print("      - GOOGLE_API_KEYê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
                print("      - API í‚¤ê°€ Gemini APIì— ëŒ€í•œ ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸")
                print("      - API í‚¤ê°€ ë§Œë£Œë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸")
            elif "timeout" in error_msg.lower():
                print("   ğŸ’¡ ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒì…ë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            elif "quota" in error_msg.lower():
                print("   ğŸ’¡ API í• ë‹¹ëŸ‰ ì´ˆê³¼ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
            else:
                print("   ğŸ’¡ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ì…ë‹ˆë‹¤. API í‚¤ì™€ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
            raise
    
    def _initialize_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”"""
        print(f"   ğŸ”„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™” ì¤‘... (ìŠ¤íƒ€ì¼: {self.prompt_style})")
        
        # í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ë³„ í…œí”Œë¦¿ ì •ì˜
        prompt_templates = {
            "detailed": ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ê¸°ìˆ ì  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
3. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”
4. í•„ìš”ì‹œ ì˜ˆì‹œë‚˜ êµ¬ì²´ì ì¸ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”
5. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
6. ë‹µë³€ì˜ ì‹œì‘ì— "ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."ë¼ëŠ” ë¬¸êµ¬ë¥¼ í¬í•¨í•˜ì„¸ìš”
7. ë‹µë³€ì˜ ëì— "ì´ìƒì´ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ë‚´ìš©ì…ë‹ˆë‹¤."ë¼ëŠ” ë¬¸êµ¬ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”
8. ë§Œì•½ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
9. ë‹µë³€ì€ êµ¬ì¡°í™”í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš” (ì˜ˆ: ì£¼ìš” ê°œë…, íŠ¹ì§•, í™œìš©ë²• ë“±)"""),
                ("human", """ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.""")
            ]),
            
            "simple": ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ê¸°ìˆ ì  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ë‚´ìš©ë§Œ í¬í•¨í•˜ì„¸ìš”
3. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
4. ë¶ˆí•„ìš”í•œ í˜•ì‹ì  ë¬¸êµ¬ëŠ” ìƒëµí•˜ì„¸ìš”"""),
                ("human", """ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€í•´ì£¼ì„¸ìš”.""")
            ]),
            
            "academic": ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ê¸ˆìœµê³µí•™ ë° ê¸°ìˆ ì  ë¶„ì„ ë¶„ì•¼ì˜ í•™ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìˆ ì ì´ê³  ì²´ê³„ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìˆ ì  ê´€ì ì—ì„œ ë‹µë³€í•˜ì„¸ìš”
2. ê°œë…ì  ì •ì˜, ìˆ˜í•™ì  ì›ë¦¬, ì‹¤ë¬´ ì ìš© ìˆœì„œë¡œ êµ¬ì„±í•˜ì„¸ìš”
3. ì „ë¬¸ ìš©ì–´ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ë˜, ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”
4. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
5. ë‹µë³€ì˜ ì‹œì‘ì— "í•™ìˆ ì  ê´€ì ì—ì„œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."ë¼ëŠ” ë¬¸êµ¬ë¥¼ í¬í•¨í•˜ì„¸ìš”
6. ë‹µë³€ì˜ ëì— "ì´ìƒì´ í•™ìˆ ì  ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤."ë¼ëŠ” ë¬¸êµ¬ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”"""),
                ("human", """ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ í•™ìˆ ì  ê´€ì ì—ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

í•™ìˆ ì ì´ê³  ì²´ê³„ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.""")
            ])
        }
        
        # ì„ íƒëœ ìŠ¤íƒ€ì¼ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
        if self.prompt_style in prompt_templates:
            self.prompt_template = prompt_templates[self.prompt_style]
        else:
            print(f"   âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ '{self.prompt_style}'. ê¸°ë³¸ ìŠ¤íƒ€ì¼(detailed)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.prompt_template = prompt_templates["detailed"]
        
        # LLM ì²´ì¸ ìƒì„± (ìµœì‹  LangChain ë°©ì‹)
        self.llm_chain = self.prompt_template | self.llm | StrOutputParser()
        
        print(f"   âœ… ChatPromptTemplate ì´ˆê¸°í™” ì™„ë£Œ (ìŠ¤íƒ€ì¼: {self.prompt_style})")
    
    def load_index(self) -> bool:
        """ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            if not (self.index_dir / "index.faiss").exists():
                print(f"   âŒ FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.index_dir}")
                print("   ğŸ’¡ ë¨¼ì € 'faiss_index_builder.py'ë¥¼ ì‹¤í–‰í•˜ì—¬ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•´ì£¼ì„¸ìš”.")
                return False
            
            print("ğŸ“‚ FAISS HNSW ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
            self.faiss_index = FAISS.load_local(
                str(self.index_dir),
                self.embeddings,
                allow_dangerous_deserialization=True  # ë¡œì»¬ì—ì„œ ìƒì„±í•œ íŒŒì¼ì´ë¯€ë¡œ ì•ˆì „
            )
            print("   âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            
            # ì¸ë±ìŠ¤ ì •ë³´ ì¶œë ¥
            self._print_index_info()
            
            return True
            
        except Exception as e:
            print(f"   âŒ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _print_index_info(self):
        """ì¸ë±ìŠ¤ ì •ë³´ ì¶œë ¥"""
        if not self.faiss_index:
            return
        
        print("\nğŸ“Š ì¸ë±ìŠ¤ ì •ë³´:")
        print("-" * 30)
        
        # ê¸°ë³¸ ì •ë³´
        print(f"   â€¢ ì„ë² ë”© ëª¨ë¸: {self.embedding_type}")
        print(f"   â€¢ LLM ëª¨ë¸: Gemini 2.5 Flash")
        print(f"   â€¢ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {self.prompt_style}")
        print(f"   â€¢ ì¸ë±ìŠ¤ íƒ€ì…: FAISS HNSW")
        print(f"   â€¢ ë¡œë“œ ìœ„ì¹˜: {self.index_dir}")
        
        # HNSW ì •ë³´
        if hasattr(self.faiss_index.index, 'hnsw'):
            print(f"   â€¢ HNSW ë…¸ë“œ ìˆ˜: {self.faiss_index.index.hnsw.levels.size()}")
            print(f"   â€¢ HNSW ìµœëŒ€ ë ˆë²¨: {self.faiss_index.index.hnsw.max_level}")
            print(f"   â€¢ HNSW efSearch: {self.faiss_index.index.hnsw.efSearch}")
        
        # ë¬¸ì„œ ì •ë³´
        docs = list(self.faiss_index.docstore.values())
        if docs:
            technical_docs = sum(1 for doc in docs if doc.metadata.get('has_technical_content', False))
            print(f"   â€¢ ì´ ë¬¸ì„œ ìˆ˜: {len(docs)}")
            print(f"   â€¢ ê¸°ìˆ ì  ë‚´ìš© ë¬¸ì„œ: {technical_docs}")
            print(f"   â€¢ í‰ê·  ë¬¸ì„œ ê¸¸ì´: {sum(len(doc.page_content) for doc in docs) // len(docs)}ì")
    
    def search_and_generate_answer(self, query: str, k: int = 5) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ì™€ ìƒì„±ëœ ë‹µë³€ì„ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        """
        if not self.faiss_index:
            print("âŒ FAISS ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        try:
            print(f"\nğŸ” ì§ˆë¬¸: '{query}'")
            print("=" * 60)
            
            # HNSW ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì„¤ì •
            if hasattr(self.faiss_index.index, 'hnsw'):
                self.faiss_index.index.hnsw.efSearch = self.hnsw_config['efSearch']
                print(f"   ğŸ”§ HNSW efSearch ì„¤ì •: {self.hnsw_config['efSearch']}")
            
            # 1ë‹¨ê³„: ìœ ì‚¬ë„ ê²€ìƒ‰
            print("   ğŸ”„ 1ë‹¨ê³„: ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
            
            # ì§ì ‘ FAISS ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰
            try:
                # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
                query_embedding = self.embeddings.embed_query(query)
                query_vector = np.array([query_embedding], dtype=np.float32)
                
                # FAISS ê²€ìƒ‰ ìˆ˜í–‰
                distances, indices = self.faiss_index.index.search(query_vector, k)
                
                # ê²°ê³¼ êµ¬ì„±
                docs_and_scores = []
                for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                    if idx != -1:  # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ ê²½ìš°
                        doc = self.faiss_index.docstore[idx]
                        # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ì¤€)
                        similarity_score = 1.0 - distance
                        docs_and_scores.append((doc, similarity_score))
                
                print(f"   âœ… {len(docs_and_scores)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
                
            except Exception as search_error:
                print(f"   âš ï¸ ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨: {str(search_error)}")
                print("   ğŸ”„ LangChain ë˜í¼ë¡œ ì¬ì‹œë„...")
                
                # LangChain ë˜í¼ë¡œ ì¬ì‹œë„
                docs_and_scores = self.faiss_index.similarity_search_with_score(
                    query, k=k
                )
            
            if not docs_and_scores:
                print("   âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    'query': query,
                    'search_results': [],
                    'answer': "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    'context': "",
                    'sources': []
                }
            
            # 2ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            print("   ğŸ”„ 2ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘...")
            
            # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ({len(docs_and_scores)}ê°œ):")
            print("-" * 40)
            
            context_parts = []
            sources = []
            
            for i, (doc, score) in enumerate(docs_and_scores, 1):
                print(f"   ğŸ“„ ë¬¸ì„œ {i}:")
                print(f"      - ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}")
                print(f"      - í˜ì´ì§€: {doc.metadata.get('page', 'N/A')}")
                print(f"      - ì²­í¬ í¬ê¸°: {doc.metadata.get('chunk_size', 'N/A')}ì")
                print(f"      - ê¸°ìˆ ì  ë‚´ìš©: {'âœ…' if doc.metadata.get('has_technical_content', False) else 'âŒ'}")
                print(f"      - ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:100]}...")
                print()
                
                # ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                context_parts.append(f"[ë¬¸ì„œ {i}] {doc.page_content}")
                sources.append({
                    'page': doc.metadata.get('page', 'N/A'),
                    'chunk_id': doc.metadata.get('chunk_id', 'N/A'),
                    'score': float(score),
                    'content_preview': doc.page_content[:200] + "..."
                })
            
            # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = "\n\n".join(context_parts)
            print(f"   âœ… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ ({len(context)}ì)")
            
            # 3ë‹¨ê³„: ë‹µë³€ ìƒì„±
            print("   ğŸ”„ 3ë‹¨ê³„: Gemini 2.5 Flashë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
            
            try:
                # LLM ì²´ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„± (ìµœì‹  LangChain ë°©ì‹)
                response = self.llm_chain.invoke({
                    "context": context,
                    "question": query
                })
                
                answer = str(response)
                print("   âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
                
            except Exception as llm_error:
                print(f"   âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(llm_error)}")
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'query': query,
                'search_results': docs_and_scores,
                'answer': answer,
                'context': context,
                'sources': sources,
                'embedding_type': self.embedding_type,
                'llm_model': 'gemini-2.5-flash',
                'timestamp': datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            print(f"   âŒ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                'query': query,
                'search_results': [],
                'answer': f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'context': "",
                'sources': []
            }
    
    def save_rag_results(self, result: Dict[str, Any]):
        """RAG ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"rag_results_{timestamp}.json"
        
        try:
            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            save_result = {
                'query': result.get('query', ''),
                'answer': result.get('answer', ''),
                'embedding_type': result.get('embedding_type', ''),
                'llm_model': result.get('llm_model', ''),
                'timestamp': result.get('timestamp', ''),
                'sources': result.get('sources', []),
                'context_preview': result.get('context', '')[:1000] + "..." if result.get('context') else ""
            }
            
            output_path = Path("DocumentsLoader/rag_results") / filename
            output_path.parent.mkdir(exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(save_result, f, ensure_ascii=False, indent=2)
            
            print(f"   âœ… RAG ê²°ê³¼ ì €ì¥: {output_path}")
            
        except Exception as e:
            print(f"   âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def interactive_rag(self):
        """ëŒ€í™”í˜• RAG ëª¨ë“œ"""
        print("\nğŸ¤– ëŒ€í™”í˜• RAG ëª¨ë“œ")
        print("=" * 60)
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:")
        print("  - ì§ˆë¬¸ë§Œ ì…ë ¥: ì§ì ‘ ì§ˆë¬¸ (ì˜ˆ: 'RSIë€ ë¬´ì—‡ì¸ê°€ìš”?', 'ë³¼ë¦°ì €ë°´ë“œ ì‚¬ìš©ë²•')")
        print("  - 'ask <ì§ˆë¬¸>': ëª…ì‹œì  ì§ˆë¬¸ ëª…ë ¹")
        print("  - 'info': ì‹œìŠ¤í…œ ì •ë³´")
        print("  - 'quit': ì¢…ë£Œ")
        print(f"\ní˜„ì¬ ëª¨ë¸:")
        print(f"  â€¢ ì„ë² ë”©: {self.embedding_type}")
        print(f"  â€¢ LLM: Gemini 2.5 Flash")
        print(f"  â€¢ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {self.prompt_style}")
        print()
        
        while True:
            try:
                command = input("ì§ˆë¬¸ ì…ë ¥: ").strip()
                
                if command.lower() == 'quit':
                    break
                elif command.lower() == 'info':
                    self._print_index_info()
                elif command.startswith('ask '):
                    # ëª…ì‹œì  ask ëª…ë ¹ì–´ ì²˜ë¦¬
                    question = command[4:].strip()
                    if question:
                        result = self.search_and_generate_answer(question)
                        if result and result.get('answer'):
                            print(f"\nğŸ¤– ë‹µë³€:")
                            print("-" * 40)
                            print(result['answer'])
                            print()
                            self.save_rag_results(result)
                    else:
                        print("   âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                elif command:
                    # ë‹¨ìˆœ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬ (ask ì ‘ë‘ì‚¬ ì—†ì´)
                    result = self.search_and_generate_answer(command)
                    if result and result.get('answer'):
                        print(f"\nğŸ¤– ë‹µë³€:")
                        print("-" * 40)
                        print(result['answer'])
                        print()
                        self.save_rag_results(result)
                    else:
                        print("   âŒ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    print("   âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                
                print()
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ RAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
    embedding_type = "huggingface"
    prompt_style = "detailed"
    
    if len(sys.argv) > 1:
        embedding_type = sys.argv[1].lower()
    
    if len(sys.argv) > 2:
        prompt_style = sys.argv[2].lower()
    
    # ìœ íš¨ì„± ê²€ì‚¬
    if embedding_type not in ["huggingface", "gemini"]:
        print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© íƒ€ì…ì…ë‹ˆë‹¤. 'huggingface' ë˜ëŠ” 'gemini'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        return
    
    if prompt_style not in ["detailed", "simple", "academic"]:
        print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ì…ë‹ˆë‹¤. 'detailed', 'simple', 'academic' ì¤‘ ì„ íƒí•˜ì„¸ìš”.")
        return
    
    print(f"ğŸ¤– FAISS RAG ì‹œìŠ¤í…œ")
    print(f"   â€¢ ì„ë² ë”© ëª¨ë¸: {embedding_type}")
    print(f"   â€¢ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {prompt_style}")
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = FAISSRAGSystem(embedding_type=embedding_type, prompt_style=prompt_style)
    
    # ì¸ë±ìŠ¤ ë¡œë“œ
    if rag_system.load_index():
        print("\nğŸ‰ RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
        
        # ëŒ€í™”í˜• RAG ëª¨ë“œ ì‹¤í–‰
        rag_system.interactive_rag()
        
    else:
        print("âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë¨¼ì € 'faiss_index_builder.py'ë¥¼ ì‹¤í–‰í•˜ì—¬ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main() 