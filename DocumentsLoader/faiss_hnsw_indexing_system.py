# LangChainì„ í™œìš©í•œ FAISS HNSW ì¸ë±ì‹± ì‹œìŠ¤í…œ
# ê¸°ìˆ ì  ë¶„ì„ PDF ë‚´ìš©ì„ ë²¡í„°í™”í•˜ì—¬ íš¨ìœ¨ì ì¸ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•

import os
import re
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# ê¸°ì¡´ PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ ì„í¬íŠ¸
from process_technical_analysis_pdf_improved import ImprovedPDFProcessor

print("ğŸ”§ LangChain FAISS HNSW ì¸ë±ì‹± ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
print("=" * 60)

class FAISSHNSWIndexingSystem:
    """LangChainì„ í™œìš©í•œ FAISS HNSW ì¸ë±ì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.pdf_path = Path("DocumentsLoader/data/ê¸°ìˆ ì ì°¨íŠ¸ë¶„ì„ì´ë¡ ë°ë°©ë²•.pdf")
        self.index_dir = Path("DocumentsLoader/faiss_index")
        self.index_dir.mkdir(exist_ok=True)
        
        # í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (í•œêµ­ì–´ ìµœì í™”)
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # FAISS ì¸ë±ìŠ¤ ì„¤ì • (HNSW ë°©ì‹)
        self.faiss_index = None
        self.documents = []
        
        # ê¸°ìˆ ì  ë¶„ì„ í‚¤ì›Œë“œ (ê²€ìƒ‰ ìµœì í™”)
        self.technical_keywords = {
            'indicators': [
                'RSI', 'ìƒëŒ€ê°•ë„ì§€ìˆ˜', 'MACD', 'ì´ë™í‰ê· ìˆ˜ë ´í™•ì‚°',
                'ë³¼ë¦°ì €ë°´ë“œ', 'Bollinger', 'ìŠ¤í† ìºìŠ¤í‹±', 'Stochastic',
                'ì´ë™í‰ê· ì„ ', 'ì´í‰ì„ ', 'ì´ë™í‰ê· ', 'ì´í‰',
                'ì¼ëª©ê· í˜•í‘œ', 'ì¼ëª©ê· í˜•', 'ê· í˜•í‘œ',
                'í”¼ë³´ë‚˜ì¹˜', 'Fibonacci', 'í”¼ë³´ë‚˜ì¹˜ë˜ëŒë¦¼', 'í”¼ë³´ë‚˜ì¹˜í™•ì¥',
                'ì—˜ë¦¬ì–´íŠ¸', 'Elliott', 'ì—˜ë¦¬ì–´íŠ¸íŒŒë™', 'íŒŒë™ì´ë¡ '
            ],
            'concepts': [
                'ì§€ì§€ì„ ', 'ì €í•­ì„ ', 'ì¶”ì„¸ì„ ', 'ê³¼ë§¤ìˆ˜', 'ê³¼ë§¤ë„', 'ë‹¤ì´ë²„ì „ìŠ¤',
                'ê³¨ë“ í¬ë¡œìŠ¤', 'ë°ë“œí¬ë¡œìŠ¤', 'ê±°ë˜ëŸ‰', 'ë§¤ë¬¼ëŒ€',
                'ì§€ì§€', 'ì €í•­', 'ì¶”ì„¸', 'ì¶”ì„¸ëŒ€',
                'í¬ë¡œìŠ¤', 'í¬ë¡œìŠ¤ì˜¤ë²„', 'ë¸Œë ˆì´í¬ì•„ì›ƒ', 'ë¸Œë ˆì´í¬', 'ëŒíŒŒ',
                'í’€ë°±', 'ë˜ëŒë¦¼', 'ì¡°ì •', 'ë°˜ë“±'
            ]
        }
    
    def load_and_process_pdf(self) -> List[Document]:
        """PDF ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("ğŸ“„ PDF ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘...")
        
        try:
            # ê¸°ì¡´ PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ í™œìš©
            pdf_processor = ImprovedPDFProcessor()
            
            if not pdf_processor.check_pdf_exists():
                print("âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # PDF ë¡œë“œ
            docs = pdf_processor.load_pdf_with_pypdf()
            if not docs:
                print("âŒ PDF ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return []
            
            # LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            langchain_docs = []
            for i, doc in enumerate(docs):
                # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                metadata = {
                    'source': str(self.pdf_path),
                    'page': i + 1,
                    'total_pages': len(docs),
                    'content_type': 'technical_analysis',
                    'language': 'ko',
                    'domain': 'stock_analysis'
                }
                
                # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë³‘í•©
                if hasattr(doc, 'metadata') and doc.metadata:
                    metadata.update(doc.metadata)
                
                # LangChain Document ìƒì„±
                langchain_doc = Document(
                    page_content=doc.page_content,
                    metadata=metadata
                )
                langchain_docs.append(langchain_doc)
            
            print(f"   âœ… {len(langchain_docs)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
            return langchain_docs
            
        except Exception as e:
            print(f"   âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• """
        print("âœ‚ï¸ ë¬¸ì„œ ì²­í¬ ë¶„í•  ì¤‘...")
        
        try:
            split_docs = self.text_splitter.split_documents(documents)
            
            # ì²­í¬ë³„ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for i, doc in enumerate(split_docs):
                doc.metadata.update({
                    'chunk_id': i,
                    'chunk_size': len(doc.page_content),
                    'processing_time': datetime.now().isoformat()
                })
            
            print(f"   âœ… {len(split_docs)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
            print(f"   ğŸ“Š í‰ê·  ì²­í¬ í¬ê¸°: {sum(len(doc.page_content) for doc in split_docs) // len(split_docs)}ì")
            
            return split_docs
            
        except Exception as e:
            print(f"   âŒ ë¬¸ì„œ ë¶„í•  ì‹¤íŒ¨: {str(e)}")
            return []
    
    def create_faiss_index(self, documents: List[Document]) -> bool:
        """FAISS HNSW ì¸ë±ìŠ¤ ìƒì„±"""
        print("ğŸ” FAISS HNSW ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        try:
            # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (HNSW ë°©ì‹)
            self.faiss_index = FAISS.from_documents(
                documents=documents,
                embedding=self.embeddings,
                index_name="technical_analysis_hnsw"
            )
            
            # ì¸ë±ìŠ¤ ì €ì¥
            self.faiss_index.save_local(str(self.index_dir))
            
            print(f"   âœ… FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
            print(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.index_dir}")
            print(f"   ğŸ“Š ì´ ë²¡í„° ìˆ˜: {len(documents)}")
            
            return True
            
        except Exception as e:
            print(f"   âŒ FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False
    
    def load_existing_index(self) -> bool:
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            if (self.index_dir / "index.faiss").exists():
                print("ğŸ“‚ ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
                self.faiss_index = FAISS.load_local(
                    str(self.index_dir),
                    self.embeddings,
                    index_name="technical_analysis_hnsw"
                )
                print("   âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                print("   âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
        except Exception as e:
            print(f"   âŒ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def semantic_search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ìˆ˜í–‰"""
        if not self.faiss_index:
            print("âŒ FAISS ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            print(f"ğŸ” ì˜ë¯¸ë¡ ì  ê²€ìƒ‰: '{query}'")
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
            docs_and_scores = self.faiss_index.similarity_search_with_score(
                query, k=k
            )
            
            print(f"   âœ… {len(docs_and_scores)}ê°œ ê²°ê³¼ ë°œê²¬")
            
            # ê²°ê³¼ ì¶œë ¥
            for i, (doc, score) in enumerate(docs_and_scores, 1):
                print(f"   ğŸ“„ ê²°ê³¼ {i}:")
                print(f"      - ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}")
                print(f"      - í˜ì´ì§€: {doc.metadata.get('page', 'N/A')}")
                print(f"      - ì²­í¬ í¬ê¸°: {doc.metadata.get('chunk_size', 'N/A')}ì")
                print(f"      - ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:100]}...")
                print()
            
            return docs_and_scores
            
        except Exception as e:
            print(f"   âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def keyword_search(self, keywords: List[str], k: int = 5) -> List[Tuple[Document, float]]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        if not self.faiss_index:
            print("âŒ FAISS ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            print(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰: {', '.join(keywords)}")
            
            # í‚¤ì›Œë“œë¥¼ ê²°í•©í•œ ì¿¼ë¦¬ ìƒì„±
            query = " ".join(keywords)
            
            # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ìˆ˜í–‰
            results = self.semantic_search(query, k=k)
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ í•„í„°ë§
            filtered_results = []
            for doc, score in results:
                content_lower = doc.page_content.lower()
                keyword_matches = sum(1 for keyword in keywords if keyword.lower() in content_lower)
                
                if keyword_matches > 0:
                    # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ì¶”ê°€
                    enhanced_score = score + (keyword_matches * 0.1)
                    filtered_results.append((doc, enhanced_score))
            
            # ì ìˆ˜ë¡œ ì¬ì •ë ¬
            filtered_results.sort(key=lambda x: x[1], reverse=True)
            
            print(f"   âœ… í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼: {len(filtered_results)}ê°œ")
            return filtered_results[:k]
            
        except Exception as e:
            print(f"   âŒ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def technical_analysis_search(self, indicator: str, concept: str = None) -> List[Tuple[Document, float]]:
        """ê¸°ìˆ ì  ë¶„ì„ íŠ¹í™” ê²€ìƒ‰"""
        print(f"ğŸ“ˆ ê¸°ìˆ ì  ë¶„ì„ ê²€ìƒ‰: {indicator}")
        if concept:
            print(f"   ê´€ë ¨ ê°œë…: {concept}")
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_terms = [indicator]
        if concept:
            search_terms.append(concept)
        
        # í•œêµ­ì–´ ì„¤ëª… ì¶”ê°€
        korean_explanations = {
            'RSI': 'ìƒëŒ€ê°•ë„ì§€ìˆ˜',
            'MACD': 'ì´ë™í‰ê· ìˆ˜ë ´í™•ì‚°',
            'ë³¼ë¦°ì €ë°´ë“œ': 'ë³¼ë¦°ì €ë°´ë“œ',
            'ì´ë™í‰ê· ì„ ': 'ì´ë™í‰ê· ì„ ',
            'ìŠ¤í† ìºìŠ¤í‹±': 'ìŠ¤í† ìºìŠ¤í‹±',
            'ì¼ëª©ê· í˜•í‘œ': 'ì¼ëª©ê· í˜•í‘œ',
            'í”¼ë³´ë‚˜ì¹˜': 'í”¼ë³´ë‚˜ì¹˜ë˜ëŒë¦¼',
            'ì—˜ë¦¬ì–´íŠ¸': 'ì—˜ë¦¬ì–´íŠ¸íŒŒë™'
        }
        
        if indicator in korean_explanations:
            search_terms.append(korean_explanations[indicator])
        
        return self.keyword_search(search_terms, k=10)
    
    def get_index_statistics(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´"""
        if not self.faiss_index:
            return {}
        
        try:
            # ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘
            stats = {
                'total_vectors': len(self.faiss_index.docstore._dict),
                'index_type': 'FAISS HNSW',
                'embedding_model': 'paraphrase-multilingual-MiniLM-L12-v2',
                'index_path': str(self.index_dir),
                'created_time': datetime.now().isoformat()
            }
            
            # ë¬¸ì„œ í†µê³„
            docs = list(self.faiss_index.docstore._dict.values())
            if docs:
                stats.update({
                    'total_documents': len(docs),
                    'avg_document_length': sum(len(doc.page_content) for doc in docs) // len(docs),
                    'total_characters': sum(len(doc.page_content) for doc in docs),
                    'pages_covered': len(set(doc.metadata.get('page', 0) for doc in docs))
                })
            
            return stats
            
        except Exception as e:
            print(f"âŒ í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def save_search_results(self, results: List[Tuple[Document, float]], query: str, filename: str = None):
        """ê²€ìƒ‰ ê²°ê³¼ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"search_results_{timestamp}.json"
        
        try:
            # ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            search_data = {
                'query': query,
                'timestamp': datetime.now().isoformat(),
                'total_results': len(results),
                'results': []
            }
            
            for doc, score in results:
                result_item = {
                    'score': float(score),
                    'page': doc.metadata.get('page', 'N/A'),
                    'chunk_id': doc.metadata.get('chunk_id', 'N/A'),
                    'content': doc.page_content,
                    'metadata': doc.metadata
                }
                search_data['results'].append(result_item)
            
            # íŒŒì¼ ì €ì¥
            output_path = Path("DocumentsLoader/search_results") / filename
            output_path.parent.mkdir(exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(search_data, f, ensure_ascii=False, indent=2)
            
            print(f"   âœ… ê²€ìƒ‰ ê²°ê³¼ ì €ì¥: {output_path}")
            
        except Exception as e:
            print(f"   âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def build_index(self) -> bool:
        """ì „ì²´ ì¸ë±ìŠ¤ êµ¬ì¶• í”„ë¡œì„¸ìŠ¤"""
        print("ğŸš€ FAISS HNSW ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘")
        print("=" * 60)
        
        # 1. ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
        if self.load_existing_index():
            print("âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚¬ìš© ê°€ëŠ¥")
            return True
        
        # 2. PDF ë¡œë“œ ë° ì²˜ë¦¬
        documents = self.load_and_process_pdf()
        if not documents:
            return False
        
        # 3. ë¬¸ì„œ ì²­í¬ ë¶„í• 
        split_docs = self.split_documents(documents)
        if not split_docs:
            return False
        
        # 4. FAISS ì¸ë±ìŠ¤ ìƒì„±
        success = self.create_faiss_index(split_docs)
        
        if success:
            # 5. í†µê³„ ì •ë³´ ì¶œë ¥
            stats = self.get_index_statistics()
            print(f"\nğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
            for key, value in stats.items():
                print(f"   â€¢ {key}: {value}")
        
        return success
    
    def interactive_search(self):
        """ëŒ€í™”í˜• ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤"""
        print("\nğŸ” ëŒ€í™”í˜• ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤")
        print("=" * 40)
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:")
        print("  - 'search <ê²€ìƒ‰ì–´>': ì˜ë¯¸ë¡ ì  ê²€ìƒ‰")
        print("  - 'keyword <í‚¤ì›Œë“œ1,í‚¤ì›Œë“œ2>': í‚¤ì›Œë“œ ê²€ìƒ‰")
        print("  - 'technical <ì§€í‘œëª…>': ê¸°ìˆ ì  ë¶„ì„ ê²€ìƒ‰")
        print("  - 'stats': ì¸ë±ìŠ¤ í†µê³„")
        print("  - 'quit': ì¢…ë£Œ")
        print()
        
        while True:
            try:
                command = input("ê²€ìƒ‰ ëª…ë ¹ì–´ ì…ë ¥: ").strip()
                
                if command.lower() == 'quit':
                    break
                elif command.lower() == 'stats':
                    stats = self.get_index_statistics()
                    print(f"\nğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
                    for key, value in stats.items():
                        print(f"   â€¢ {key}: {value}")
                
                elif command.startswith('search '):
                    query = command[7:].strip()
                    results = self.semantic_search(query)
                    if results:
                        self.save_search_results(results, query)
                
                elif command.startswith('keyword '):
                    keywords_str = command[8:].strip()
                    keywords = [k.strip() for k in keywords_str.split(',')]
                    results = self.keyword_search(keywords)
                    if results:
                        self.save_search_results(results, f"keywords: {keywords_str}")
                
                elif command.startswith('technical '):
                    indicator = command[10:].strip()
                    results = self.technical_analysis_search(indicator)
                    if results:
                        self.save_search_results(results, f"technical: {indicator}")
                
                else:
                    print("âŒ ì˜ëª»ëœ ëª…ë ¹ì–´ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                
                print()
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # FAISS ì¸ë±ì‹± ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    indexing_system = FAISSHNSWIndexingSystem()
    
    # ì¸ë±ìŠ¤ êµ¬ì¶•
    if indexing_system.build_index():
        print("\nğŸ‰ FAISS HNSW ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        
        # ìƒ˜í”Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\nğŸ§ª ìƒ˜í”Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
        
        # 1. ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\n1. RSI ì˜ë¯¸ë¡ ì  ê²€ìƒ‰:")
        indexing_system.semantic_search("RSI ìƒëŒ€ê°•ë„ì§€ìˆ˜ ë¶„ì„ ë°©ë²•")
        
        # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\n2. MACD í‚¤ì›Œë“œ ê²€ìƒ‰:")
        indexing_system.keyword_search(["MACD", "ì´ë™í‰ê· ìˆ˜ë ´í™•ì‚°", "ê³¨ë“ í¬ë¡œìŠ¤"])
        
        # 3. ê¸°ìˆ ì  ë¶„ì„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print("\n3. ë³¼ë¦°ì €ë°´ë“œ ê¸°ìˆ ì  ë¶„ì„ ê²€ìƒ‰:")
        indexing_system.technical_analysis_search("ë³¼ë¦°ì €ë°´ë“œ", "ê³¼ë§¤ìˆ˜")
        
        # 4. ëŒ€í™”í˜• ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤
        print("\n4. ëŒ€í™”í˜• ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤ ì‹œì‘:")
        indexing_system.interactive_search()
        
    else:
        print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶•ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 