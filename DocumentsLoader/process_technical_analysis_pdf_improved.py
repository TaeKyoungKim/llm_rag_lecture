# ê°œì„ ëœ ì‹¤ì œ PDF íŒŒì¼ ì²˜ë¦¬ ì‹œìŠ¤í…œ - í•œêµ­ì–´ ê¸°ìˆ ì  ë¶„ì„ ìš©ì–´ ìµœì í™”

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
# uv add langchain_community pypdf rapidocr-onnxruntime pytesseract pandas requests

import os
import re
import requests
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Any

# LangChain PDF ë¡œë”
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders import FileSystemBlobLoader
from langchain_community.document_loaders.parsers import PyPDFParser
from langchain_community.document_loaders.parsers import RapidOCRBlobParser, TesseractBlobParser

print("ğŸ”§ ê°œì„ ëœ ì‹¤ì œ PDF íŒŒì¼ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
print("=" * 60)

class ImprovedPDFProcessor:
    """ê°œì„ ëœ ì‹¤ì œ PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.pdf_path = Path("DocumentsLoader/data/ê¸°ìˆ ì ì°¨íŠ¸ë¶„ì„ì´ë¡ ë°ë°©ë²•.pdf")
        
        # í•œêµ­ì–´ ê¸°ìˆ ì  ë¶„ì„ í‚¤ì›Œë“œ (ë” í¬ê´„ì )
        self.technical_keywords = {
            'indicators': [
                # ê¸°ë³¸ ì§€í‘œ
                'RSI', 'ìƒëŒ€ê°•ë„ì§€ìˆ˜', 'MACD', 'ì´ë™í‰ê· ìˆ˜ë ´í™•ì‚°',
                'ë³¼ë¦°ì €ë°´ë“œ', 'Bollinger', 'ìŠ¤í† ìºìŠ¤í‹±', 'Stochastic',
                'ATR', 'CCI', 'Williams %R', 'ì´ë™í‰ê· ', 'Moving Average',
                'KDJ', 'DMI', 'OBV', 'VR', 'ROC', 'MOM', 'ADX',
                # í•œêµ­ì–´ ë³€í˜•
                'ì´ë™í‰ê· ì„ ', 'ì´í‰ì„ ', 'ì´ë™í‰ê· ', 'ì´í‰', 'ì´ë™í‰ê· ìˆ˜ë ´',
                'ìƒëŒ€ê°•ë„', 'ìƒëŒ€ê°•ë„ì§€ìˆ˜', 'RSIì§€ìˆ˜', 'RSIì§€í‘œ',
                'ë³¼ë¦°ì €', 'ë³¼ë¦°ì €ë°´ë“œ', 'ë³¼ë¦°ì €ë°´ë“œì§€í‘œ',
                'ìŠ¤í† ìºìŠ¤í‹±', 'ìŠ¤í† ìºìŠ¤í‹±ì§€í‘œ', 'ìŠ¤í† ìºìŠ¤í‹±ì˜¤ì‹¤ë ˆì´í„°',
                'íŒŒë¼ë³¼ë¦­', 'Parabolic', 'SAR', 'Parabolic SAR',
                'ì¼ëª©ê· í˜•í‘œ', 'ì¼ëª©ê· í˜•', 'ê· í˜•í‘œ',
                'í”¼ë³´ë‚˜ì¹˜', 'Fibonacci', 'í”¼ë³´ë‚˜ì¹˜ë˜ëŒë¦¼', 'í”¼ë³´ë‚˜ì¹˜í™•ì¥',
                'ì—˜ë¦¬ì–´íŠ¸', 'Elliott', 'ì—˜ë¦¬ì–´íŠ¸íŒŒë™', 'íŒŒë™ì´ë¡ '
            ],
            'patterns': [
                # ê¸°ë³¸ íŒ¨í„´
                'ì‚¼ê°í˜•', 'ìê¸°í˜•', 'í”Œë˜ê·¸', 'í—¤ë“œì•¤ìˆ„ë”', 'ë”ë¸”íƒ‘', 'ë”ë¸”ë°”í…€',
                'ì»µì•¤í•¸ë“¤', 'ì—­í—¤ë“œì•¤ìˆ„ë”', 'ìƒìŠ¹ì‚¼ê°í˜•', 'í•˜ë½ì‚¼ê°í˜•',
                'Triangle', 'Wedge', 'Flag', 'Head and Shoulders', 'Double Top', 'Double Bottom',
                'Cup and Handle', 'Inverse Head and Shoulders',
                # í•œêµ­ì–´ ë³€í˜•
                'ì‚¼ê°í˜•íŒ¨í„´', 'ìê¸°í˜•íŒ¨í„´', 'í”Œë˜ê·¸íŒ¨í„´', 'í—¤ë“œì•¤ìˆ„ë”íŒ¨í„´',
                'ë”ë¸”íƒ‘íŒ¨í„´', 'ë”ë¸”ë°”í…€íŒ¨í„´', 'ì»µì•¤í•¸ë“¤íŒ¨í„´',
                'ì¼ë°˜íŒ¨í„´', 'ì§€ì†íŒ¨í„´', 'ë°˜ì „íŒ¨í„´', 'ì°¨íŠ¸íŒ¨í„´'
            ],
            'concepts': [
                # ê¸°ë³¸ ê°œë…
                'ì§€ì§€ì„ ', 'ì €í•­ì„ ', 'ì¶”ì„¸ì„ ', 'ê³¼ë§¤ìˆ˜', 'ê³¼ë§¤ë„', 'ë‹¤ì´ë²„ì „ìŠ¤',
                'í¬ë¡œìŠ¤ì˜¤ë²„', 'ë¸Œë ˆì´í¬ì•„ì›ƒ', 'í’€ë°±', 'ë˜ëŒë¦¼', 'ê³¨ë“ í¬ë¡œìŠ¤', 'ë°ë“œí¬ë¡œìŠ¤',
                'Support', 'Resistance', 'Trend', 'Overbought', 'Oversold', 'Divergence',
                'Crossover', 'Breakout', 'Pullback', 'Retracement',
                # í•œêµ­ì–´ ë³€í˜•
                'ì§€ì§€', 'ì €í•­', 'ì¶”ì„¸', 'ì¶”ì„¸ëŒ€', 'ì¶”ì„¸ì„ ',
                'ê³¼ë§¤ìˆ˜', 'ê³¼ë§¤ë„', 'ì˜¤ë²„ë°”ì´', 'ì˜¤ë²„ì†”ë“œ',
                'ë‹¤ì´ë²„ì „ìŠ¤', 'ë‹¤ì´ë²„ì „ìŠ¤ì´í•´', 'ë‹¤ì´ë²„ì „ìŠ¤ì¢…ë¥˜',
                'í¬ë¡œìŠ¤', 'í¬ë¡œìŠ¤ì˜¤ë²„', 'ê³¨ë“ í¬ë¡œìŠ¤', 'ë°ë“œí¬ë¡œìŠ¤',
                'ë¸Œë ˆì´í¬ì•„ì›ƒ', 'ë¸Œë ˆì´í¬', 'ëŒíŒŒ', 'ëŒíŒŒì„ ',
                'í’€ë°±', 'ë˜ëŒë¦¼', 'ì¡°ì •', 'ë°˜ë“±',
                'ê±°ë˜ëŸ‰', 'ê±°ë˜ëŸ‰ê¸°ì´ˆ', 'ê±°ë˜ëŸ‰ê³¼ì‹œì„¸', 'ê±°ë˜ëŸ‰ìœ¼ë¡œë³´ëŠ”ì„¸ë ¥í™œë™',
                'ë§¤ë¬¼ëŒ€', 'ë§¤ë¬¼', 'ë§¤ë¬¼ëŒ€ë¶„ì„',
                'í¬íŠ¸í´ë¦¬ì˜¤', 'í¬íŠ¸í´ë¦¬ì˜¤ì´ë¡ ',
                'ê¹€ì¹˜í”„ë¦¬ë¯¸ì—„', 'ì—­í”„ë¦¬ë¯¸ì—„'
            ],
            'levels': [
                # ê¸°ë³¸ ìˆ˜ì¹˜
                '30', '70', '50', '20', '80', '0.618', '0.382', '1.618',
                '0.236', '0.5', '0.786', '1.0', '1.272', '1.414', '2.0',
                # í•œêµ­ì–´ ë³€í˜•
                '30%', '70%', '50%', '20%', '80%',
                '0.618', '0.382', '1.618', 'í™©ê¸ˆë¹„ìœ¨',
                'í”¼ë³´ë‚˜ì¹˜ë ˆë²¨', 'í”¼ë³´ë‚˜ì¹˜ìˆ˜ì—´', 'í™©ê¸ˆë¹„ìœ¨'
            ]
        }
    
    def check_pdf_exists(self) -> bool:
        """PDF íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        if self.pdf_path.exists():
            print(f"âœ… PDF íŒŒì¼ ë°œê²¬: {self.pdf_path}")
            print(f"   íŒŒì¼ í¬ê¸°: {self.pdf_path.stat().st_size / 1024:.1f} KB")
            return True
        else:
            print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.pdf_path}")
            return False
    
    def load_pdf_with_pypdf(self) -> Optional[List]:
        """PyPDFLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ PDF ë¡œë“œ"""
        try:
            print(f"\nğŸ“„ PyPDFLoaderë¡œ PDF ë¡œë“œ ì¤‘...")
            print(f"   íŒŒì¼ ê²½ë¡œ: {self.pdf_path}")
            
            loader = PyPDFLoader(str(self.pdf_path))
            docs = loader.load()
            
            print(f"   âœ… PDF ë¡œë“œ ì„±ê³µ!")
            print(f"   ğŸ“Š ë¬¸ì„œ ì •ë³´:")
            print(f"      - ì´ ë¬¸ì„œ ìˆ˜: {len(docs)}ê°œ")
            
            for i, doc in enumerate(docs[:3]):  # ì²˜ìŒ 3ê°œ ë¬¸ì„œë§Œ í‘œì‹œ
                print(f"      - ë¬¸ì„œ {i+1}: {len(doc.page_content)}ì")
                if doc.metadata:
                    print(f"        ë©”íƒ€ë°ì´í„°: {doc.metadata}")
            
            if len(docs) > 3:
                print(f"      - ... ì™¸ {len(docs) - 3}ê°œ ë¬¸ì„œ")
            
            return docs
            
        except Exception as e:
            print(f"   âŒ PDF ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def extract_technical_indicators_improved(self, content: str) -> Dict[str, Any]:
        """ê°œì„ ëœ ê¸°ìˆ ì  ë¶„ì„ ì§€í‘œ ì¶”ì¶œ"""
        extracted_info = {
            'indicators_found': [],
            'patterns_found': [],
            'concepts_found': [],
            'numeric_levels': [],
            'analysis_summary': {}
        }
        
        content_lower = content.lower()
        
        # ì§€í‘œ ê²€ìƒ‰ (ë” ì •í™•í•œ ë§¤ì¹­)
        for category, keywords in self.technical_keywords.items():
            found_items = []
            for keyword in keywords:
                # ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­ (ë¶€ë¶„ ë¬¸ìì—´ì´ ì•„ë‹Œ)
                pattern = r'\b' + re.escape(keyword.lower()) + r'\b'
                if re.search(pattern, content_lower) or keyword in content:
                    found_items.append(keyword)
            
            if category == 'levels':
                extracted_info['numeric_levels'] = found_items
            else:
                extracted_info[f'{category[:-1]}_found'] = found_items
        
        # ì¶”ê°€ íŒ¨í„´ ê²€ìƒ‰ (ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©)
        additional_patterns = {
            'indicators': [
                r'RSI\s*\([^)]*\)',  # RSI (Relative Strength Index)
                r'MACD\s*\([^)]*\)',  # MACD (...)
                r'ë³¼ë¦°ì €\s*ë°´ë“œ',  # ë³¼ë¦°ì € ë°´ë“œ
                r'ì´ë™í‰ê· \s*ì„ ',  # ì´ë™í‰ê·  ì„ 
                r'ìŠ¤í† ìºìŠ¤í‹±',  # ìŠ¤í† ìºìŠ¤í‹±
                r'íŒŒë¼ë³¼ë¦­\s*SAR',  # íŒŒë¼ë³¼ë¦­ SAR
                r'ì¼ëª©\s*ê· í˜•í‘œ',  # ì¼ëª© ê· í˜•í‘œ
                r'í”¼ë³´ë‚˜ì¹˜\s*ë˜ëŒë¦¼',  # í”¼ë³´ë‚˜ì¹˜ ë˜ëŒë¦¼
                r'ì—˜ë¦¬ì–´íŠ¸\s*íŒŒë™'  # ì—˜ë¦¬ì–´íŠ¸ íŒŒë™
            ],
            'concepts': [
                r'ì§€ì§€\s*ì„ ',  # ì§€ì§€ ì„ 
                r'ì €í•­\s*ì„ ',  # ì €í•­ ì„ 
                r'ì¶”ì„¸\s*ì„ ',  # ì¶”ì„¸ ì„ 
                r'ê³¼ë§¤ìˆ˜',  # ê³¼ë§¤ìˆ˜
                r'ê³¼ë§¤ë„',  # ê³¼ë§¤ë„
                r'ë‹¤ì´ë²„ì „ìŠ¤',  # ë‹¤ì´ë²„ì „ìŠ¤
                r'ê³¨ë“ \s*í¬ë¡œìŠ¤',  # ê³¨ë“  í¬ë¡œìŠ¤
                r'ë°ë“œ\s*í¬ë¡œìŠ¤',  # ë°ë“œ í¬ë¡œìŠ¤
                r'ê±°ë˜ëŸ‰',  # ê±°ë˜ëŸ‰
                r'ë§¤ë¬¼ëŒ€'  # ë§¤ë¬¼ëŒ€
            ]
        }
        
        # ì¶”ê°€ íŒ¨í„´ ê²€ìƒ‰
        for category, patterns in additional_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if category == 'indicators' and match not in extracted_info['indicators_found']:
                        extracted_info['indicators_found'].append(match)
                    elif category == 'concepts' and match not in extracted_info['concepts_found']:
                        extracted_info['concepts_found'].append(match)
        
        # ë¶„ì„ ìš”ì•½ ìƒì„±
        total_indicators = len(extracted_info['indicators_found'])
        total_patterns = len(extracted_info['patterns_found'])
        total_concepts = len(extracted_info['concepts_found'])
        
        extracted_info['analysis_summary'] = {
            'total_indicators': total_indicators,
            'total_patterns': total_patterns,
            'total_concepts': total_concepts,
            'content_quality_score': min(100, (total_indicators * 10) + (total_patterns * 5) + (total_concepts * 3)),
            'main_focus': self._determine_main_focus(extracted_info)
        }
        
        return extracted_info
    
    def _determine_main_focus(self, extracted_info: Dict) -> str:
        """ì£¼ìš” í¬ì»¤ìŠ¤ ê²°ì •"""
        indicators = extracted_info['indicators_found']
        concepts = extracted_info['concepts_found']
        
        # RSI ê´€ë ¨
        if any('RSI' in ind or 'ìƒëŒ€ê°•ë„' in ind for ind in indicators):
            if any('MACD' in ind or 'ì´ë™í‰ê· ìˆ˜ë ´' in ind for ind in indicators):
                return "RSI + MACD ë³µí•© ë¶„ì„"
            return "RSI ì¤‘ì‹¬ ë¶„ì„"
        
        # MACD ê´€ë ¨
        elif any('MACD' in ind or 'ì´ë™í‰ê· ìˆ˜ë ´' in ind for ind in indicators):
            return "MACD ì¤‘ì‹¬ ë¶„ì„"
        
        # ë³¼ë¦°ì €ë°´ë“œ ê´€ë ¨
        elif any('ë³¼ë¦°ì €' in ind or 'Bollinger' in ind for ind in indicators):
            return "ë³¼ë¦°ì €ë°´ë“œ ì¤‘ì‹¬ ë¶„ì„"
        
        # ì´ë™í‰ê· ì„  ê´€ë ¨
        elif any('ì´ë™í‰ê· ' in ind or 'ì´í‰' in ind for ind in indicators):
            return "ì´ë™í‰ê· ì„  ì¤‘ì‹¬ ë¶„ì„"
        
        # ì—˜ë¦¬ì–´íŠ¸ íŒŒë™ ê´€ë ¨
        elif any('ì—˜ë¦¬ì–´íŠ¸' in ind or 'íŒŒë™' in ind for ind in indicators):
            return "ì—˜ë¦¬ì–´íŠ¸ íŒŒë™ ì´ë¡ "
        
        # ì¼ëª©ê· í˜•í‘œ ê´€ë ¨
        elif any('ì¼ëª©' in ind or 'ê· í˜•í‘œ' in ind for ind in indicators):
            return "ì¼ëª©ê· í˜•í‘œ ë¶„ì„"
        
        # í”¼ë³´ë‚˜ì¹˜ ê´€ë ¨
        elif any('í”¼ë³´ë‚˜ì¹˜' in ind for ind in indicators):
            return "í”¼ë³´ë‚˜ì¹˜ ë˜ëŒë¦¼ ë¶„ì„"
        
        # ê±°ë˜ëŸ‰ ê´€ë ¨
        elif any('ê±°ë˜ëŸ‰' in concept for concept in concepts):
            return "ê±°ë˜ëŸ‰ ë¶„ì„"
        
        # ì¶”ì„¸ ê´€ë ¨
        elif any('ì¶”ì„¸' in concept for concept in concepts):
            return "ì¶”ì„¸ ë¶„ì„"
        
        else:
            return "ì¼ë°˜ ê¸°ìˆ ì  ë¶„ì„"
    
    def analyze_pdf_content(self, docs: List) -> Dict[str, Any]:
        """PDF ë‚´ìš© ì¢…í•© ë¶„ì„"""
        print(f"\nğŸ” PDF ë‚´ìš© ë¶„ì„ ì¤‘...")
        
        # ëª¨ë“  ë¬¸ì„œ ë‚´ìš© í•©ì¹˜ê¸°
        full_content = ""
        for i, doc in enumerate(docs):
            full_content += f"\n--- í˜ì´ì§€ {i+1} ---\n"
            full_content += doc.page_content
            full_content += "\n"
        
        print(f"   ğŸ“ ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_content):,}ì")
        
        # ê°œì„ ëœ ê¸°ìˆ ì  ë¶„ì„ ì§€í‘œ ì¶”ì¶œ
        analysis_results = self.extract_technical_indicators_improved(full_content)
        
        return {
            'full_content': full_content,
            'analysis_results': analysis_results,
            'document_count': len(docs)
        }
    
    def generate_detailed_report(self, analysis_data: Dict):
        """ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"\nğŸ“ˆ ê¸°ìˆ ì  ë¶„ì„ PDF ìƒì„¸ ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        # ê¸°ë³¸ ì •ë³´
        print(f"\nğŸ“„ PDF ê¸°ë³¸ ì •ë³´:")
        print(f"   â€¢ íŒŒì¼ëª…: {self.pdf_path.name}")
        print(f"   â€¢ ë¬¸ì„œ ìˆ˜: {analysis_data['document_count']}ê°œ")
        print(f"   â€¢ ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(analysis_data['full_content']):,}ì")
        
        # ê¸°ìˆ ì  ë¶„ì„ ë‚´ìš© ì¶”ì¶œ ê²°ê³¼
        analysis_results = analysis_data['analysis_results']
        summary = analysis_results['analysis_summary']
        
        print(f"\nğŸ” ê¸°ìˆ ì  ë¶„ì„ ë‚´ìš© ì¶”ì¶œ ê²°ê³¼:")
        print(f"   â€¢ ë°œê²¬ëœ ê¸°ìˆ  ì§€í‘œ: {summary['total_indicators']}ê°œ")
        if analysis_results['indicators_found']:
            print(f"     â†’ {', '.join(analysis_results['indicators_found'][:10])}")
            if len(analysis_results['indicators_found']) > 10:
                print(f"     â†’ ... ì™¸ {len(analysis_results['indicators_found']) - 10}ê°œ")
        
        print(f"   â€¢ ë°œê²¬ëœ íŒ¨í„´: {summary['total_patterns']}ê°œ")
        if analysis_results['patterns_found']:
            print(f"     â†’ {', '.join(analysis_results['patterns_found'])}")
        
        print(f"   â€¢ ë°œê²¬ëœ ê°œë…: {summary['total_concepts']}ê°œ")
        if analysis_results['concepts_found']:
            print(f"     â†’ {', '.join(analysis_results['concepts_found'][:10])}")
            if len(analysis_results['concepts_found']) > 10:
                print(f"     â†’ ... ì™¸ {len(analysis_results['concepts_found']) - 10}ê°œ")
        
        print(f"   â€¢ ì£¼ìš” í¬ì»¤ìŠ¤: {summary['main_focus']}")
        print(f"   â€¢ ë‚´ìš© í’ˆì§ˆ ì ìˆ˜: {summary['content_quality_score']}/100")
        
        # í’ˆì§ˆ ë“±ê¸‰
        score = summary['content_quality_score']
        if score >= 80:
            grade = "ğŸ¥‡ ìµœìš°ìˆ˜ (ì „ë¬¸ ìˆ˜ì¤€)"
        elif score >= 60:
            grade = "ğŸ¥ˆ ìš°ìˆ˜ (ì‹¤ìš© ìˆ˜ì¤€)"
        elif score >= 40:
            grade = "ğŸ¥‰ ì–‘í˜¸ (ê¸°ì´ˆ ìˆ˜ì¤€)"
        else:
            grade = "ğŸ“ ê¸°ë³¸ (ì…ë¬¸ ìˆ˜ì¤€)"
        
        print(f"   â€¢ í’ˆì§ˆ ë“±ê¸‰: {grade}")
        
        # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
        print(f"\nğŸ“– ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì):")
        preview = analysis_data['full_content'][:500]
        print(f"   {preview}...")
        
        return analysis_results
    
    def save_analysis_results(self, analysis_data: Dict, output_file: str = "improved_technical_analysis_results.txt"):
        """ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write("ê°œì„ ëœ ê¸°ìˆ ì  ë¶„ì„ PDF ì²˜ë¦¬ ê²°ê³¼\n")
                f.write("=" * 50 + "\n\n")
                
                f.write(f"íŒŒì¼ëª…: {self.pdf_path.name}\n")
                f.write(f"ì²˜ë¦¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ë¬¸ì„œ ìˆ˜: {analysis_data['document_count']}ê°œ\n")
                f.write(f"ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(analysis_data['full_content']):,}ì\n\n")
                
                analysis_results = analysis_data['analysis_results']
                summary = analysis_results['analysis_summary']
                
                f.write("ë°œê²¬ëœ ê¸°ìˆ  ì§€í‘œ:\n")
                for indicator in analysis_results['indicators_found']:
                    f.write(f"  - {indicator}\n")
                f.write("\n")
                
                f.write("ë°œê²¬ëœ íŒ¨í„´:\n")
                for pattern in analysis_results['patterns_found']:
                    f.write(f"  - {pattern}\n")
                f.write("\n")
                
                f.write("ë°œê²¬ëœ ê°œë…:\n")
                for concept in analysis_results['concepts_found']:
                    f.write(f"  - {concept}\n")
                f.write("\n")
                
                f.write(f"ì£¼ìš” í¬ì»¤ìŠ¤: {summary['main_focus']}\n")
                f.write(f"í’ˆì§ˆ ì ìˆ˜: {summary['content_quality_score']}/100\n\n")
                
                f.write("ì „ì²´ ë‚´ìš©:\n")
                f.write("-" * 30 + "\n")
                f.write(analysis_data['full_content'])
            
            print(f"   âœ… ë¶„ì„ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"   âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def process_pdf(self):
        """PDF ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        print("ğŸš€ ê°œì„ ëœ PDF ì²˜ë¦¬ ì‹œì‘")
        print("=" * 60)
        
        # 1. PDF íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not self.check_pdf_exists():
            return None
        
        # 2. PyPDFLoaderë¡œ PDF ë¡œë“œ
        docs = self.load_pdf_with_pypdf()
        if not docs:
            print("âŒ PDF ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
        
        # 3. PDF ë‚´ìš© ë¶„ì„
        analysis_data = self.analyze_pdf_content(docs)
        
        # 4. ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
        analysis_results = self.generate_detailed_report(analysis_data)
        
        # 5. ê²°ê³¼ ì €ì¥
        self.save_analysis_results(analysis_data)
        
        return analysis_data

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    processor = ImprovedPDFProcessor()
    result = processor.process_pdf()
    
    if result:
        print(f"\nğŸ‰ ê°œì„ ëœ PDF ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ğŸ“Š ì´ {result['document_count']}ê°œ ë¬¸ì„œ ì²˜ë¦¬")
        print(f"   ğŸ“ {len(result['full_content']):,}ì í…ìŠ¤íŠ¸ ì¶”ì¶œ")
        print(f"   ğŸ” {result['analysis_results']['analysis_summary']['total_indicators']}ê°œ ê¸°ìˆ  ì§€í‘œ ë°œê²¬")
        print(f"   ğŸ¯ {result['analysis_results']['analysis_summary']['main_focus']}")
    else:
        print(f"\nâŒ PDF ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 