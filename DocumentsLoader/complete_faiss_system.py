# ì™„ì „í•œ FAISS HNSW ì‹œìŠ¤í…œ - ì‹¤ì œ PDF ë°ì´í„° í™œìš© (HNSW ëª…ì‹œì  êµ¬í˜„ + Gemini ì„ë² ë”©)

import os
import json
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import faiss

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# ê¸°ì¡´ PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ ì„í¬íŠ¸
from process_technical_analysis_pdf_improved import ImprovedPDFProcessor

print("ğŸ”§ ì™„ì „í•œ FAISS HNSW ì‹œìŠ¤í…œ ì´ˆê¸°í™” (HNSW ëª…ì‹œì  êµ¬í˜„ + Gemini ì„ë² ë”©)")
print("=" * 80)

class CompleteFAISSSystem:
    """ì‹¤ì œ PDF ë°ì´í„°ë¥¼ í™œìš©í•œ ì™„ì „í•œ FAISS HNSW ì‹œìŠ¤í…œ (HNSW ëª…ì‹œì  êµ¬í˜„ + Gemini ì„ë² ë”© ì§€ì›)"""
    
    def __init__(self, embedding_type: str = "huggingface"):
        """
        ì´ˆê¸°í™”
        Args:
            embedding_type: "huggingface" ë˜ëŠ” "gemini"
        """
        self.pdf_path = Path("DocumentsLoader/data/ê¸°ìˆ ì ì°¨íŠ¸ë¶„ì„ì´ë¡ ë°ë°©ë²•.pdf")
        self.index_dir = Path("DocumentsLoader/faiss_index_complete")
        self.index_dir.mkdir(exist_ok=True)
        
        # í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì • (ì‹¤ì œ PDFì— ìµœì í™”)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        self.embedding_type = embedding_type
        if embedding_type == "gemini":
            try:
                self.embeddings = GoogleGenerativeAIEmbeddings(
                    model="models/embedding-001",
                    google_api_key=os.getenv("GOOGLE_API_KEY")
                )
                print("   âœ… Gemini ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"   âš ï¸ Gemini ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨, HuggingFaceë¡œ ëŒ€ì²´: {str(e)}")
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                    model_kwargs={'device': 'cpu'},
                    encode_kwargs={'normalize_embeddings': True}
                )
                self.embedding_type = "huggingface"
        else:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            print("   âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        self.faiss_index = None
        self.pdf_processor = ImprovedPDFProcessor()
        
        # HNSW ì„¤ì •
        self.hnsw_config = {
            'M': 16,  # ê° ë…¸ë“œì˜ ìµœëŒ€ ì—°ê²° ìˆ˜
            'efConstruction': 200,  # êµ¬ì¶• ì‹œ íƒìƒ‰í•  ì´ì›ƒ ìˆ˜
            'efSearch': 50,  # ê²€ìƒ‰ ì‹œ íƒìƒ‰í•  ì´ì›ƒ ìˆ˜
            'metric': faiss.METRIC_INNER_PRODUCT  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        }
        
        # ê¸°ìˆ ì  ë¶„ì„ ê²€ìƒ‰ ìµœì í™” í‚¤ì›Œë“œ
        self.search_keywords = {
            'RSI': ['RSI', 'ìƒëŒ€ê°•ë„ì§€ìˆ˜', 'ìƒëŒ€ê°•ë„', 'ê³¼ë§¤ìˆ˜', 'ê³¼ë§¤ë„', '70', '30'],
            'MACD': ['MACD', 'ì´ë™í‰ê· ìˆ˜ë ´í™•ì‚°', 'ì´ë™í‰ê· ìˆ˜ë ´', 'ê³¨ë“ í¬ë¡œìŠ¤', 'ë°ë“œí¬ë¡œìŠ¤'],
            'ë³¼ë¦°ì €ë°´ë“œ': ['ë³¼ë¦°ì €ë°´ë“œ', 'ë³¼ë¦°ì €', 'ë³€ë™ì„±', 'ë°´ë“œ', 'í‘œì¤€í¸ì°¨'],
            'ì´ë™í‰ê· ì„ ': ['ì´ë™í‰ê· ì„ ', 'ì´í‰ì„ ', 'ì´ë™í‰ê· ', 'ì´í‰', 'ì¶”ì„¸'],
            'ìŠ¤í† ìºìŠ¤í‹±': ['ìŠ¤í† ìºìŠ¤í‹±', 'ì˜¤ì‹¤ë ˆì´í„°', '%K', '%D'],
            'ì¼ëª©ê· í˜•í‘œ': ['ì¼ëª©ê· í˜•í‘œ', 'ì¼ëª©ê· í˜•', 'êµ¬ë¦„ëŒ€', 'ì‹œê°„ë¡ ', 'ê°€ê²©ë¡ '],
            'í”¼ë³´ë‚˜ì¹˜': ['í”¼ë³´ë‚˜ì¹˜', 'ë˜ëŒë¦¼', '23.6', '38.2', '61.8', 'í™©ê¸ˆë¹„ìœ¨'],
            'ì—˜ë¦¬ì–´íŠ¸': ['ì—˜ë¦¬ì–´íŠ¸', 'íŒŒë™ì´ë¡ ', 'íŒŒë™', 'ìƒìŠ¹íŒŒ', 'í•˜ë½íŒŒ'],
            'ì§€ì§€ì €í•­': ['ì§€ì§€ì„ ', 'ì €í•­ì„ ', 'ì§€ì§€', 'ì €í•­', 'ì§€ì§€ëŒ€', 'ì €í•­ëŒ€'],
            'ê±°ë˜ëŸ‰': ['ê±°ë˜ëŸ‰', 'ê±°ë˜', 'í™œì„±ë„', 'ë§¤ë¬¼ëŒ€', 'ì„¸ë ¥í™œë™']
        }
    
    def load_pdf_documents(self) -> List[Document]:
        """PDF ë¬¸ì„œ ë¡œë“œ ë° LangChain Document ë³€í™˜"""
        print("ğŸ“„ PDF ë¬¸ì„œ ë¡œë“œ ì¤‘...")
        
        try:
            # PDF ì¡´ì¬ í™•ì¸
            if not self.pdf_processor.check_pdf_exists():
                print("âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # PDF ë¡œë“œ
            docs = self.pdf_processor.load_pdf_with_pypdf()
            if not docs:
                print("âŒ PDF ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return []
            
            # LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            langchain_docs = []
            for i, doc in enumerate(docs):
                # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                metadata = {
                    'source': str(self.pdf_path),
                    'page': i + 1,
                    'total_pages': len(docs),
                    'content_type': 'technical_analysis',
                    'language': 'ko',
                    'domain': 'stock_analysis',
                    'embedding_type': self.embedding_type,
                    'file_size': self.pdf_path.stat().st_size,
                    'processing_time': datetime.now().isoformat()
                }
                
                # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë³‘í•©
                if hasattr(doc, 'metadata') and doc.metadata:
                    metadata.update(doc.metadata)
                
                # LangChain Document ìƒì„±
                langchain_doc = Document(
                    page_content=doc.page_content,
                    metadata=metadata
                )
                langchain_docs.append(langchain_doc)
            
            print(f"   âœ… {len(langchain_docs)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
            return langchain_docs
            
        except Exception as e:
            print(f"   âŒ PDF ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def split_and_process_documents(self, documents: List[Document]) -> List[Document]:
        """ë¬¸ì„œ ë¶„í•  ë° ì „ì²˜ë¦¬"""
        print("âœ‚ï¸ ë¬¸ì„œ ë¶„í•  ë° ì „ì²˜ë¦¬ ì¤‘...")
        
        try:
            # ë¬¸ì„œ ë¶„í• 
            split_docs = self.text_splitter.split_documents(documents)
            
            # ì²­í¬ë³„ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for i, doc in enumerate(split_docs):
                doc.metadata.update({
                    'chunk_id': i,
                    'chunk_size': len(doc.page_content),
                    'chunk_processing_time': datetime.now().isoformat(),
                    'has_technical_content': self._check_technical_content(doc.page_content),
                    'embedding_type': self.embedding_type
                })
            
            print(f"   âœ… {len(split_docs)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
            print(f"   ğŸ“Š í‰ê·  ì²­í¬ í¬ê¸°: {sum(len(doc.page_content) for doc in split_docs) // len(split_docs)}ì")
            
            # ê¸°ìˆ ì  ë¶„ì„ ë‚´ìš©ì´ í¬í•¨ëœ ì²­í¬ ìˆ˜ ê³„ì‚°
            technical_chunks = sum(1 for doc in split_docs if doc.metadata.get('has_technical_content', False))
            print(f"   ğŸ” ê¸°ìˆ ì  ë¶„ì„ ë‚´ìš© í¬í•¨ ì²­í¬: {technical_chunks}ê°œ")
            
            return split_docs
            
        except Exception as e:
            print(f"   âŒ ë¬¸ì„œ ë¶„í•  ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _check_technical_content(self, content: str) -> bool:
        """ê¸°ìˆ ì  ë¶„ì„ ë‚´ìš© í¬í•¨ ì—¬ë¶€ í™•ì¸"""
        technical_terms = [
            'RSI', 'MACD', 'ë³¼ë¦°ì €', 'ì´ë™í‰ê· ', 'ìŠ¤í† ìºìŠ¤í‹±', 'ì¼ëª©ê· í˜•í‘œ',
            'í”¼ë³´ë‚˜ì¹˜', 'ì—˜ë¦¬ì–´íŠ¸', 'ì§€ì§€ì„ ', 'ì €í•­ì„ ', 'ê±°ë˜ëŸ‰', 'ì¶”ì„¸',
            'ê³¼ë§¤ìˆ˜', 'ê³¼ë§¤ë„', 'ê³¨ë“ í¬ë¡œìŠ¤', 'ë°ë“œí¬ë¡œìŠ¤', 'ë‹¤ì´ë²„ì „ìŠ¤'
        ]
        
        content_lower = content.lower()
        return any(term.lower() in content_lower for term in technical_terms)
    
    def create_faiss_index(self, documents: List[Document]) -> bool:
        """FAISS HNSW ì¸ë±ìŠ¤ ìƒì„± (ëª…ì‹œì  HNSW êµ¬í˜„)"""
        print("ğŸ” FAISS HNSW ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        print(f"   ğŸ“Š ì„ë² ë”© ëª¨ë¸: {self.embedding_type}")
        print(f"   ğŸ”§ HNSW ì„¤ì •: M={self.hnsw_config['M']}, efConstruction={self.hnsw_config['efConstruction']}")
        
        try:
            # ì„ë² ë”© ìƒì„±
            print("   ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings_list = []
            for doc in documents:
                embedding = self.embeddings.embed_query(doc.page_content)
                embeddings_list.append(embedding)
            
            # FAISS ì¸ë±ìŠ¤ ìƒì„± (HNSW ëª…ì‹œì  êµ¬í˜„)
            dimension = len(embeddings_list[0])
            print(f"   ğŸ“ ì„ë² ë”© ì°¨ì›: {dimension}")
            
            # HNSW ì¸ë±ìŠ¤ ìƒì„±
            index = faiss.IndexHNSWFlat(dimension, self.hnsw_config['M'])
            index.hnsw.efConstruction = self.hnsw_config['efConstruction']
            index.hnsw.efSearch = self.hnsw_config['efSearch']
            index.metric_type = self.hnsw_config['metric']
            
            # ë²¡í„° ì¶”ê°€
            embeddings_array = faiss.vector_to_array(embeddings_list)
            index.add(embeddings_array)
            
            # LangChain FAISS ë˜í¼ ìƒì„±
            self.faiss_index = FAISS(
                embedding_function=self.embeddings,
                index=index,
                docstore=self._create_docstore(documents),
                index_to_docstore_id={i: i for i in range(len(documents))}
            )
            
            # ì¸ë±ìŠ¤ ì €ì¥
            self.faiss_index.save_local(str(self.index_dir))
            
            print(f"   âœ… FAISS HNSW ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
            print(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.index_dir}")
            print(f"   ğŸ“Š ì´ ë²¡í„° ìˆ˜: {index.ntotal}")
            print(f"   ğŸ”§ HNSW ë…¸ë“œ ìˆ˜: {index.hnsw.levels.size()}")
            
            return True
            
        except Exception as e:
            print(f"   âŒ FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _create_docstore(self, documents: List[Document]):
        """ë¬¸ì„œ ì €ì¥ì†Œ ìƒì„±"""
        from langchain.docstore.document import Document as LangChainDocument
        
        docstore = {}
        for i, doc in enumerate(documents):
            docstore[i] = LangChainDocument(
                page_content=doc.page_content,
                metadata=doc.metadata
            )
        return docstore
    
    def load_existing_index(self) -> bool:
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            if (self.index_dir / "index.faiss").exists():
                print("ğŸ“‚ ê¸°ì¡´ FAISS HNSW ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
                self.faiss_index = FAISS.load_local(
                    str(self.index_dir),
                    self.embeddings
                )
                print("   âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
                
                # HNSW ì •ë³´ ì¶œë ¥
                if hasattr(self.faiss_index.index, 'hnsw'):
                    print(f"   ğŸ”§ HNSW ë…¸ë“œ ìˆ˜: {self.faiss_index.index.hnsw.levels.size()}")
                    print(f"   ğŸ”§ HNSW ìµœëŒ€ ë ˆë²¨: {self.faiss_index.index.hnsw.max_level}")
                
                return True
            else:
                print("   âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
        except Exception as e:
            print(f"   âŒ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def semantic_search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ (HNSW ìµœì í™”)"""
        if not self.faiss_index:
            print("âŒ FAISS ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            print(f"ğŸ” HNSW ì˜ë¯¸ë¡ ì  ê²€ìƒ‰: '{query}'")
            
            # HNSW ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì„¤ì •
            if hasattr(self.faiss_index.index, 'hnsw'):
                self.faiss_index.index.hnsw.efSearch = self.hnsw_config['efSearch']
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
            docs_and_scores = self.faiss_index.similarity_search_with_score(
                query, k=k
            )
            
            print(f"   âœ… {len(docs_and_scores)}ê°œ ê²°ê³¼ ë°œê²¬")
            
            # ê²°ê³¼ ì¶œë ¥
            for i, (doc, score) in enumerate(docs_and_scores, 1):
                print(f"   ğŸ“„ ê²°ê³¼ {i}:")
                print(f"      - ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}")
                print(f"      - í˜ì´ì§€: {doc.metadata.get('page', 'N/A')}")
                print(f"      - ì²­í¬ í¬ê¸°: {doc.metadata.get('chunk_size', 'N/A')}ì")
                print(f"      - ì„ë² ë”© íƒ€ì…: {doc.metadata.get('embedding_type', 'N/A')}")
                print(f"      - ê¸°ìˆ ì  ë‚´ìš©: {'âœ…' if doc.metadata.get('has_technical_content', False) else 'âŒ'}")
                print(f"      - ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:100]}...")
                print()
            
            return docs_and_scores
            
        except Exception as e:
            print(f"   âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def technical_search(self, indicator: str, k: int = 5) -> List[Tuple[Document, float]]:
        """ê¸°ìˆ ì  ë¶„ì„ íŠ¹í™” ê²€ìƒ‰ (HNSW ìµœì í™”)"""
        if indicator not in self.search_keywords:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì§€í‘œì…ë‹ˆë‹¤: {indicator}")
            return []
        
        print(f"ğŸ“ˆ HNSW ê¸°ìˆ ì  ë¶„ì„ ê²€ìƒ‰: {indicator}")
        
        # ê´€ë ¨ í‚¤ì›Œë“œë“¤ë¡œ ê²€ìƒ‰
        keywords = self.search_keywords[indicator]
        query = " ".join(keywords)
        
        print(f"   ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords)}")
        
        # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ìˆ˜í–‰
        results = self.semantic_search(query, k=k*2)  # ë” ë§ì€ ê²°ê³¼ì—ì„œ í•„í„°ë§
        
        # ê¸°ìˆ ì  ë‚´ìš©ì´ í¬í•¨ëœ ê²°ê³¼ë§Œ í•„í„°ë§
        filtered_results = []
        for doc, score in results:
            if doc.metadata.get('has_technical_content', False):
                # ê´€ë ¨ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ì¶”ê°€
                content_lower = doc.page_content.lower()
                keyword_matches = sum(1 for keyword in keywords if keyword.lower() in content_lower)
                enhanced_score = score + (keyword_matches * 0.1)
                filtered_results.append((doc, enhanced_score))
        
        # ì ìˆ˜ë¡œ ì¬ì •ë ¬
        filtered_results.sort(key=lambda x: x[1], reverse=True)
        
        print(f"   âœ… ê¸°ìˆ ì  ë¶„ì„ ê²°ê³¼: {len(filtered_results)}ê°œ")
        return filtered_results[:k]
    
    def get_index_statistics(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´"""
        if not self.faiss_index:
            return {}
        
        try:
            # ì¸ë±ìŠ¤ ì •ë³´ ìˆ˜ì§‘
            stats = {
                'embedding_type': self.embedding_type,
                'total_vectors': len(self.faiss_index.docstore._dict),
                'index_type': 'FAISS HNSW',
                'hnsw_config': self.hnsw_config,
                'index_path': str(self.index_dir),
                'created_time': datetime.now().isoformat()
            }
            
            # HNSW íŠ¹ì • ì •ë³´
            if hasattr(self.faiss_index.index, 'hnsw'):
                stats.update({
                    'hnsw_nodes': self.faiss_index.index.hnsw.levels.size(),
                    'hnsw_max_level': self.faiss_index.index.hnsw.max_level,
                    'hnsw_ef_search': self.faiss_index.index.hnsw.efSearch,
                    'hnsw_ef_construction': self.faiss_index.index.hnsw.efConstruction
                })
            
            # ë¬¸ì„œ í†µê³„
            docs = list(self.faiss_index.docstore._dict.values())
            if docs:
                technical_docs = sum(1 for doc in docs if doc.metadata.get('has_technical_content', False))
                stats.update({
                    'total_documents': len(docs),
                    'technical_documents': technical_docs,
                    'avg_document_length': sum(len(doc.page_content) for doc in docs) // len(docs),
                    'total_characters': sum(len(doc.page_content) for doc in docs),
                    'pages_covered': len(set(doc.metadata.get('page', 0) for doc in docs))
                })
            
            return stats
            
        except Exception as e:
            print(f"âŒ í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def save_search_results(self, results: List[Tuple[Document, float]], query: str, search_type: str = "semantic"):
        """ê²€ìƒ‰ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"search_results_{search_type}_{timestamp}.json"
        
        try:
            # ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            search_data = {
                'query': query,
                'search_type': search_type,
                'embedding_type': self.embedding_type,
                'hnsw_config': self.hnsw_config,
                'timestamp': datetime.now().isoformat(),
                'total_results': len(results),
                'results': []
            }
            
            for doc, score in results:
                result_item = {
                    'score': float(score),
                    'page': doc.metadata.get('page', 'N/A'),
                    'chunk_id': doc.metadata.get('chunk_id', 'N/A'),
                    'chunk_size': doc.metadata.get('chunk_size', 'N/A'),
                    'embedding_type': doc.metadata.get('embedding_type', 'N/A'),
                    'has_technical_content': doc.metadata.get('has_technical_content', False),
                    'content': doc.page_content,
                    'metadata': doc.metadata
                }
                search_data['results'].append(result_item)
            
            # íŒŒì¼ ì €ì¥
            output_path = Path("DocumentsLoader/search_results") / filename
            output_path.parent.mkdir(exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(search_data, f, ensure_ascii=False, indent=2)
            
            print(f"   âœ… ê²€ìƒ‰ ê²°ê³¼ ì €ì¥: {output_path}")
            
        except Exception as e:
            print(f"   âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def build_index(self) -> bool:
        """ì „ì²´ ì¸ë±ìŠ¤ êµ¬ì¶• í”„ë¡œì„¸ìŠ¤"""
        print("ğŸš€ ì™„ì „í•œ FAISS HNSW ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘")
        print(f"ğŸ“Š ì„ë² ë”© ëª¨ë¸: {self.embedding_type}")
        print("=" * 80)
        
        # 1. ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
        if self.load_existing_index():
            print("âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚¬ìš© ê°€ëŠ¥")
            return True
        
        # 2. PDF ë¬¸ì„œ ë¡œë“œ
        documents = self.load_pdf_documents()
        if not documents:
            return False
        
        # 3. ë¬¸ì„œ ë¶„í•  ë° ì „ì²˜ë¦¬
        split_docs = self.split_and_process_documents(documents)
        if not split_docs:
            return False
        
        # 4. FAISS ì¸ë±ìŠ¤ ìƒì„±
        success = self.create_faiss_index(split_docs)
        
        if success:
            # 5. í†µê³„ ì •ë³´ ì¶œë ¥
            stats = self.get_index_statistics()
            print(f"\nğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
            for key, value in stats.items():
                print(f"   â€¢ {key}: {value}")
        
        return success
    
    def run_demo(self):
        """ë°ëª¨ ê²€ìƒ‰ ì‹¤í–‰"""
        print("\nğŸ§ª HNSW ë°ëª¨ ê²€ìƒ‰ ì‹¤í–‰")
        print("=" * 50)
        
        # 1. RSI ê²€ìƒ‰
        print("\n1. RSI ê¸°ìˆ ì  ë¶„ì„ ê²€ìƒ‰:")
        rsi_results = self.technical_search("RSI")
        if rsi_results:
            self.save_search_results(rsi_results, "RSI", "technical")
        
        # 2. MACD ê²€ìƒ‰
        print("\n2. MACD ê¸°ìˆ ì  ë¶„ì„ ê²€ìƒ‰:")
        macd_results = self.technical_search("MACD")
        if macd_results:
            self.save_search_results(macd_results, "MACD", "technical")
        
        # 3. ë³¼ë¦°ì €ë°´ë“œ ê²€ìƒ‰
        print("\n3. ë³¼ë¦°ì €ë°´ë“œ ê¸°ìˆ ì  ë¶„ì„ ê²€ìƒ‰:")
        bb_results = self.technical_search("ë³¼ë¦°ì €ë°´ë“œ")
        if bb_results:
            self.save_search_results(bb_results, "ë³¼ë¦°ì €ë°´ë“œ", "technical")
        
        # 4. ì´ë™í‰ê· ì„  ê²€ìƒ‰
        print("\n4. ì´ë™í‰ê· ì„  ê¸°ìˆ ì  ë¶„ì„ ê²€ìƒ‰:")
        ma_results = self.technical_search("ì´ë™í‰ê· ì„ ")
        if ma_results:
            self.save_search_results(ma_results, "ì´ë™í‰ê· ì„ ", "technical")
        
        # 5. ìŠ¤í† ìºìŠ¤í‹± ê²€ìƒ‰
        print("\n5. ìŠ¤í† ìºìŠ¤í‹± ê¸°ìˆ ì  ë¶„ì„ ê²€ìƒ‰:")
        stoch_results = self.technical_search("ìŠ¤í† ìºìŠ¤í‹±")
        if stoch_results:
            self.save_search_results(stoch_results, "ìŠ¤í† ìºìŠ¤í‹±", "technical")
        
        # 6. ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ì˜ˆì‹œ
        print("\n6. ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ì˜ˆì‹œ:")
        semantic_results = self.semantic_search("ê¸°ìˆ ì  ë¶„ì„ ì§€í‘œì˜ í™œìš© ë°©ë²•")
        if semantic_results:
            self.save_search_results(semantic_results, "ê¸°ìˆ ì  ë¶„ì„ ì§€í‘œì˜ í™œìš© ë°©ë²•", "semantic")
    
    def interactive_search(self):
        """ëŒ€í™”í˜• ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤"""
        print("\nğŸ” HNSW ëŒ€í™”í˜• ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤")
        print("=" * 50)
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:")
        print("  - 'search <ê²€ìƒ‰ì–´>': ì˜ë¯¸ë¡ ì  ê²€ìƒ‰")
        print("  - 'technical <ì§€í‘œëª…>': ê¸°ìˆ ì  ë¶„ì„ ê²€ìƒ‰")
        print("  - 'stats': ì¸ë±ìŠ¤ í†µê³„")
        print("  - 'quit': ì¢…ë£Œ")
        print(f"\ní˜„ì¬ ì„ë² ë”© ëª¨ë¸: {self.embedding_type}")
        print("\nì§€ì›í•˜ëŠ” ì§€í‘œ: RSI, MACD, ë³¼ë¦°ì €ë°´ë“œ, ì´ë™í‰ê· ì„ , ìŠ¤í† ìºìŠ¤í‹±, ì¼ëª©ê· í˜•í‘œ, í”¼ë³´ë‚˜ì¹˜, ì—˜ë¦¬ì–´íŠ¸, ì§€ì§€ì €í•­, ê±°ë˜ëŸ‰")
        print()
        
        while True:
            try:
                command = input("ê²€ìƒ‰ ëª…ë ¹ì–´ ì…ë ¥: ").strip()
                
                if command.lower() == 'quit':
                    break
                elif command.lower() == 'stats':
                    stats = self.get_index_statistics()
                    print(f"\nğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
                    for key, value in stats.items():
                        print(f"   â€¢ {key}: {value}")
                
                elif command.startswith('search '):
                    query = command[7:].strip()
                    results = self.semantic_search(query)
                    if results:
                        self.save_search_results(results, query, "semantic")
                
                elif command.startswith('technical '):
                    indicator = command[10:].strip()
                    if indicator in self.search_keywords:
                        results = self.technical_search(indicator)
                        if results:
                            self.save_search_results(results, indicator, "technical")
                    else:
                        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì§€í‘œì…ë‹ˆë‹¤: {indicator}")
                        print(f"   ì§€ì›í•˜ëŠ” ì§€í‘œ: {', '.join(self.search_keywords.keys())}")
                
                else:
                    print("âŒ ì˜ëª»ëœ ëª…ë ¹ì–´ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                
                print()
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„ë² ë”© íƒ€ì… ì„ íƒ
    embedding_type = "huggingface"
    if len(sys.argv) > 1:
        embedding_type = sys.argv[1].lower()
    
    if embedding_type not in ["huggingface", "gemini"]:
        print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© íƒ€ì…ì…ë‹ˆë‹¤. 'huggingface' ë˜ëŠ” 'gemini'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        return
    
    print(f"ğŸ”§ ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸: {embedding_type}")
    
    # ì™„ì „í•œ FAISS ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    faiss_system = CompleteFAISSSystem(embedding_type=embedding_type)
    
    # ì¸ë±ìŠ¤ êµ¬ì¶•
    if faiss_system.build_index():
        print("\nğŸ‰ ì™„ì „í•œ FAISS HNSW ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        
        # ë°ëª¨ ê²€ìƒ‰ ì‹¤í–‰
        faiss_system.run_demo()
        
        # ëŒ€í™”í˜• ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤
        print("\nğŸ” ëŒ€í™”í˜• ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤ ì‹œì‘:")
        faiss_system.interactive_search()
        
    else:
        print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶•ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 