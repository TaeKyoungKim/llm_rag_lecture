# ê°„ë‹¨í•œ FAISS HNSW í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ (HNSW ëª…ì‹œì  êµ¬í˜„ + Gemini ì„ë² ë”© ì§€ì›)

import os
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import faiss

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

print("ğŸ”§ ê°„ë‹¨í•œ FAISS HNSW í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ (HNSW ëª…ì‹œì  êµ¬í˜„ + Gemini ì„ë² ë”©)")
print("=" * 70)

class SimpleFAISSTest:
    """ê°„ë‹¨í•œ FAISS í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ (HNSW ëª…ì‹œì  êµ¬í˜„ + Gemini ì„ë² ë”© ì§€ì›)"""
    
    def __init__(self, embedding_type: str = "huggingface"):
        """
        ì´ˆê¸°í™”
        Args:
            embedding_type: "huggingface" ë˜ëŠ” "gemini"
        """
        self.index_dir = Path("DocumentsLoader/faiss_index")
        self.index_dir.mkdir(exist_ok=True)
        
        # í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        self.embedding_type = embedding_type
        if embedding_type == "gemini":
            try:
                self.embeddings = GoogleGenerativeAIEmbeddings(
                    model="models/embedding-001",
                    google_api_key=os.getenv("GOOGLE_API_KEY")
                )
                print("   âœ… Gemini ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"   âš ï¸ Gemini ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨, HuggingFaceë¡œ ëŒ€ì²´: {str(e)}")
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                    model_kwargs={'device': 'cpu'},
                    encode_kwargs={'normalize_embeddings': True}
                )
                self.embedding_type = "huggingface"
        else:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            print("   âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        self.faiss_index = None
        
        # HNSW ì„¤ì •
        self.hnsw_config = {
            'M': 16,  # ê° ë…¸ë“œì˜ ìµœëŒ€ ì—°ê²° ìˆ˜
            'efConstruction': 200,  # êµ¬ì¶• ì‹œ íƒìƒ‰í•  ì´ì›ƒ ìˆ˜
            'efSearch': 50,  # ê²€ìƒ‰ ì‹œ íƒìƒ‰í•  ì´ì›ƒ ìˆ˜
            'metric': faiss.METRIC_INNER_PRODUCT  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        }
    
    def create_sample_documents(self) -> List[Document]:
        """ìƒ˜í”Œ ê¸°ìˆ ì  ë¶„ì„ ë¬¸ì„œ ìƒì„±"""
        print("ğŸ“ ìƒ˜í”Œ ë¬¸ì„œ ìƒì„± ì¤‘...")
        
        sample_texts = [
            "RSI(Relative Strength Index)ëŠ” ìƒëŒ€ê°•ë„ì§€ìˆ˜ë¡œ, ì£¼ê°€ì˜ ìƒìŠ¹í­ê³¼ í•˜ë½í­ì„ ë¹„êµí•˜ì—¬ ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ ìƒíƒœë¥¼ íŒë‹¨í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ 70 ì´ìƒì´ë©´ ê³¼ë§¤ìˆ˜, 30 ì´í•˜ë©´ ê³¼ë§¤ë„ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.",
            
            "MACD(Moving Average Convergence Divergence)ëŠ” ì´ë™í‰ê· ìˆ˜ë ´í™•ì‚°ì§€ìˆ˜ë¡œ, ë‘ ê°œì˜ ì´ë™í‰ê· ì„ ì˜ ì°¨ì´ë¥¼ ì´ìš©í•œ ì¶”ì„¸ ì¶”ì¢…í˜• ì§€í‘œì…ë‹ˆë‹¤. ê³¨ë“ í¬ë¡œìŠ¤ì™€ ë°ë“œí¬ë¡œìŠ¤ë¡œ ë§¤ë§¤ ì‹œì ì„ íŒë‹¨í•©ë‹ˆë‹¤.",
            
            "ë³¼ë¦°ì €ë°´ë“œëŠ” ì£¼ê°€ì˜ ë³€ë™ì„±ì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œë¡œ, ì¤‘ì‹¬ì„ (ì´ë™í‰ê· )ê³¼ ìƒí•˜í•œì„ (í‘œì¤€í¸ì°¨)ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ë°´ë“œê°€ ì¢ì•„ì§€ë©´ í° ì›€ì§ì„ì´ ì˜ˆìƒë˜ê³ , ë°´ë“œê°€ ë„“ì–´ì§€ë©´ ë³€ë™ì„±ì´ ì»¤ì§‘ë‹ˆë‹¤.",
            
            "ì´ë™í‰ê· ì„ ì€ ì¼ì • ê¸°ê°„ì˜ ì£¼ê°€ í‰ê· ì„ ì—°ê²°í•œ ì„ ìœ¼ë¡œ, ì¶”ì„¸ë¥¼ íŒŒì•…í•˜ëŠ” ê¸°ë³¸ì ì¸ ì§€í‘œì…ë‹ˆë‹¤. ë‹¨ê¸°, ì¤‘ê¸°, ì¥ê¸° ì´ë™í‰ê· ì„ ì˜ ë°°ì—´ë¡œ ë§¤ë§¤ ì‹œì ì„ íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            
            "ìŠ¤í† ìºìŠ¤í‹±ì€ ì£¼ê°€ê°€ ì¼ì • ê¸°ê°„ì˜ ê³ ê°€ì™€ ì €ê°€ ë²”ìœ„ ë‚´ì—ì„œ ì–´ëŠ ìœ„ì¹˜ì— ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì˜¤ì‹¤ë ˆì´í„°ì…ë‹ˆë‹¤. %Kì™€ %D ë‘ ì„ ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ êµ¬ê°„ì„ íŒë‹¨í•©ë‹ˆë‹¤.",
            
            "ì¼ëª©ê· í˜•í‘œëŠ” ì¼ë³¸ì˜ ì¼ëª©ì‚°ì¸ì´ ê°œë°œí•œ ê¸°ìˆ ì  ë¶„ì„ ë„êµ¬ë¡œ, ì‹œê°„ë¡ , ê°€ê²©ë¡ , íŒŒë™ë¡ , í˜•ë³´ë¡ ì˜ ë„¤ ê°€ì§€ ìš”ì†Œë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. êµ¬ë¦„ëŒ€(ì¼ëª©ê· í˜•í‘œ)ë¥¼ í†µí•´ ì§€ì§€/ì €í•­ì„ íŒë‹¨í•©ë‹ˆë‹¤.",
            
            "í”¼ë³´ë‚˜ì¹˜ ë˜ëŒë¦¼ì€ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ì´ìš©í•œ ê¸°ìˆ ì  ë¶„ì„ ë„êµ¬ë¡œ, ì£¼ê°€ì˜ ìƒìŠ¹ì´ë‚˜ í•˜ë½ í›„ ë˜ëŒë¦¼ì˜ ê¹Šì´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì£¼ìš” ë˜ëŒë¦¼ ë ˆë²¨ì€ 23.6%, 38.2%, 50%, 61.8%ì…ë‹ˆë‹¤.",
            
            "ì—˜ë¦¬ì–´íŠ¸ íŒŒë™ì´ë¡ ì€ ì£¼ê°€ì˜ ì›€ì§ì„ì´ ì¼ì •í•œ íŒ¨í„´ì„ ë°˜ë³µí•œë‹¤ëŠ” ì´ë¡ ìœ¼ë¡œ, 5ê°œì˜ ìƒìŠ¹íŒŒì™€ 3ê°œì˜ í•˜ë½íŒŒë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. íŒŒë™ì˜ íŠ¹ì„±ì„ ì´í•´í•˜ë©´ ì‹œì¥ì˜ ì „í™˜ì ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            
            "ì§€ì§€ì„ ê³¼ ì €í•­ì„ ì€ ì£¼ê°€ê°€ ìƒìŠ¹í•˜ê±°ë‚˜ í•˜ë½í•  ë•Œ ë©ˆì¶”ëŠ” ê°€ê²©ëŒ€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì§€ì§€ì„ ì€ ì£¼ê°€ê°€ í•˜ë½í•  ë•Œ ì§€ì§€ë°›ëŠ” ê°€ê²©ëŒ€ì´ê³ , ì €í•­ì„ ì€ ì£¼ê°€ê°€ ìƒìŠ¹í•  ë•Œ ì €í•­ë°›ëŠ” ê°€ê²©ëŒ€ì…ë‹ˆë‹¤.",
            
            "ê±°ë˜ëŸ‰ì€ ì£¼ì‹ì˜ ê±°ë˜ í™œì„±ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë¡œ, ì£¼ê°€ì™€ í•¨ê»˜ ë¶„ì„í•˜ë©´ ì‹œì¥ì˜ ê°•ì•½ì„ íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê±°ë˜ëŸ‰ì´ ì¦ê°€í•˜ë©´ì„œ ì£¼ê°€ê°€ ìƒìŠ¹í•˜ë©´ ê°•ì„¸ ì‹ í˜¸ë¡œ í•´ì„ë©ë‹ˆë‹¤."
        ]
        
        documents = []
        for i, text in enumerate(sample_texts):
            doc = Document(
                page_content=text,
                metadata={
                    'id': i + 1,
                    'type': 'technical_analysis',
                    'language': 'ko',
                    'embedding_type': self.embedding_type,
                    'created_time': datetime.now().isoformat()
                }
            )
            documents.append(doc)
        
        print(f"   âœ… {len(documents)}ê°œ ìƒ˜í”Œ ë¬¸ì„œ ìƒì„± ì™„ë£Œ")
        return documents
    
    def create_faiss_index(self, documents: List[Document]) -> bool:
        """FAISS HNSW ì¸ë±ìŠ¤ ìƒì„± (ëª…ì‹œì  HNSW êµ¬í˜„)"""
        print("ğŸ” FAISS HNSW ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        print(f"   ğŸ“Š ì„ë² ë”© ëª¨ë¸: {self.embedding_type}")
        print(f"   ğŸ”§ HNSW ì„¤ì •: M={self.hnsw_config['M']}, efConstruction={self.hnsw_config['efConstruction']}")
        
        try:
            # ë¬¸ì„œ ë¶„í• 
            split_docs = self.text_splitter.split_documents(documents)
            print(f"   ğŸ“Š {len(split_docs)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
            
            # ì„ë² ë”© ìƒì„±
            print("   ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings_list = []
            for doc in split_docs:
                embedding = self.embeddings.embed_query(doc.page_content)
                embeddings_list.append(embedding)
            
            # FAISS ì¸ë±ìŠ¤ ìƒì„± (HNSW ëª…ì‹œì  êµ¬í˜„)
            dimension = len(embeddings_list[0])
            print(f"   ğŸ“ ì„ë² ë”© ì°¨ì›: {dimension}")
            
            # HNSW ì¸ë±ìŠ¤ ìƒì„±
            index = faiss.IndexHNSWFlat(dimension, self.hnsw_config['M'])
            index.hnsw.efConstruction = self.hnsw_config['efConstruction']
            index.hnsw.efSearch = self.hnsw_config['efSearch']
            index.metric_type = self.hnsw_config['metric']
            
            # ë²¡í„° ì¶”ê°€
            embeddings_array = faiss.vector_to_array(embeddings_list)
            index.add(embeddings_array)
            
            # LangChain FAISS ë˜í¼ ìƒì„±
            self.faiss_index = FAISS(
                embedding_function=self.embeddings,
                index=index,
                docstore=self._create_docstore(split_docs),
                index_to_docstore_id={i: i for i in range(len(split_docs))}
            )
            
            # ì¸ë±ìŠ¤ ì €ì¥
            self.faiss_index.save_local(str(self.index_dir))
            
            print(f"   âœ… FAISS HNSW ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
            print(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.index_dir}")
            print(f"   ğŸ“Š ì´ ë²¡í„° ìˆ˜: {index.ntotal}")
            print(f"   ğŸ”§ HNSW ë…¸ë“œ ìˆ˜: {index.hnsw.levels.size()}")
            
            return True
            
        except Exception as e:
            print(f"   âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _create_docstore(self, documents: List[Document]):
        """ë¬¸ì„œ ì €ì¥ì†Œ ìƒì„±"""
        from langchain.docstore.document import Document as LangChainDocument
        
        docstore = {}
        for i, doc in enumerate(documents):
            docstore[i] = LangChainDocument(
                page_content=doc.page_content,
                metadata=doc.metadata
            )
        return docstore
    
    def load_index(self) -> bool:
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            if (self.index_dir / "index.faiss").exists():
                print("ğŸ“‚ ê¸°ì¡´ FAISS HNSW ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
                self.faiss_index = FAISS.load_local(
                    str(self.index_dir),
                    self.embeddings
                )
                print("   âœ… ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
                
                # HNSW ì •ë³´ ì¶œë ¥
                if hasattr(self.faiss_index.index, 'hnsw'):
                    print(f"   ğŸ”§ HNSW ë…¸ë“œ ìˆ˜: {self.faiss_index.index.hnsw.levels.size()}")
                    print(f"   ğŸ”§ HNSW ìµœëŒ€ ë ˆë²¨: {self.faiss_index.index.hnsw.max_level}")
                
                return True
            else:
                print("   âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
        except Exception as e:
            print(f"   âŒ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def search(self, query: str, k: int = 3) -> List[Tuple[Document, float]]:
        """ê²€ìƒ‰ ìˆ˜í–‰ (HNSW ìµœì í™”)"""
        if not self.faiss_index:
            print("âŒ ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            print(f"ğŸ” HNSW ê²€ìƒ‰: '{query}'")
            
            # HNSW ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì„¤ì •
            if hasattr(self.faiss_index.index, 'hnsw'):
                self.faiss_index.index.hnsw.efSearch = self.hnsw_config['efSearch']
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰
            results = self.faiss_index.similarity_search_with_score(query, k=k)
            
            print(f"   âœ… {len(results)}ê°œ ê²°ê³¼ ë°œê²¬")
            
            # ê²°ê³¼ ì¶œë ¥
            for i, (doc, score) in enumerate(results, 1):
                print(f"   ğŸ“„ ê²°ê³¼ {i}:")
                print(f"      - ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}")
                print(f"      - ë¬¸ì„œ ID: {doc.metadata.get('id', 'N/A')}")
                print(f"      - ì„ë² ë”© íƒ€ì…: {doc.metadata.get('embedding_type', 'N/A')}")
                print(f"      - ë‚´ìš©: {doc.page_content[:100]}...")
                print()
            
            return results
            
        except Exception as e:
            print(f"   âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def test_searches(self):
        """ë‹¤ì–‘í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª HNSW ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 50)
        
        # 1. RSI ê²€ìƒ‰
        print("\n1. RSI ê²€ìƒ‰:")
        self.search("RSI ìƒëŒ€ê°•ë„ì§€ìˆ˜")
        
        # 2. MACD ê²€ìƒ‰
        print("\n2. MACD ê²€ìƒ‰:")
        self.search("MACD ì´ë™í‰ê· ìˆ˜ë ´í™•ì‚°")
        
        # 3. ë³¼ë¦°ì €ë°´ë“œ ê²€ìƒ‰
        print("\n3. ë³¼ë¦°ì €ë°´ë“œ ê²€ìƒ‰:")
        self.search("ë³¼ë¦°ì €ë°´ë“œ ë³€ë™ì„±")
        
        # 4. ì´ë™í‰ê· ì„  ê²€ìƒ‰
        print("\n4. ì´ë™í‰ê· ì„  ê²€ìƒ‰:")
        self.search("ì´ë™í‰ê· ì„  ì¶”ì„¸")
        
        # 5. ìŠ¤í† ìºìŠ¤í‹± ê²€ìƒ‰
        print("\n5. ìŠ¤í† ìºìŠ¤í‹± ê²€ìƒ‰:")
        self.search("ìŠ¤í† ìºìŠ¤í‹± ì˜¤ì‹¤ë ˆì´í„°")
        
        # 6. ì¼ëª©ê· í˜•í‘œ ê²€ìƒ‰
        print("\n6. ì¼ëª©ê· í˜•í‘œ ê²€ìƒ‰:")
        self.search("ì¼ëª©ê· í˜•í‘œ êµ¬ë¦„ëŒ€")
        
        # 7. í”¼ë³´ë‚˜ì¹˜ ê²€ìƒ‰
        print("\n7. í”¼ë³´ë‚˜ì¹˜ ê²€ìƒ‰:")
        self.search("í”¼ë³´ë‚˜ì¹˜ ë˜ëŒë¦¼ ë ˆë²¨")
        
        # 8. ì—˜ë¦¬ì–´íŠ¸ íŒŒë™ ê²€ìƒ‰
        print("\n8. ì—˜ë¦¬ì–´íŠ¸ íŒŒë™ ê²€ìƒ‰:")
        self.search("ì—˜ë¦¬ì–´íŠ¸ íŒŒë™ì´ë¡ ")
        
        # 9. ì§€ì§€ì €í•­ ê²€ìƒ‰
        print("\n9. ì§€ì§€ì €í•­ ê²€ìƒ‰:")
        self.search("ì§€ì§€ì„  ì €í•­ì„ ")
        
        # 10. ê±°ë˜ëŸ‰ ê²€ìƒ‰
        print("\n10. ê±°ë˜ëŸ‰ ê²€ìƒ‰:")
        self.search("ê±°ë˜ëŸ‰ ë¶„ì„")
    
    def get_index_info(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ"""
        if not self.faiss_index:
            return {}
        
        try:
            info = {
                'embedding_type': self.embedding_type,
                'total_vectors': len(self.faiss_index.docstore._dict),
                'index_type': 'FAISS HNSW',
                'hnsw_config': self.hnsw_config,
                'index_path': str(self.index_dir)
            }
            
            # HNSW íŠ¹ì • ì •ë³´
            if hasattr(self.faiss_index.index, 'hnsw'):
                info.update({
                    'hnsw_nodes': self.faiss_index.index.hnsw.levels.size(),
                    'hnsw_max_level': self.faiss_index.index.hnsw.max_level,
                    'hnsw_ef_search': self.faiss_index.index.hnsw.efSearch,
                    'hnsw_ef_construction': self.faiss_index.index.hnsw.efConstruction
                })
            
            return info
            
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        print("ğŸš€ FAISS HNSW í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì‹œì‘")
        print(f"ğŸ“Š ì„ë² ë”© ëª¨ë¸: {self.embedding_type}")
        
        # 1. ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
        if not self.load_index():
            # 2. ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            print("\nğŸ“ ìƒˆ HNSW ì¸ë±ìŠ¤ ìƒì„± ì‹œì‘")
            documents = self.create_sample_documents()
            if not self.create_faiss_index(documents):
                print("âŒ ì¸ë±ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return
        
        # 3. ì¸ë±ìŠ¤ ì •ë³´ ì¶œë ¥
        info = self.get_index_info()
        print(f"\nğŸ“Š ì¸ë±ìŠ¤ ì •ë³´:")
        for key, value in info.items():
            print(f"   â€¢ {key}: {value}")
        
        # 4. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
        self.test_searches()
        
        print("\nğŸ‰ FAISS HNSW í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„ë² ë”© íƒ€ì… ì„ íƒ
    embedding_type = "huggingface"
    if len(sys.argv) > 1:
        embedding_type = sys.argv[1].lower()
    
    if embedding_type not in ["huggingface", "gemini"]:
        print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© íƒ€ì…ì…ë‹ˆë‹¤. 'huggingface' ë˜ëŠ” 'gemini'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        return
    
    print(f"ğŸ”§ ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸: {embedding_type}")
    
    test_system = SimpleFAISSTest(embedding_type=embedding_type)
    test_system.run()

if __name__ == "__main__":
    main() 