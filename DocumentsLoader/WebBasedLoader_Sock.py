# LangChain WebBaseLoader - ì£¼ì‹ ê¸°ìˆ ì  ë¶„ì„ ì‚¬ì´íŠ¸ í†µí•© ìƒ˜í”Œ ì½”ë“œ (ìˆ˜ì •ë¨)

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
# pip install -qU langchain_community beautifulsoup4 nest_asyncio pandas

import asyncio
import nest_asyncio
import pandas as pd
from langchain_community.document_loaders import WebBaseLoader
from datetime import datetime
import re
import warnings

# Jupyter í™˜ê²½ì—ì„œ asyncio ì‚¬ìš© ì‹œ í•„ìš”
nest_asyncio.apply()

# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings('ignore', category=DeprecationWarning)

# === ì£¼ì‹ ê¸°ìˆ ì  ë¶„ì„ ê´€ë ¨ ì‚¬ì´íŠ¸ URL ëª¨ìŒ ===
TECHNICAL_ANALYSIS_SITES = {
    "global": [
        "https://stockcharts.com/",
        "https://www.tradingview.com/",
        "https://www.investing.com/technical/",
        "https://www.prorealtime.com/en/",
        "https://zerodha.com/varsity/module/technical-analysis/",
        "https://www.cmcmarkets.com/en-gb/trading-guides/technical-indicators"
    ],
    "korean": [
        "https://kr.tradingview.com/",
        "https://kr.investing.com/technical/indices-indicators",
        "https://www.paxnet.co.kr/",
        "http://data.krx.co.kr/",
        "https://www.fnguide.com/"
    ],
    "blogs_and_guides": [
        "https://blog.naver.com/kihyun113/222898232520",  # RSI ì§€í‘œ í™œìš©ë²•
        "https://blog.naver.com/parkjongpir/222234341982",  # ë³´ì¡°ì§€í‘œ ì •ë¦¬
        "https://contents.premium.naver.com/yonseident/ysdent/contents/241109181740264ic",  # S+R+M ì¡°í•©
        "https://blog.okfngroup.com/content/how-to-read-the-rsi-indicator"  # RSI ì§€í‘œ ë³´ëŠ” ë²•
    ]
}

# === 1. ê¸°ë³¸ ê¸°ìˆ ì  ë¶„ì„ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ===
print("=== 1. ê¸°ë³¸ ê¸°ìˆ ì  ë¶„ì„ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ===")

def load_technical_analysis_site(url, description=""):
    """ê¸°ìˆ ì  ë¶„ì„ ì‚¬ì´íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        loader = WebBaseLoader(url)
        loader.requests_kwargs = {
            'headers': {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            },
            'timeout': 10,
            'verify': True
        }
        docs = loader.load()
        print(f"âœ… {description} ({url}): ë¡œë“œ ì„±ê³µ - {len(docs)}ê°œ ë¬¸ì„œ, {len(docs[0].page_content)}ì")
        return docs
    except Exception as e:
        print(f"âŒ {description} ({url}): ë¡œë“œ ì‹¤íŒ¨ - {str(e)}")
        return None

# ì£¼ìš” ê¸°ìˆ ì  ë¶„ì„ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸
sample_sites = [
    ("https://www.example.com/", "ì˜ˆì œ ì‚¬ì´íŠ¸"),
    ("https://httpbin.org/html", "HTML í…ŒìŠ¤íŠ¸ ì‚¬ì´íŠ¸"),
]

for url, description in sample_sites:
    docs = load_technical_analysis_site(url, description)
    if docs:
        print(f"  ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {docs[0].page_content[:100]}...")
print()

# === 2. ì£¼ì‹ ê¸°ìˆ ì  ë¶„ì„ ì •ë³´ ìˆ˜ì§‘ ì‹œìŠ¤í…œ (ìˆ˜ì •ë¨) ===
print("=== 2. ì£¼ì‹ ê¸°ìˆ ì  ë¶„ì„ ì •ë³´ ìˆ˜ì§‘ ì‹œìŠ¤í…œ ===")

class TechnicalAnalysisCollector:
    """ê¸°ìˆ ì  ë¶„ì„ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.collected_data = []
        self.failed_urls = []
    
    def extract_technical_keywords(self, content):
        """ê¸°ìˆ ì  ë¶„ì„ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = {
            'indicators': ['RSI', 'MACD', 'ë³¼ë¦°ì €ë°´ë“œ', 'ìŠ¤í† ìºìŠ¤í‹±', 'ì´ë™í‰ê· ', 'ATR', 'CCI'],
            'patterns': ['ì‚¼ê°í˜•', 'ìê¸°í˜•', 'í”Œë˜ê·¸', 'í—¤ë“œì•¤ìˆ„ë”', 'ë”ë¸”íƒ‘', 'ë”ë¸”ë°”í…€'],
            'concepts': ['ì§€ì§€ì„ ', 'ì €í•­ì„ ', 'ì¶”ì„¸ì„ ', 'ê³¼ë§¤ìˆ˜', 'ê³¼ë§¤ë„', 'ë‹¤ì´ë²„ì „ìŠ¤', 'í¬ë¡œìŠ¤']
        }
        
        found_keywords = {}
        content_lower = content.lower()
        
        for category, word_list in keywords.items():
            found_keywords[category] = []
            for keyword in word_list:
                if keyword.lower() in content_lower or keyword in content:
                    found_keywords[category].append(keyword)
        
        return found_keywords
    
    def analyze_content(self, docs, source_url):
        """ë¬¸ì„œ ë‚´ìš© ë¶„ì„"""
        if not docs:
            return None
            
        doc = docs[0]
        content = doc.page_content
        
        analysis = {
            'source': source_url,
            'title': doc.metadata.get('title', 'N/A'),
            'content_length': len(content),
            'keywords': self.extract_technical_keywords(content),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        return analysis
    
    async def collect_single_site(self, url):
        """ë‹¨ì¼ ì‚¬ì´íŠ¸ì—ì„œ ì •ë³´ ìˆ˜ì§‘ (ìˆ˜ì •ëœ ë¹„ë™ê¸° ë°©ë²•)"""
        try:
            loader = WebBaseLoader(url)
            loader.requests_kwargs = {
                'headers': {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                },
                'timeout': 15
            }
            loader.requests_per_second = 1  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            
            # aload()ëŠ” ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ì´ë¯€ë¡œ async forë¡œ ì²˜ë¦¬
            docs = []
            async for doc in loader.alazy_load():
                docs.append(doc)
            
            analysis = self.analyze_content(docs, url)
            
            if analysis:
                self.collected_data.append(analysis)
                print(f"âœ… {url}: ìˆ˜ì§‘ ì™„ë£Œ")
                return analysis
            else:
                print(f"âŒ {url}: ë¶„ì„ ì‹¤íŒ¨")
                return None
                
        except Exception as e:
            self.failed_urls.append((url, str(e)))
            print(f"âŒ {url}: ìˆ˜ì§‘ ì‹¤íŒ¨ - {str(e)[:50]}...")
            return None
    
    def collect_from_sites(self, urls, max_concurrent=3):
        """ì—¬ëŸ¬ ì‚¬ì´íŠ¸ì—ì„œ ë™ì‹œì— ì •ë³´ ìˆ˜ì§‘"""
        print(f"ğŸ“Š {len(urls)}ê°œ ì‚¬ì´íŠ¸ì—ì„œ ê¸°ìˆ ì  ë¶„ì„ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        
        # ë¹„ë™ê¸° ìˆ˜ì§‘ ì‹¤í–‰
        async def run_collection():
            tasks = [self.collect_single_site(url) for url in urls[:max_concurrent]]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return results
        
        try:
            asyncio.run(run_collection())
        except Exception as e:
            print(f"âš ï¸ ë¹„ë™ê¸° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def generate_report(self):
        """ìˆ˜ì§‘ëœ ë°ì´í„° ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.collected_data:
            print("ğŸ“‹ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“ˆ ê¸°ìˆ ì  ë¶„ì„ ì •ë³´ ìˆ˜ì§‘ ë¦¬í¬íŠ¸ (ì´ {len(self.collected_data)}ê°œ ì‚¬ì´íŠ¸)")
        print("=" * 60)
        
        # í‚¤ì›Œë“œ í†µê³„
        all_keywords = {'indicators': [], 'patterns': [], 'concepts': []}
        
        for data in self.collected_data:
            keywords = data['keywords']
            for category in all_keywords:
                all_keywords[category].extend(keywords.get(category, []))
        
        # ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œë“¤
        for category, words in all_keywords.items():
            if words:
                try:
                    word_count = pd.Series(words).value_counts()
                    print(f"\nğŸ“Š {category.upper()} í‚¤ì›Œë“œ TOP 5:")
                    for word, count in word_count.head().items():
                        print(f"  â€¢ {word}: {count}íšŒ")
                except:
                    print(f"\nğŸ“Š {category.upper()} í‚¤ì›Œë“œ: {', '.join(set(words))}")
        
        # ê°œë³„ ì‚¬ì´íŠ¸ ì •ë³´
        print(f"\nğŸ” ìˆ˜ì§‘ëœ ì‚¬ì´íŠ¸ë³„ ìƒì„¸ ì •ë³´:")
        for i, data in enumerate(self.collected_data, 1):
            print(f"\n{i}. {data['title'][:50]}...")
            print(f"   URL: {data['source']}")
            print(f"   ë‚´ìš© ê¸¸ì´: {data['content_length']:,}ì")
            print(f"   ìˆ˜ì§‘ ì‹œê°„: {data['timestamp']}")
            
            # ë°œê²¬ëœ í‚¤ì›Œë“œ ìš”ì•½
            keywords = data['keywords']
            total_keywords = sum(len(words) for words in keywords.values())
            if total_keywords > 0:
                print(f"   ê¸°ìˆ ì  ë¶„ì„ í‚¤ì›Œë“œ: {total_keywords}ê°œ ë°œê²¬")

# ê¸°ìˆ ì  ë¶„ì„ ìˆ˜ì§‘ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
collector = TechnicalAnalysisCollector()

# í…ŒìŠ¤íŠ¸ìš© URLë“¤ (ì‹¤ì œ ê¸°ìˆ ì  ë¶„ì„ ì‚¬ì´íŠ¸ë“¤)
test_urls = [
    "https://www.example.com/",
    "https://httpbin.org/html",
    "https://httpbin.org/json"
]

collector.collect_from_sites(test_urls)
collector.generate_report()
print()

# === 3. í•œêµ­ ì£¼ì‹ ê¸°ìˆ ì  ë¶„ì„ ë¸”ë¡œê·¸ ì½˜í…ì¸  ì¶”ì¶œ ===
print("=== 3. í•œêµ­ ì£¼ì‹ ê¸°ìˆ ì  ë¶„ì„ ë¸”ë¡œê·¸ ì½˜í…ì¸  ì¶”ì¶œ ===")

def extract_korean_technical_content(url, title=""):
    """í•œêµ­ ê¸°ìˆ ì  ë¶„ì„ ë¸”ë¡œê·¸ì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ"""
    try:
        loader = WebBaseLoader(url)
        loader.requests_kwargs = {
            'headers': {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Charset': 'utf-8'
            },
            'timeout': 15
        }
        
        docs = loader.load()
        
        if docs:
            content = docs[0].page_content
            
            # í•œêµ­ì–´ ê¸°ìˆ ì  ë¶„ì„ ê´€ë ¨ ìš©ì–´ ì¶”ì¶œ
            korean_terms = {
                'RSI': ['ìƒëŒ€ê°•ë„ì§€ìˆ˜', 'RSI', 'ê³¼ë§¤ìˆ˜', 'ê³¼ë§¤ë„', '70ì„ ', '30ì„ '],
                'MACD': ['MACD', 'ì´ë™í‰ê· ìˆ˜ë ´í™•ì‚°', 'ì‹œê·¸ë„ë¼ì¸', 'íˆìŠ¤í† ê·¸ë¨'],
                'Bollinger': ['ë³¼ë¦°ì €ë°´ë“œ', 'í‘œì¤€í¸ì°¨', 'ìƒí•œì„ ', 'í•˜í•œì„ ', 'ìŠ¤í€´ì¦ˆ'],
                'Stochastic': ['ìŠ¤í† ìºìŠ¤í‹±', '%K', '%D', 'ì˜¤ì‹¤ë ˆì´í„°'],
                'Moving Average': ['ì´ë™í‰ê· ', 'ë‹¨ìˆœì´ë™í‰ê· ', 'ì§€ìˆ˜ì´ë™í‰ê· ', 'ì •ë°°ì—´', 'ì—­ë°°ì—´'],
                'Support_Resistance': ['ì§€ì§€ì„ ', 'ì €í•­ì„ ', 'ì¶”ì„¸ì„ ', 'ëŒíŒŒ', 'ì´íƒˆ'],
                'Patterns': ['ì‚¼ê°í˜•íŒ¨í„´', 'í”Œë˜ê·¸íŒ¨í„´', 'ìê¸°í˜•', 'í—¤ë“œì•¤ìˆ„ë”', 'ë”ë¸”íƒ‘', 'ë”ë¸”ë°”í…€']
            }
            
            found_terms = {}
            for category, terms in korean_terms.items():
                found_terms[category] = []
                for term in terms:
                    if term in content:
                        found_terms[category].append(term)
            
            # ë‚´ìš© ìš”ì•½
            summary = {
                'title': title,
                'url': url,
                'content_length': len(content),
                'found_terms': found_terms,
                'key_sentences': []
            }
            
            # ì¤‘ìš”í•œ ë¬¸ì¥ ì¶”ì¶œ (RSI, MACD ë“±ì´ í¬í•¨ëœ ë¬¸ì¥)
            sentences = content.split('.')
            for sentence in sentences[:20]:  # ì²˜ìŒ 20ê°œ ë¬¸ì¥ë§Œ í™•ì¸
                sentence = sentence.strip()
                if any(term in sentence for term_list in korean_terms.values() for term in term_list):
                    if len(sentence) > 10 and len(sentence) < 200:
                        summary['key_sentences'].append(sentence)
            
            return summary
            
    except Exception as e:
        print(f"âŒ {title} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return None

# í•œêµ­ ê¸°ìˆ ì  ë¶„ì„ ì½˜í…ì¸  ì˜ˆì œ (ê³µê°œì ìœ¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œ ì‚¬ì´íŠ¸ë“¤)
korean_analysis_examples = [
    ("https://httpbin.org/html", "HTML ì˜ˆì œ"),
    ("https://www.example.com/", "ê¸°ë³¸ ì˜ˆì œ"),
]

print("ğŸ“š í•œêµ­ ì£¼ì‹ ê¸°ìˆ ì  ë¶„ì„ ì½˜í…ì¸  ì¶”ì¶œ ê²°ê³¼:")
for url, title in korean_analysis_examples:
    print(f"\nğŸ” {title} ë¶„ì„ ì¤‘...")
    result = extract_korean_technical_content(url, title)
    
    if result:
        print(f"âœ… ì„±ê³µ: {result['content_length']:,}ì ì¶”ì¶œ")
        print(f"ğŸ“Š ë°œê²¬ëœ ê¸°ìˆ ì  ë¶„ì„ ìš©ì–´:")
        
        for category, terms in result['found_terms'].items():
            if terms:
                print(f"  â€¢ {category}: {', '.join(terms)}")
        
        if result['key_sentences']:
            print(f"ğŸ¯ í•µì‹¬ ë¬¸ì¥ (ìƒìœ„ 3ê°œ):")
            for i, sentence in enumerate(result['key_sentences'][:3], 1):
                print(f"  {i}. {sentence[:100]}...")
    else:
        print("âŒ ì¶”ì¶œ ì‹¤íŒ¨")

print()

# === 4. ì‹¤ì‹œê°„ ê¸°ìˆ ì  ë¶„ì„ ë°ì´í„° ëª¨ë‹ˆí„°ë§ (ìˆ˜ì •ë¨) ===
print("=== 4. ì‹¤ì‹œê°„ ê¸°ìˆ ì  ë¶„ì„ ë°ì´í„° ëª¨ë‹ˆí„°ë§ ===")

class TechnicalAnalysisMonitor:
    """ê¸°ìˆ ì  ë¶„ì„ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.monitoring_sites = [
            "https://httpbin.org/json",
            "https://httpbin.org/html",
            "https://www.example.com/"
        ]
        self.collected_indicators = {}
    
    async def fetch_market_data(self, url):
        """ê°œë³„ ì‚¬ì´íŠ¸ì—ì„œ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ (ìˆ˜ì •ëœ ë²„ì „)"""
        try:
            loader = WebBaseLoader(url)
            loader.requests_kwargs = {
                'headers': {
                    'User-Agent': 'Technical-Analysis-Bot/1.0',
                    'Accept': 'text/html,application/json'
                },
                'timeout': 10
            }
            
            # ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë¬¸ì„œ ë¡œë“œ
            docs = loader.load()
            
            # ê°€ìƒì˜ ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
            import random
            
            indicators = {
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source': url,
                'RSI': round(random.uniform(20, 80), 2),
                'MACD': round(random.uniform(-2, 2), 4),
                'Bollinger_Upper': round(random.uniform(50000, 60000), 0),
                'Bollinger_Lower': round(random.uniform(40000, 50000), 0),
                'Volume': random.randint(1000000, 5000000),
                'status': 'success' if docs else 'failed'
            }
            
            return indicators
            
        except Exception as e:
            return {
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source': url,
                'status': 'error',
                'error': str(e)
            }
    
    async def monitor_multiple_sources(self):
        """ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ ë™ì‹œì— ëª¨ë‹ˆí„°ë§"""
        print("ğŸ“Š ì‹¤ì‹œê°„ ê¸°ìˆ ì  ë¶„ì„ ë°ì´í„° ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
        
        tasks = [self.fetch_market_data(url) for url in self.monitoring_sites]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        print(f"\nâ° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ë°ì´í„° ìŠ¤ëƒ…ìƒ·:")
        print("-" * 80)
        
        for i, result in enumerate(results):
            if isinstance(result, dict) and result.get('status') == 'success':
                print(f"ğŸ“ˆ ì†ŒìŠ¤ {i+1}: {result['source']}")
                print(f"   RSI: {result['RSI']} | MACD: {result['MACD']} | ê±°ë˜ëŸ‰: {result['Volume']:,}")
                print(f"   ë³¼ë¦°ì €ë°´ë“œ: {result['Bollinger_Lower']:,} ~ {result['Bollinger_Upper']:,}")
                
                # RSI ê¸°ë°˜ ë§¤ë§¤ ì‹ í˜¸
                if result['RSI'] > 70:
                    print("   ğŸ”´ ê³¼ë§¤ìˆ˜ êµ¬ê°„ - ë§¤ë„ ê³ ë ¤")
                elif result['RSI'] < 30:
                    print("   ğŸŸ¢ ê³¼ë§¤ë„ êµ¬ê°„ - ë§¤ìˆ˜ ê³ ë ¤")
                else:
                    print("   ğŸŸ¡ ì¤‘ë¦½ êµ¬ê°„")
                
            else:
                print(f"âŒ ì†ŒìŠ¤ {i+1}: ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                if isinstance(result, dict) and 'error' in result:
                    print(f"   ì˜¤ë¥˜: {result['error']}")
            
            print()

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
monitor = TechnicalAnalysisMonitor()
asyncio.run(monitor.monitor_multiple_sources())
print()

# === 5. ê¸°ìˆ ì  ë¶„ì„ êµìœ¡ ì½˜í…ì¸  ì¶”ì¶œ ë° ì •ë¦¬ ===
print("=== 5. ê¸°ìˆ ì  ë¶„ì„ êµìœ¡ ì½˜í…ì¸  ì¶”ì¶œ ë° ì •ë¦¬ ===")

class TechnicalAnalysisEducator:
    """ê¸°ìˆ ì  ë¶„ì„ êµìœ¡ ì½˜í…ì¸ ë¥¼ ìˆ˜ì§‘í•˜ê³  ì •ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.educational_content = {}
        self.learning_modules = {
            'RSI': {
                'description': 'ìƒëŒ€ê°•ë„ì§€ìˆ˜ - ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ ìƒíƒœ íŒë‹¨',
                'key_levels': [30, 50, 70],
                'interpretation': {
                    'above_70': 'ê³¼ë§¤ìˆ˜ - ë§¤ë„ ê³ ë ¤',
                    'below_30': 'ê³¼ë§¤ë„ - ë§¤ìˆ˜ ê³ ë ¤',
                    'around_50': 'ì¤‘ë¦½ - ì¶”ì„¸ í™•ì¸ í•„ìš”'
                }
            },
            'MACD': {
                'description': 'ì´ë™í‰ê· ìˆ˜ë ´í™•ì‚° - ì¶”ì„¸ ì „í™˜ì  í¬ì°©',
                'components': ['MACDì„ ', 'ì‹œê·¸ë„ì„ ', 'íˆìŠ¤í† ê·¸ë¨'],
                'signals': {
                    'golden_cross': 'MACDì„ ì´ ì‹œê·¸ë„ì„ ì„ ìƒí–¥ ëŒíŒŒ - ë§¤ìˆ˜ì‹ í˜¸',
                    'dead_cross': 'MACDì„ ì´ ì‹œê·¸ë„ì„ ì„ í•˜í–¥ ëŒíŒŒ - ë§¤ë„ì‹ í˜¸'
                }
            },
            'BollingerBands': {
                'description': 'ë³¼ë¦°ì €ë°´ë“œ - ë³€ë™ì„±ê³¼ ì§€ì§€/ì €í•­ ìˆ˜ì¤€',
                'components': ['ì¤‘ì‹¬ì„ (20ì¼ ì´í‰)', 'ìƒí•œì„ (+2Ïƒ)', 'í•˜í•œì„ (-2Ïƒ)'],
                'strategies': {
                    'band_walk': 'ìƒí•œì„  ê·¼ì²˜ ìœ ì§€ì‹œ ê°•í•œ ìƒìŠ¹ì¶”ì„¸',
                    'squeeze': 'ë°´ë“œí­ ì¶•ì†Œì‹œ í° ì›€ì§ì„ ì˜ˆê³ ',
                    'reversal': 'ë°´ë“œ í„°ì¹˜ í›„ ë°˜ëŒ€ë°©í–¥ ì´ë™'
                }
            }
        }
    
    def create_educational_summary(self):
        """êµìœ¡ìš© ê¸°ìˆ ì  ë¶„ì„ ìš”ì•½ ìƒì„±"""
        print("ğŸ“š ê¸°ìˆ ì  ë¶„ì„ í•™ìŠµ ê°€ì´ë“œ")
        print("=" * 50)
        
        for indicator, info in self.learning_modules.items():
            print(f"\nğŸ“Š {indicator}")
            print(f"   ì •ì˜: {info['description']}")
            
            if 'key_levels' in info:
                print(f"   ì£¼ìš” ìˆ˜ì¤€: {', '.join(map(str, info['key_levels']))}")
            
            if 'interpretation' in info:
                print("   í•´ì„:")
                for condition, meaning in info['interpretation'].items():
                    print(f"     â€¢ {condition.replace('_', ' ').title()}: {meaning}")
            
            if 'signals' in info:
                print("   ì£¼ìš” ì‹ í˜¸:")
                for signal, meaning in info['signals'].items():
                    print(f"     â€¢ {signal.replace('_', ' ').title()}: {meaning}")
            
            if 'strategies' in info:
                print("   ì „ëµ:")
                for strategy, description in info['strategies'].items():
                    print(f"     â€¢ {strategy.replace('_', ' ').title()}: {description}")
    
    def extract_learning_content(self, url):
        """í•™ìŠµ ì½˜í…ì¸ ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ"""
        try:
            loader = WebBaseLoader(url)
            loader.requests_kwargs = {
                'headers': {'User-Agent': 'Educational-Content-Extractor/1.0'},
                'timeout': 10
            }
            
            docs = loader.load()
            
            if docs:
                content = docs[0].page_content
                
                # êµìœ¡ì  í‚¤ì›Œë“œ ì¶”ì¶œ
                educational_keywords = [
                    'ì„¤ëª…', 'ë°©ë²•', 'ê³„ì‚°', 'ê³µì‹', 'ì˜ˆì œ', 'í™œìš©', 'ì „ëµ', 
                    'ì£¼ì˜ì‚¬í•­', 'ì¥ì ', 'ë‹¨ì ', 'í•œê³„', 'ë³´ì™„'
                ]
                
                educational_sentences = []
                sentences = content.split('.')
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if any(keyword in sentence for keyword in educational_keywords):
                        if 20 < len(sentence) < 150:
                            educational_sentences.append(sentence)
                
                return {
                    'url': url,
                    'educational_content': educational_sentences[:5],  # ìƒìœ„ 5ê°œ
                    'content_quality': len(educational_sentences)
                }
        
        except Exception as e:
            print(f"âŒ í•™ìŠµ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def generate_practice_scenarios(self):
        """ì‹¤ìŠµ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        print("\nğŸ¯ ì‹¤ìŠµ ì‹œë‚˜ë¦¬ì˜¤")
        print("-" * 30)
        
        scenarios = [
            {
                'situation': 'RSI 75, ì£¼ê°€ ì‹ ê³ ì  ê·¼ì²˜',
                'analysis': 'ê³¼ë§¤ìˆ˜ ìƒíƒœ, ë‹¨ê¸° ì¡°ì • ê°€ëŠ¥ì„± ë†’ìŒ',
                'action': 'ë¶„í• ë§¤ë„ ë˜ëŠ” ê´€ë§'
            },
            {
                'situation': 'MACD ê³¨ë“ í¬ë¡œìŠ¤ ë°œìƒ, ê±°ë˜ëŸ‰ ì¦ê°€',
                'analysis': 'ìƒìŠ¹ ì¶”ì„¸ ì „í™˜ ì‹ í˜¸',
                'action': 'ë§¤ìˆ˜ í¬ì§€ì…˜ ê³ ë ¤'
            },
            {
                'situation': 'ë³¼ë¦°ì €ë°´ë“œ í•˜í•œì„  í„°ì¹˜, RSI 25',
                'analysis': 'ê³¼ë§¤ë„ + ì§€ì§€ì„  ë„ë‹¬',
                'action': 'ë°˜ë“± ë§¤ìˆ˜ ê¸°íšŒ'
            }
        ]
        
        for i, scenario in enumerate(scenarios, 1):
            print(f"{i}. ìƒí™©: {scenario['situation']}")
            print(f"   ë¶„ì„: {scenario['analysis']}")
            print(f"   ëŒ€ì‘: {scenario['action']}\n")

# êµìœ¡ ì½˜í…ì¸  ìƒì„± ë° ì‹¤í–‰
educator = TechnicalAnalysisEducator()
educator.create_educational_summary()

# ì‹¤ì œ êµìœ¡ ì‚¬ì´íŠ¸ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ (ì˜ˆì œ)
sample_educational_sites = [
    "https://www.example.com/",
    "https://httpbin.org/html"
]

print(f"\nğŸ” êµìœ¡ ì½˜í…ì¸  í’ˆì§ˆ ë¶„ì„:")
for url in sample_educational_sites:
    result = educator.extract_learning_content(url)
    if result:
        print(f"âœ… {url}: êµìœ¡ í’ˆì§ˆ ì ìˆ˜ {result['content_quality']}")
        if result['educational_content']:
            print(f"   ğŸ“ í•µì‹¬ ë‚´ìš©: {result['educational_content'][0][:80]}...")

educator.generate_practice_scenarios()
print()

# === ë‚˜ë¨¸ì§€ ê¸°ëŠ¥ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼) ===

# 6. XML íŒŒì„œ ì‚¬ìš©
print("=== 6. XML íŒŒì„œ ì‚¬ìš© ===")
xml_loader = WebBaseLoader("https://httpbin.org/xml")
xml_loader.default_parser = "xml"
try:
    xml_docs = xml_loader.load()
    print(f"XML ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(xml_docs)}ê°œ")
    print(f"XML ë‚´ìš© (ì²˜ìŒ 200ì): {xml_docs[0].page_content[:200]}")
except Exception as e:
    print(f"XML ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
print()

# 7. Lazy Loading (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
print("=== 7. Lazy Loading ===")
loader_lazy = WebBaseLoader("https://www.example.com/")

pages = []
for doc in loader_lazy.lazy_load():
    pages.append(doc)
    print(f"Lazy loadë¡œ ë¬¸ì„œ ë¡œë“œ: {doc.metadata.get('title', 'N/A')}")
print()

# 8. ë¹„ë™ê¸° Lazy Loading (ìˆ˜ì •ë¨)
async def async_lazy_load_example():
    print("=== 8. ë¹„ë™ê¸° Lazy Loading ===")
    loader_async_lazy = WebBaseLoader("https://www.example.com/")
    
    pages_async = []
    async for doc in loader_async_lazy.alazy_load():
        pages_async.append(doc)
        print(f"Async lazy loadë¡œ ë¬¸ì„œ ë¡œë“œ: {doc.metadata.get('title', 'N/A')}")
    
    return pages_async

# ë¹„ë™ê¸° lazy loading ì‹¤í–‰
try:
    pages_async = asyncio.run(async_lazy_load_example())
    print(f"ë¹„ë™ê¸° lazy loading ì™„ë£Œ: {len(pages_async)}ê°œ ë¬¸ì„œ")
except Exception as e:
    print(f"ë¹„ë™ê¸° lazy loading ì¤‘ ì˜¤ë¥˜: {e}")
print()

# 9. ì—ëŸ¬ ì²˜ë¦¬ ì˜ˆì œ
print("=== 9. ì—ëŸ¬ ì²˜ë¦¬ ì˜ˆì œ ===")
def safe_load_web_content(url):
    """ì•ˆì „í•œ ì›¹ ì»¨í…ì¸  ë¡œë”© í•¨ìˆ˜"""
    try:
        loader = WebBaseLoader(url)
        docs = loader.load()
        return docs
    except Exception as e:
        print(f"URL {url} ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# ìœ íš¨í•˜ì§€ ì•Šì€ URL í…ŒìŠ¤íŠ¸
invalid_docs = safe_load_web_content("https://nonexistent-site-12345.com")
valid_docs = safe_load_web_content("https://www.example.com/")

if valid_docs:
    print("ìœ íš¨í•œ URLë¡œ ë¬¸ì„œ ë¡œë“œ ì„±ê³µ")
print()

# 10. ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë¶„ì„
print("=== 10. ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë¶„ì„ ===")
loader_meta = WebBaseLoader("https://www.example.com/")
docs_meta = loader_meta.load()

if docs_meta:
    doc = docs_meta[0]
    print("ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:")
    for key, value in doc.metadata.items():
        print(f"  {key}: {value}")
    
    print(f"\në¬¸ì„œ ë‚´ìš© ê¸¸ì´: {len(doc.page_content)} ë¬¸ì")
    print(f"ë¬¸ì„œ ë‚´ìš© (ì²˜ìŒ 100ì): {doc.page_content[:100]}...")
print()

# === ì¶”ê°€ ê¸°ëŠ¥: ê³ ê¸‰ ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ ===
print("=== 11. ê³ ê¸‰ ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ ===")

import time
from typing import Optional, List

class RobustWebLoader:
    """ê²¬ê³ í•œ ì›¹ ë¡œë” í´ë˜ìŠ¤ - ì¬ì‹œë„ ë° ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨"""
    
    def __init__(self, max_retries: int = 3, retry_delay: float = 1.0):
        self.max_retries = max_retries
        self.retry_delay = retry_delay
    
    def load_with_retry(self, url: str, description: str = "") -> Optional[List]:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ë¬¸ì„œ ë¡œë“œ"""
        for attempt in range(self.max_retries):
            try:
                print(f"ğŸ”„ ì‹œë„ {attempt + 1}/{self.max_retries}: {description or url}")
                
                loader = WebBaseLoader(url)
                loader.requests_kwargs = {
                    'headers': {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    },
                    'timeout': 10 + (attempt * 5),  # ì¬ì‹œë„ë§ˆë‹¤ íƒ€ì„ì•„ì›ƒ ì¦ê°€
                    'verify': True
                }
                
                docs = loader.load()
                
                if docs and len(docs) > 0:
                    print(f"âœ… ì„±ê³µ: {len(docs)}ê°œ ë¬¸ì„œ, {len(docs[0].page_content)}ì")
                    return docs
                else:
                    print(f"âš ï¸ ë¹ˆ ë¬¸ì„œ ë°˜í™˜")
                    
            except Exception as e:
                print(f"âŒ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {str(e)[:100]}...")
                
                if attempt < self.max_retries - 1:
                    print(f"â³ {self.retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(self.retry_delay)
                    self.retry_delay *= 1.5  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    print(f"ğŸ’¥ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨: {url}")
        
        return None
    
    async def async_load_with_retry(self, url: str, description: str = "") -> Optional[List]:
        """ë¹„ë™ê¸° ì¬ì‹œë„ ë¡œì§"""
        for attempt in range(self.max_retries):
            try:
                print(f"ğŸ”„ ë¹„ë™ê¸° ì‹œë„ {attempt + 1}/{self.max_retries}: {description or url}")
                
                loader = WebBaseLoader(url)
                loader.requests_kwargs = {
                    'headers': {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    },
                    'timeout': 10 + (attempt * 5),
                }
                
                # ì˜¬ë°”ë¥¸ ë¹„ë™ê¸° ì²˜ë¦¬
                docs = []
                async for doc in loader.alazy_load():
                    docs.append(doc)
                
                if docs and len(docs) > 0:
                    print(f"âœ… ë¹„ë™ê¸° ì„±ê³µ: {len(docs)}ê°œ ë¬¸ì„œ")
                    return docs
                    
            except Exception as e:
                print(f"âŒ ë¹„ë™ê¸° ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {str(e)[:100]}...")
                
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(self.retry_delay)
                    self.retry_delay *= 1.5
        
        return None

# ê²¬ê³ í•œ ë¡œë” í…ŒìŠ¤íŠ¸
robust_loader = RobustWebLoader(max_retries=2, retry_delay=1.0)

test_urls_with_descriptions = [
    ("https://www.example.com/", "ì˜ˆì œ ì‚¬ì´íŠ¸"),
    ("https://httpbin.org/html", "HTTP í…ŒìŠ¤íŠ¸"),
    ("https://nonexistent-really-fake-url.com/", "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸")
]

for url, desc in test_urls_with_descriptions:
    result = robust_loader.load_with_retry(url, desc)
    if result:
        print(f"ğŸ“Š {desc}: ìµœì¢… ì„±ê³µ\n")
    else:
        print(f"ğŸ’” {desc}: ìµœì¢… ì‹¤íŒ¨\n")

# === 12. ë°°ì¹˜ ì²˜ë¦¬ ë° ë³‘ë ¬ ì‹¤í–‰ ===
print("=== 12. ë°°ì¹˜ ì²˜ë¦¬ ë° ë³‘ë ¬ ì‹¤í–‰ ===")

class BatchWebLoader:
    """ë°°ì¹˜ ì²˜ë¦¬ ì›¹ ë¡œë”"""
    
    def __init__(self, max_concurrent: int = 3, delay_between_batches: float = 1.0):
        self.max_concurrent = max_concurrent
        self.delay_between_batches = delay_between_batches
    
    async def process_urls_in_batches(self, urls: List[str], batch_size: int = 3):
        """URLë“¤ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬"""
        print(f"ğŸ“¦ ì´ {len(urls)}ê°œ URLì„ {batch_size}ê°œì”© ë°°ì¹˜ ì²˜ë¦¬")
        
        results = []
        
        for i in range(0, len(urls), batch_size):
            batch = urls[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(urls) + batch_size - 1) // batch_size
            
            print(f"\nğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘...")
            
            # ë°°ì¹˜ ë‚´ URLë“¤ì„ ë³‘ë ¬ ì²˜ë¦¬
            batch_tasks = [self.load_single_url(url) for url in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for j, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    print(f"âŒ ë°°ì¹˜ {batch_num} URL {j+1}: {str(result)[:50]}...")
                    results.append(None)
                else:
                    results.append(result)
            
            # ë°°ì¹˜ ê°„ ì§€ì—°
            if i + batch_size < len(urls):
                print(f"â³ ë‹¤ìŒ ë°°ì¹˜ê¹Œì§€ {self.delay_between_batches}ì´ˆ ëŒ€ê¸°...")
                await asyncio.sleep(self.delay_between_batches)
        
        # ê²°ê³¼ ìš”ì•½
        successful = len([r for r in results if r is not None])
        print(f"\nğŸ“ˆ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {successful}/{len(urls)} ì„±ê³µ")
        
        return results
    
    async def load_single_url(self, url: str):
        """ë‹¨ì¼ URL ë¡œë“œ"""
        try:
            loader = WebBaseLoader(url)
            loader.requests_kwargs = {
                'headers': {
                    'User-Agent': 'Batch-Web-Loader/1.0'
                },
                'timeout': 10
            }
            
            docs = []
            async for doc in loader.alazy_load():
                docs.append(doc)
            
            if docs:
                print(f"âœ… {url}: {len(docs[0].page_content)}ì")
                return {
                    'url': url,
                    'docs': docs,
                    'status': 'success'
                }
            else:
                print(f"âš ï¸ {url}: ë¹ˆ ì‘ë‹µ")
                return None
                
        except Exception as e:
            print(f"âŒ {url}: {str(e)[:50]}...")
            raise e

# ë°°ì¹˜ ë¡œë” í…ŒìŠ¤íŠ¸
batch_loader = BatchWebLoader(max_concurrent=2, delay_between_batches=0.5)

batch_test_urls = [
    "https://www.example.com/",
    "https://httpbin.org/html", 
    "https://httpbin.org/json",
    "https://httpbin.org/xml",
    "https://httpbin.org/user-agent"
]

batch_results = asyncio.run(batch_loader.process_urls_in_batches(batch_test_urls, batch_size=2))

# === 13. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° í†µê³„ ===
print("\n=== 13. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° í†µê³„ ===")

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_bytes': 0,
            'total_time': 0,
            'avg_response_time': 0,
            'fastest_request': float('inf'),
            'slowest_request': 0
        }
    
    def record_request(self, success: bool, response_time: float, content_size: int = 0):
        """ìš”ì²­ ê¸°ë¡"""
        self.stats['total_requests'] += 1
        self.stats['total_time'] += response_time
        
        if success:
            self.stats['successful_requests'] += 1
            self.stats['total_bytes'] += content_size
            self.stats['fastest_request'] = min(self.stats['fastest_request'], response_time)
            self.stats['slowest_request'] = max(self.stats['slowest_request'], response_time)
        else:
            self.stats['failed_requests'] += 1
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        if self.stats['total_requests'] > 0:
            self.stats['avg_response_time'] = self.stats['total_time'] / self.stats['total_requests']
    
    def get_report(self) -> str:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        success_rate = (self.stats['successful_requests'] / max(1, self.stats['total_requests'])) * 100
        avg_content_size = self.stats['total_bytes'] / max(1, self.stats['successful_requests'])
        
        report = f"""
ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸
{'='*50}
ì´ ìš”ì²­ ìˆ˜: {self.stats['total_requests']}
ì„±ê³µí•œ ìš”ì²­: {self.stats['successful_requests']}
ì‹¤íŒ¨í•œ ìš”ì²­: {self.stats['failed_requests']}
ì„±ê³µë¥ : {success_rate:.1f}%

â±ï¸ ì‘ë‹µ ì‹œê°„ í†µê³„:
  í‰ê· : {self.stats['avg_response_time']:.2f}ì´ˆ
  ìµœê³ ì†: {self.stats['fastest_request']:.2f}ì´ˆ
  ìµœì €ì†: {self.stats['slowest_request']:.2f}ì´ˆ

ğŸ“¦ ë°ì´í„° í†µê³„:
  ì´ ë°”ì´íŠ¸: {self.stats['total_bytes']:,}
  í‰ê·  ì½˜í…ì¸  í¬ê¸°: {avg_content_size:,.0f} ë°”ì´íŠ¸
  ì´ ì²˜ë¦¬ ì‹œê°„: {self.stats['total_time']:.2f}ì´ˆ
"""
        return report

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸
monitor = PerformanceMonitor()

test_performance_urls = [
    "https://www.example.com/",
    "https://httpbin.org/html",
    "https://httpbin.org/delay/1"  # 1ì´ˆ ì§€ì—° URL
]

print("ğŸ” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘...")

for url in test_performance_urls:
    start_time = time.time()
    
    try:
        loader = WebBaseLoader(url)
        docs = loader.load()
        
        end_time = time.time()
        response_time = end_time - start_time
        content_size = len(docs[0].page_content) if docs else 0
        
        monitor.record_request(success=True, response_time=response_time, content_size=content_size)
        print(f"âœ… {url}: {response_time:.2f}ì´ˆ, {content_size:,}ë°”ì´íŠ¸")
        
    except Exception as e:
        end_time = time.time()
        response_time = end_time - start_time
        
        monitor.record_request(success=False, response_time=response_time)
        print(f"âŒ {url}: {response_time:.2f}ì´ˆ, ì‹¤íŒ¨")

print(monitor.get_report())

print("\n" + "="*80)
print("ğŸŠ ìˆ˜ì •ëœ WebBaseLoader ìƒ˜í”Œ ì½”ë“œ ì‹¤í–‰ ì™„ë£Œ!")
print("="*80)
print("""
ğŸ”§ ì£¼ìš” ìˆ˜ì • ì‚¬í•­:
   âœ… aload() ë¹„ë™ê¸° ì²˜ë¦¬ ì˜¤ë¥˜ í•´ê²°
   âœ… async for ë£¨í”„ë¥¼ ì‚¬ìš©í•œ ì˜¬ë°”ë¥¸ ë¹„ë™ê¸° ì²˜ë¦¬
   âœ… ì¬ì‹œë„ ë¡œì§ ë° ê²¬ê³ í•œ ì—ëŸ¬ ì²˜ë¦¬
   âœ… ë°°ì¹˜ ì²˜ë¦¬ ë° ë³‘ë ¬ ì‹¤í–‰ ê¸°ëŠ¥
   âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° í†µê³„ ìˆ˜ì§‘
   âœ… ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§

ğŸ’¡ í•µì‹¬ ê°œì„ ì :
   â€¢ aload() â†’ async for alazy_load() íŒ¨í„´ ì‚¬ìš©
   â€¢ ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™” ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
   â€¢ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëŒ€ëŸ‰ URL íš¨ìœ¨ì  ì²˜ë¦¬
   â€¢ ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
   â€¢ ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—° ë¡œì§

ğŸš€ ì‹¤ë¬´ í™œìš© íŒ:
   â€¢ ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œ ë°°ì¹˜ ì²˜ë¦¬ í™œìš©
   â€¢ ë¶ˆì•ˆì •í•œ ë„¤íŠ¸ì›Œí¬ í™˜ê²½ì—ì„œ ì¬ì‹œë„ ë¡œì§ í•„ìˆ˜
   â€¢ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ë³‘ëª© ì§€ì  íŒŒì•…
   â€¢ ì„œë²„ ì •ì±…ì— ë§ëŠ” ìš”ì²­ ë¹ˆë„ ì¡°ì ˆ
""")