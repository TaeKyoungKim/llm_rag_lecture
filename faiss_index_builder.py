"""
FAISS HNSW ì¸ë±ìŠ¤ êµ¬ì¶•ê¸°
PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì„ë² ë”©í•˜ì—¬ FAISS HNSW ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³  ì €ì¥í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import os
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import numpy as np

# FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬
import faiss

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
from PyPDF2 import PdfReader

from dotenv import load_dotenv
load_dotenv()

class FAISSIndexBuilder:
    """
    FAISS HNSW ì¸ë±ìŠ¤ êµ¬ì¶• í´ë˜ìŠ¤
    PDF ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì €ì¥
    """
    
    def __init__(self, embedding_type: str = "huggingface"):
        """
        ì¸ë±ìŠ¤ êµ¬ì¶•ê¸° ì´ˆê¸°í™”
        Args:
            embedding_type: "huggingface" ë˜ëŠ” "gemini"
        """
        print("ğŸ”§ FAISS ì¸ë±ìŠ¤ êµ¬ì¶•ê¸° ì´ˆê¸°í™”")
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.pdf_path = Path("DocumentsLoader/data/ê¸°ìˆ ì ì°¨íŠ¸ë¶„ì„ì´ë¡ ë°ë°©ë²•.pdf")
        self.index_dir = Path("DocumentsLoader/educational_faiss_index")
        self.index_dir.mkdir(exist_ok=True)
        
        # ì„ë² ë”© íƒ€ì… ì„¤ì •
        self.embedding_type = embedding_type
        
        # HNSW ì„¤ì •
        self.hnsw_config = {
            'M': 16,  # ê° ë…¸ë“œì˜ ìµœëŒ€ ì—°ê²° ìˆ˜
            'efConstruction': 100,  # êµ¬ì¶• ì‹œ íƒìƒ‰í•  ì´ì›ƒ ìˆ˜
            'efSearch': 50,  # ê²€ìƒ‰ ì‹œ íƒìƒ‰í•  ì´ì›ƒ ìˆ˜
            'metric': faiss.METRIC_INNER_PRODUCT  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        }
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_embeddings()
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
        self._initialize_text_splitter()
        
        print("   âœ… ì¸ë±ìŠ¤ êµ¬ì¶•ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        print("   ğŸ”„ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        if self.embedding_type == "gemini":
            try:
                # Gemini API í‚¤ í™•ì¸
                api_key = os.getenv("GOOGLE_API_KEY")
                if not api_key:
                    print("   âš ï¸ GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    print("   ğŸ’¡ HuggingFace ì„ë² ë”©ìœ¼ë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤.")
                    self._load_huggingface_embeddings()
                    return
                
                print("   ğŸ”‘ Gemini API í‚¤ í™•ì¸ë¨")
                
                # Gemini ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
                self.embeddings = GoogleGenerativeAIEmbeddings(
                    model="models/embedding-001",
                    google_api_key=api_key,
                    task_type="retrieval_query",
                    title="Technical Analysis Document"
                )
                
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ ì—°ê²° í™•ì¸
                test_embedding = self.embeddings.embed_query("test")
                if test_embedding and len(test_embedding) > 0:
                    print("   âœ… Gemini ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                else:
                    raise Exception("ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                    
            except Exception as e:
                error_msg = str(e)
                print(f"   âš ï¸ Gemini ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {error_msg}")
                
                # ì˜¤ë¥˜ ìœ í˜•ë³„ ì•ˆë‚´ ë©”ì‹œì§€
                if "invalid_grant" in error_msg or "Bad Request" in error_msg:
                    print("   ğŸ’¡ API í‚¤ ì¸ì¦ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
                    print("      - GOOGLE_API_KEYê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
                    print("      - API í‚¤ê°€ Gemini APIì— ëŒ€í•œ ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸")
                    print("      - API í‚¤ê°€ ë§Œë£Œë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸")
                elif "timeout" in error_msg.lower():
                    print("   ğŸ’¡ ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒì…ë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                elif "quota" in error_msg.lower():
                    print("   ğŸ’¡ API í• ë‹¹ëŸ‰ ì´ˆê³¼ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
                else:
                    print("   ğŸ’¡ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ì…ë‹ˆë‹¤. HuggingFace ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                
                print("   ğŸ”„ HuggingFace ì„ë² ë”©ìœ¼ë¡œ ìë™ ì „í™˜ ì¤‘...")
                self._load_huggingface_embeddings()
        else:
            self._load_huggingface_embeddings()
    
    def _load_huggingface_embeddings(self):
        """HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        try:
            print("   ğŸ”„ HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            # ì—¬ëŸ¬ ëª¨ë¸ ì˜µì…˜ ì œê³µ
            model_options = [
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                "sentence-transformers/all-MiniLM-L6-v2",
                "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
            ]
            
            for model_name in model_options:
                try:
                    print(f"   ğŸ” ëª¨ë¸ ì‹œë„ ì¤‘: {model_name}")
                    self.embeddings = HuggingFaceEmbeddings(
                        model_name=model_name,
                        model_kwargs={'device': 'cpu'},
                        encode_kwargs={'normalize_embeddings': True}
                    )
                    
                    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
                    test_embedding = self.embeddings.embed_query("test")
                    if test_embedding and len(test_embedding) > 0:
                        print(f"   âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
                        self.embedding_type = "huggingface"
                        return
                    else:
                        raise Exception("ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                        
                except Exception as e:
                    print(f"   âš ï¸ {model_name} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                    continue
            
            # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš°
            raise Exception("ì‚¬ìš© ê°€ëŠ¥í•œ HuggingFace ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"   âŒ HuggingFace ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            print("   ğŸ’¡ ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ì „í™˜í•˜ê±°ë‚˜ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            raise
    
    def _initialize_text_splitter(self):
        """í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        print("   âœ… í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_documents(self) -> List[Document]:
        """PDF ë¬¸ì„œ ë¡œë“œ"""
        print("\nğŸ“„ PDF ë¬¸ì„œ ë¡œë“œ")
        print("-" * 40)
        
        try:
            # PDF íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not self.pdf_path.exists():
                print(f"   âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.pdf_path}")
                return []
            
            print(f"   ğŸ“ PDF íŒŒì¼ ê²½ë¡œ: {self.pdf_path}")
            
            # PDF ì½ê¸°
            reader = PdfReader(str(self.pdf_path))
            total_pages = len(reader.pages)
            print(f"   ğŸ“Š ì´ í˜ì´ì§€ ìˆ˜: {total_pages}")
            
            # ê° í˜ì´ì§€ë¥¼ Documentë¡œ ë³€í™˜
            documents = []
            for page_num, page in enumerate(reader.pages):
                # í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = page.extract_text()
                
                # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                metadata = {
                    'source': str(self.pdf_path),
                    'page': page_num + 1,
                    'total_pages': total_pages,
                    'content_type': 'technical_analysis',
                    'language': 'ko',
                    'processing_time': datetime.now().isoformat()
                }
                
                # LangChain Document ìƒì„±
                doc = Document(
                    page_content=text,
                    metadata=metadata
                )
                documents.append(doc)
                
                print(f"   ğŸ“„ í˜ì´ì§€ {page_num + 1} ë¡œë“œ ì™„ë£Œ ({len(text)}ì)")
            
            print(f"   âœ… ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
            return documents
            
        except Exception as e:
            print(f"   âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def tokenize_and_chunk(self, documents: List[Document]) -> List[Document]:
        """í† í¬ë‚˜ì´ì§• ë° ì²­í‚¹"""
        print("\nâœ‚ï¸ í† í¬ë‚˜ì´ì§• ë° ì²­í‚¹")
        print("-" * 40)
        
        try:
            print("   ğŸ”„ ë¬¸ì„œ ë¶„í•  ì¤‘...")
            
            # ë¬¸ì„œ ë¶„í• 
            split_docs = self.text_splitter.split_documents(documents)
            
            print(f"   ğŸ“Š ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(documents)}")
            print(f"   ğŸ“Š ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(split_docs)}")
            
            # ì²­í¬ë³„ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for i, doc in enumerate(split_docs):
                # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ìœ ì§€
                original_metadata = doc.metadata.copy()
                
                # ì²­í¬ ê´€ë ¨ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                doc.metadata.update({
                    'chunk_id': i,
                    'chunk_size': len(doc.page_content),
                    'chunk_processing_time': datetime.now().isoformat(),
                    'embedding_type': self.embedding_type,
                    'has_technical_content': self._check_technical_content(doc.page_content)
                })
                
                print(f"   ğŸ“„ ì²­í¬ {i+1}: {len(doc.page_content)}ì "
                      f"({'âœ…' if doc.metadata['has_technical_content'] else 'âŒ'} ê¸°ìˆ ì  ë‚´ìš©)")
            
            # í†µê³„ ì •ë³´
            avg_chunk_size = sum(len(doc.page_content) for doc in split_docs) // len(split_docs)
            technical_chunks = sum(1 for doc in split_docs if doc.metadata['has_technical_content'])
            
            print(f"   ğŸ“ˆ í‰ê·  ì²­í¬ í¬ê¸°: {avg_chunk_size}ì")
            print(f"   ğŸ” ê¸°ìˆ ì  ë‚´ìš© í¬í•¨ ì²­í¬: {technical_chunks}ê°œ")
            
            print("   âœ… í† í¬ë‚˜ì´ì§• ë° ì²­í‚¹ ì™„ë£Œ")
            return split_docs
            
        except Exception as e:
            print(f"   âŒ í† í¬ë‚˜ì´ì§• ë° ì²­í‚¹ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _check_technical_content(self, content: str) -> bool:
        """ê¸°ìˆ ì  ë¶„ì„ ë‚´ìš© í¬í•¨ ì—¬ë¶€ í™•ì¸"""
        technical_terms = [
            'RSI', 'MACD', 'ë³¼ë¦°ì €', 'ì´ë™í‰ê· ', 'ìŠ¤í† ìºìŠ¤í‹±', 'ì¼ëª©ê· í˜•í‘œ',
            'í”¼ë³´ë‚˜ì¹˜', 'ì—˜ë¦¬ì–´íŠ¸', 'ì§€ì§€ì„ ', 'ì €í•­ì„ ', 'ê±°ë˜ëŸ‰', 'ì¶”ì„¸',
            'ê³¼ë§¤ìˆ˜', 'ê³¼ë§¤ë„', 'ê³¨ë“ í¬ë¡œìŠ¤', 'ë°ë“œí¬ë¡œìŠ¤', 'ë‹¤ì´ë²„ì „ìŠ¤'
        ]
        
        content_lower = content.lower()
        return any(term.lower() in content_lower for term in technical_terms)
    
    def create_embeddings(self, documents: List[Document]) -> List[np.ndarray]:
        """ì„ë² ë”© ìƒì„±"""
        print("\nğŸ”¢ ì„ë² ë”© ìƒì„±")
        print("-" * 40)
        
        try:
            print(f"   ğŸ”„ {self.embedding_type} ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„° ìƒì„± ì¤‘...")
            
            embeddings_list = []
            failed_count = 0
            max_retries = 3
            
            for i, doc in enumerate(documents):
                # ì¬ì‹œë„ ë¡œì§
                for retry in range(max_retries):
                    try:
                        # ì„ë² ë”© ìƒì„±
                        embedding = self.embeddings.embed_query(doc.page_content)
                        
                        # ì„ë² ë”© ìœ íš¨ì„± ê²€ì‚¬
                        if embedding and len(embedding) > 0:
                            embeddings_list.append(embedding)
                            break
                        else:
                            raise Exception("ë¹ˆ ì„ë² ë”© ìƒì„±ë¨")
                            
                    except Exception as e:
                        if retry < max_retries - 1:
                            print(f"   âš ï¸ ë¬¸ì„œ {i+1} ì„ë² ë”© ì‹¤íŒ¨ (ì¬ì‹œë„ {retry+1}/{max_retries}): {str(e)}")
                            import time
                            time.sleep(2)
                        else:
                            print(f"   âŒ ë¬¸ì„œ {i+1} ì„ë² ë”© ìµœì¢… ì‹¤íŒ¨: {str(e)}")
                            failed_count += 1
                            # ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ë²¡í„° ìƒì„± (0ìœ¼ë¡œ ì±„ì›€)
                            if embeddings_list:
                                default_dim = len(embeddings_list[0])
                                embeddings_list.append([0.0] * default_dim)
                            else:
                                # ì²« ë²ˆì§¸ ì„ë² ë”©ì´ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ì°¨ì› ì‚¬ìš©
                                embeddings_list.append([0.0] * 384)
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % 10 == 0 or i == len(documents) - 1:
                    success_rate = ((i + 1 - failed_count) / (i + 1)) * 100
                    print(f"   ğŸ“Š ì§„í–‰ë¥ : {i + 1}/{len(documents)} ({((i + 1) / len(documents) * 100):.1f}%) - ì„±ê³µë¥ : {success_rate:.1f}%")
            
            # ì„ë² ë”© ì°¨ì› í™•ì¸
            if embeddings_list:
                dimension = len(embeddings_list[0])
                print(f"   ğŸ“ ì„ë² ë”© ì°¨ì›: {dimension}")
                print(f"   ğŸ“Š ì´ ì„ë² ë”© ìˆ˜: {len(embeddings_list)}")
                print(f"   âš ï¸ ì‹¤íŒ¨í•œ ì„ë² ë”© ìˆ˜: {failed_count}")
                
                if failed_count > 0:
                    print(f"   ğŸ’¡ {failed_count}ê°œì˜ ì„ë² ë”©ì´ ì‹¤íŒ¨í•˜ì—¬ ê¸°ë³¸ê°’(0)ìœ¼ë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                print("   âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ")
                return embeddings_list
            else:
                raise Exception("ëª¨ë“  ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"   âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            
            # ì˜¤ë¥˜ ìœ í˜•ë³„ ì•ˆë‚´
            error_msg = str(e).lower()
            if "timeout" in error_msg:
                print("   ğŸ’¡ íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ë” ì‘ì€ ë°°ì¹˜ë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")
            elif "quota" in error_msg:
                print("   ğŸ’¡ API í• ë‹¹ëŸ‰ ì´ˆê³¼ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
            elif "invalid_grant" in error_msg or "bad request" in error_msg:
                print("   ğŸ’¡ API ì¸ì¦ ì˜¤ë¥˜ì…ë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
            return []
    
    def create_hnsw_index(self, documents: List[Document], embeddings: List[np.ndarray]) -> bool:
        """HNSW ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥"""
        print("\nğŸ” HNSW ì¸ë±ìŠ¤ ìƒì„±")
        print("-" * 40)
        
        try:
            if not embeddings:
                print("   âŒ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
            embeddings_array = np.array(embeddings, dtype=np.float32)
            dimension = embeddings_array.shape[1]
            
            print(f"   ğŸ“Š ì„ë² ë”© ë°°ì—´ í˜•íƒœ: {embeddings_array.shape}")
            print(f"   ğŸ”§ HNSW ì„¤ì •: M={self.hnsw_config['M']}, "
                  f"efConstruction={self.hnsw_config['efConstruction']}")
            
            # HNSW ì¸ë±ìŠ¤ ìƒì„±
            print("   ğŸ”¨ HNSW ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            index = faiss.IndexHNSWFlat(dimension, self.hnsw_config['M'])
            
            # HNSW íŒŒë¼ë¯¸í„° ì„¤ì •
            index.hnsw.efConstruction = self.hnsw_config['efConstruction']
            index.hnsw.efSearch = self.hnsw_config['efSearch']
            index.metric_type = self.hnsw_config['metric']
            
            # ë²¡í„°ë¥¼ ì¸ë±ìŠ¤ì— ì¶”ê°€
            print("   ğŸ“¥ ë²¡í„°ë¥¼ ì¸ë±ìŠ¤ì— ì¶”ê°€ ì¤‘...")
            index.add(embeddings_array)
            
            # LangChain FAISS ë˜í¼ ìƒì„±
            print("   ğŸ”— LangChain FAISS ë˜í¼ ìƒì„± ì¤‘...")
            faiss_index = FAISS(
                embedding_function=self.embeddings,
                index=index,
                docstore=self._create_docstore(documents),
                index_to_docstore_id={i: i for i in range(len(documents))}
            )
            
            # ì¸ë±ìŠ¤ ì €ì¥
            print("   ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
            faiss_index.save_local(str(self.index_dir))
            
            # ì €ì¥ëœ ì¸ë±ìŠ¤ í…ŒìŠ¤íŠ¸ ë¡œë“œ (ì•ˆì „ì„± í™•ì¸)
            print("   ğŸ” ì €ì¥ëœ ì¸ë±ìŠ¤ í…ŒìŠ¤íŠ¸ ë¡œë“œ ì¤‘...")
            test_load = FAISS.load_local(
                str(self.index_dir),
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            print("   âœ… ì¸ë±ìŠ¤ ì €ì¥ ë° í…ŒìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ")
            
            # HNSW ì •ë³´ ì¶œë ¥
            print(f"   ğŸ“Š ì´ ë²¡í„° ìˆ˜: {index.ntotal}")
            print(f"   ğŸ”§ HNSW ë…¸ë“œ ìˆ˜: {index.hnsw.levels.size()}")
            print(f"   ğŸ”§ HNSW ìµœëŒ€ ë ˆë²¨: {index.hnsw.max_level}")
            print(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.index_dir}")
            
            print("   âœ… HNSW ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"   âŒ HNSW ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _create_docstore(self, documents: List[Document]):
        """ë¬¸ì„œ ì €ì¥ì†Œ ìƒì„±"""
        from langchain.docstore.document import Document as LangChainDocument
        
        docstore = {}
        for i, doc in enumerate(documents):
            docstore[i] = LangChainDocument(
                page_content=doc.page_content,
                metadata=doc.metadata
            )
        return docstore
    
    def build_index(self) -> bool:
        """ì „ì²´ ì¸ë±ìŠ¤ êµ¬ì¶• í”„ë¡œì„¸ìŠ¤"""
        print("\nğŸš€ FAISS HNSW ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘")
        print("=" * 80)
        
        # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ
        documents = self.load_documents()
        if not documents:
            print("âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨")
            return False
        
        # 2ë‹¨ê³„: í† í¬ë‚˜ì´ì§• ë° ì²­í‚¹
        chunked_docs = self.tokenize_and_chunk(documents)
        if not chunked_docs:
            print("âŒ í† í¬ë‚˜ì´ì§• ë° ì²­í‚¹ ì‹¤íŒ¨")
            return False
        
        # 3ë‹¨ê³„: ì„ë² ë”© ìƒì„±
        embeddings = self.create_embeddings(chunked_docs)
        if not embeddings:
            print("âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return False
        
        # 4ë‹¨ê³„: HNSW ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥
        success = self.create_hnsw_index(chunked_docs, embeddings)
        
        if success:
            print("\nğŸ‰ FAISS HNSW ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
            self._print_build_statistics(chunked_docs, embeddings)
        
        return success
    
    def _print_build_statistics(self, documents: List[Document], embeddings: List[np.ndarray]):
        """êµ¬ì¶• í†µê³„ ì •ë³´ ì¶œë ¥"""
        print("\nğŸ“Š êµ¬ì¶• í†µê³„:")
        print("-" * 30)
        
        # ê¸°ë³¸ ì •ë³´
        print(f"   â€¢ ì„ë² ë”© ëª¨ë¸: {self.embedding_type}")
        print(f"   â€¢ ì¸ë±ìŠ¤ íƒ€ì…: FAISS HNSW")
        print(f"   â€¢ ì €ì¥ ìœ„ì¹˜: {self.index_dir}")
        
        # ë¬¸ì„œ ì •ë³´
        if documents:
            technical_docs = sum(1 for doc in documents if doc.metadata.get('has_technical_content', False))
            print(f"   â€¢ ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}")
            print(f"   â€¢ ê¸°ìˆ ì  ë‚´ìš© ë¬¸ì„œ: {technical_docs}")
            print(f"   â€¢ í‰ê·  ë¬¸ì„œ ê¸¸ì´: {sum(len(doc.page_content) for doc in documents) // len(documents)}ì")
        
        # ì„ë² ë”© ì •ë³´
        if embeddings:
            print(f"   â€¢ ì„ë² ë”© ì°¨ì›: {len(embeddings[0])}")
            print(f"   â€¢ ì´ ì„ë² ë”© ìˆ˜: {len(embeddings)}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„ë² ë”© íƒ€ì… ì„ íƒ
    embedding_type = "huggingface"
    if len(sys.argv) > 1:
        embedding_type = sys.argv[1].lower()
    
    if embedding_type not in ["huggingface", "gemini"]:
        print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© íƒ€ì…ì…ë‹ˆë‹¤. 'huggingface' ë˜ëŠ” 'gemini'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        return
    
    print(f"ğŸ”§ FAISS ì¸ë±ìŠ¤ êµ¬ì¶•ê¸° - ì„ë² ë”© ëª¨ë¸: {embedding_type}")
    
    # ì¸ë±ìŠ¤ êµ¬ì¶•ê¸° ì´ˆê¸°í™”
    builder = FAISSIndexBuilder(embedding_type=embedding_type)
    
    # ì¸ë±ìŠ¤ êµ¬ì¶•
    if builder.build_index():
        print("\nğŸ‰ ì¸ë±ìŠ¤ êµ¬ì¶•ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ì´ì œ 'faiss_search_engine.py'ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì¸ë±ìŠ¤ êµ¬ì¶•ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 