"""
êµìœ¡ìš© FAISS HNSW ì‹œìŠ¤í…œ
ë‹¨ê³„ë³„ë¡œ ë¬¸ì„œ ë¡œë”©, í† í¬ë‚˜ì´ì§•, ì²­í‚¹, ì„ë² ë”©, HNSW ì €ì¥, ê²€ìƒ‰ì„ êµ¬í˜„
"""

import os
import json
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import numpy as np

# FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬
import faiss

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# PDF ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
from PyPDF2 import PdfReader

print("ğŸ“ êµìœ¡ìš© FAISS HNSW ì‹œìŠ¤í…œ ì‹œì‘")
print("=" * 80)
from dotenv import load_dotenv
load_dotenv()

class EducationalFAISSSystem:
    """
    êµìœ¡ ëª©ì ì˜ FAISS HNSW ì‹œìŠ¤í…œ
    ê° ë‹¨ê³„ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì „ì²´ ê³¼ì •ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ êµ¬í˜„
    """
    
    def __init__(self, embedding_type: str = "huggingface"):
        """
        ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        Args:
            embedding_type: "huggingface" ë˜ëŠ” "gemini"
        """
        print("ğŸ”§ 1ë‹¨ê³„: ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.pdf_path = Path("DocumentsLoader/data/ê¸°ìˆ ì ì°¨íŠ¸ë¶„ì„ì´ë¡ ë°ë°©ë²•.pdf")
        self.index_dir = Path("DocumentsLoader/educational_faiss_index")
        self.index_dir.mkdir(exist_ok=True)
        
        # ì„ë² ë”© íƒ€ì… ì„¤ì •
        self.embedding_type = embedding_type
        
        # HNSW ì„¤ì • (êµìœ¡ìš©ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ì„¤ì •)
        self.hnsw_config = {
            'M': 16,  # ê° ë…¸ë“œì˜ ìµœëŒ€ ì—°ê²° ìˆ˜
            'efConstruction': 100,  # êµ¬ì¶• ì‹œ íƒìƒ‰í•  ì´ì›ƒ ìˆ˜
            'efSearch': 50,  # ê²€ìƒ‰ ì‹œ íƒìƒ‰í•  ì´ì›ƒ ìˆ˜
            'metric': faiss.METRIC_INNER_PRODUCT  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        }
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_embeddings()
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
        self._initialize_text_splitter()
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.faiss_index = None
        
        print("   âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        print("   ğŸ”„ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        if self.embedding_type == "gemini":
            try:
                # Gemini API í‚¤ í™•ì¸
                api_key = os.getenv("GOOGLE_API_KEY")
                if not api_key:
                    print("   âš ï¸ GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    print("   ğŸ’¡ HuggingFace ì„ë² ë”©ìœ¼ë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤.")
                    self._load_huggingface_embeddings()
                    return
                
                print("   ğŸ”‘ Gemini API í‚¤ í™•ì¸ë¨")
                
                # Gemini ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
                self.embeddings = GoogleGenerativeAIEmbeddings(
                    model="models/embedding-001",
                    google_api_key=api_key,
                    task_type="retrieval_query",  # ê²€ìƒ‰ ìµœì í™”
                    title="Technical Analysis Document"  # ë¬¸ì„œ ì œëª©
                )
                
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ ì—°ê²° í™•ì¸
                test_embedding = self.embeddings.embed_query("test")
                if test_embedding and len(test_embedding) > 0:
                    print("   âœ… Gemini ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                else:
                    raise Exception("ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                    
            except Exception as e:
                error_msg = str(e)
                print(f"   âš ï¸ Gemini ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {error_msg}")
                
                # ì˜¤ë¥˜ ìœ í˜•ë³„ ì•ˆë‚´ ë©”ì‹œì§€
                if "invalid_grant" in error_msg or "Bad Request" in error_msg:
                    print("   ğŸ’¡ API í‚¤ ì¸ì¦ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
                    print("      - GOOGLE_API_KEYê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
                    print("      - API í‚¤ê°€ Gemini APIì— ëŒ€í•œ ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸")
                    print("      - API í‚¤ê°€ ë§Œë£Œë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸")
                elif "timeout" in error_msg.lower():
                    print("   ğŸ’¡ ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒì…ë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                elif "quota" in error_msg.lower():
                    print("   ğŸ’¡ API í• ë‹¹ëŸ‰ ì´ˆê³¼ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
                else:
                    print("   ğŸ’¡ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ì…ë‹ˆë‹¤. HuggingFace ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                
                print("   ğŸ”„ HuggingFace ì„ë² ë”©ìœ¼ë¡œ ìë™ ì „í™˜ ì¤‘...")
                self._load_huggingface_embeddings()
        else:
            self._load_huggingface_embeddings()
    
    def _load_huggingface_embeddings(self):
        """HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        try:
            print("   ğŸ”„ HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
            
            # ì—¬ëŸ¬ ëª¨ë¸ ì˜µì…˜ ì œê³µ
            model_options = [
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                "sentence-transformers/all-MiniLM-L6-v2",
                "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
            ]
            
            for model_name in model_options:
                try:
                    print(f"   ğŸ” ëª¨ë¸ ì‹œë„ ì¤‘: {model_name}")
                    self.embeddings = HuggingFaceEmbeddings(
                        model_name=model_name,
                        model_kwargs={'device': 'cpu'},
                        encode_kwargs={'normalize_embeddings': True}
                    )
                    
                    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
                    test_embedding = self.embeddings.embed_query("test")
                    if test_embedding and len(test_embedding) > 0:
                        print(f"   âœ… HuggingFace ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
                        self.embedding_type = "huggingface"
                        return
                    else:
                        raise Exception("ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                        
                except Exception as e:
                    print(f"   âš ï¸ {model_name} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                    continue
            
            # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš°
            raise Exception("ì‚¬ìš© ê°€ëŠ¥í•œ HuggingFace ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"   âŒ HuggingFace ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            print("   ğŸ’¡ ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ì „í™˜í•˜ê±°ë‚˜ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            raise
    
    def _initialize_text_splitter(self):
        """í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # ê° ì²­í¬ì˜ ìµœëŒ€ í¬ê¸°
            chunk_overlap=150,  # ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„
            length_function=len,  # ê¸¸ì´ ì¸¡ì • í•¨ìˆ˜
            separators=["\n\n", "\n", " ", ""]  # ë¶„í•  ê¸°ì¤€ êµ¬ë¶„ì
        )
        print("   âœ… í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def step1_load_documents(self) -> List[Document]:
        """
        2ë‹¨ê³„: PDF ë¬¸ì„œ ë¡œë“œ
        PDF íŒŒì¼ì„ ì½ì–´ì„œ LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        """
        print("\nğŸ“„ 2ë‹¨ê³„: PDF ë¬¸ì„œ ë¡œë“œ")
        print("-" * 40)
        
        try:
            # PDF íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not self.pdf_path.exists():
                print(f"   âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.pdf_path}")
                return []
            
            print(f"   ğŸ“ PDF íŒŒì¼ ê²½ë¡œ: {self.pdf_path}")
            
            # PDF ì½ê¸°
            reader = PdfReader(str(self.pdf_path))
            total_pages = len(reader.pages)
            print(f"   ğŸ“Š ì´ í˜ì´ì§€ ìˆ˜: {total_pages}")
            
            # ê° í˜ì´ì§€ë¥¼ Documentë¡œ ë³€í™˜
            documents = []
            for page_num, page in enumerate(reader.pages):
                # í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = page.extract_text()
                
                # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                metadata = {
                    'source': str(self.pdf_path),
                    'page': page_num + 1,
                    'total_pages': total_pages,
                    'content_type': 'technical_analysis',
                    'language': 'ko',
                    'processing_time': datetime.now().isoformat()
                }
                
                # LangChain Document ìƒì„±
                doc = Document(
                    page_content=text,
                    metadata=metadata
                )
                documents.append(doc)
                
                print(f"   ğŸ“„ í˜ì´ì§€ {page_num + 1} ë¡œë“œ ì™„ë£Œ ({len(text)}ì)")
            
            print(f"   âœ… ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
            return documents
            
        except Exception as e:
            print(f"   âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def step2_tokenize_and_chunk(self, documents: List[Document]) -> List[Document]:
        """
        3ë‹¨ê³„: í† í¬ë‚˜ì´ì§• ë° ì²­í‚¹
        ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
        """
        print("\nâœ‚ï¸ 3ë‹¨ê³„: í† í¬ë‚˜ì´ì§• ë° ì²­í‚¹")
        print("-" * 40)
        
        try:
            print("   ğŸ”„ ë¬¸ì„œ ë¶„í•  ì¤‘...")
            
            # ë¬¸ì„œ ë¶„í• 
            split_docs = self.text_splitter.split_documents(documents)
            
            print(f"   ğŸ“Š ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(documents)}")
            print(f"   ğŸ“Š ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(split_docs)}")
            
            # ì²­í¬ë³„ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for i, doc in enumerate(split_docs):
                # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ìœ ì§€
                original_metadata = doc.metadata.copy()
                
                # ì²­í¬ ê´€ë ¨ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                doc.metadata.update({
                    'chunk_id': i,
                    'chunk_size': len(doc.page_content),
                    'chunk_processing_time': datetime.now().isoformat(),
                    'embedding_type': self.embedding_type,
                    'has_technical_content': self._check_technical_content(doc.page_content)
                })
                
                print(f"   ğŸ“„ ì²­í¬ {i+1}: {len(doc.page_content)}ì "
                      f"({'âœ…' if doc.metadata['has_technical_content'] else 'âŒ'} ê¸°ìˆ ì  ë‚´ìš©)")
            
            # í†µê³„ ì •ë³´
            avg_chunk_size = sum(len(doc.page_content) for doc in split_docs) // len(split_docs)
            technical_chunks = sum(1 for doc in split_docs if doc.metadata['has_technical_content'])
            
            print(f"   ğŸ“ˆ í‰ê·  ì²­í¬ í¬ê¸°: {avg_chunk_size}ì")
            print(f"   ğŸ” ê¸°ìˆ ì  ë‚´ìš© í¬í•¨ ì²­í¬: {technical_chunks}ê°œ")
            
            print("   âœ… í† í¬ë‚˜ì´ì§• ë° ì²­í‚¹ ì™„ë£Œ")
            return split_docs
            
        except Exception as e:
            print(f"   âŒ í† í¬ë‚˜ì´ì§• ë° ì²­í‚¹ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _check_technical_content(self, content: str) -> bool:
        """ê¸°ìˆ ì  ë¶„ì„ ë‚´ìš© í¬í•¨ ì—¬ë¶€ í™•ì¸"""
        technical_terms = [
            'RSI', 'MACD', 'ë³¼ë¦°ì €', 'ì´ë™í‰ê· ', 'ìŠ¤í† ìºìŠ¤í‹±', 'ì¼ëª©ê· í˜•í‘œ',
            'í”¼ë³´ë‚˜ì¹˜', 'ì—˜ë¦¬ì–´íŠ¸', 'ì§€ì§€ì„ ', 'ì €í•­ì„ ', 'ê±°ë˜ëŸ‰', 'ì¶”ì„¸',
            'ê³¼ë§¤ìˆ˜', 'ê³¼ë§¤ë„', 'ê³¨ë“ í¬ë¡œìŠ¤', 'ë°ë“œí¬ë¡œìŠ¤', 'ë‹¤ì´ë²„ì „ìŠ¤'
        ]
        
        content_lower = content.lower()
        return any(term.lower() in content_lower for term in technical_terms)
    
    def step3_create_embeddings(self, documents: List[Document]) -> List[np.ndarray]:
        """
        4ë‹¨ê³„: ì„ë² ë”© ìƒì„±
        ê° ë¬¸ì„œ ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        """
        print("\nğŸ”¢ 4ë‹¨ê³„: ì„ë² ë”© ìƒì„±")
        print("-" * 40)
        
        try:
            print(f"   ğŸ”„ {self.embedding_type} ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„° ìƒì„± ì¤‘...")
            
            embeddings_list = []
            failed_count = 0
            max_retries = 3
            
            for i, doc in enumerate(documents):
                # ì¬ì‹œë„ ë¡œì§
                for retry in range(max_retries):
                    try:
                        # ì„ë² ë”© ìƒì„±
                        embedding = self.embeddings.embed_query(doc.page_content)
                        
                        # ì„ë² ë”© ìœ íš¨ì„± ê²€ì‚¬
                        if embedding and len(embedding) > 0:
                            embeddings_list.append(embedding)
                            break
                        else:
                            raise Exception("ë¹ˆ ì„ë² ë”© ìƒì„±ë¨")
                            
                    except Exception as e:
                        if retry < max_retries - 1:
                            print(f"   âš ï¸ ë¬¸ì„œ {i+1} ì„ë² ë”© ì‹¤íŒ¨ (ì¬ì‹œë„ {retry+1}/{max_retries}): {str(e)}")
                            import time
                            time.sleep(2)  # ì ì‹œ ëŒ€ê¸°
                        else:
                            print(f"   âŒ ë¬¸ì„œ {i+1} ì„ë² ë”© ìµœì¢… ì‹¤íŒ¨: {str(e)}")
                            failed_count += 1
                            # ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ë²¡í„° ìƒì„± (0ìœ¼ë¡œ ì±„ì›€)
                            if embeddings_list:
                                default_dim = len(embeddings_list[0])
                                embeddings_list.append([0.0] * default_dim)
                            else:
                                # ì²« ë²ˆì§¸ ì„ë² ë”©ì´ ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ì°¨ì› ì‚¬ìš©
                                embeddings_list.append([0.0] * 384)  # ì¼ë°˜ì ì¸ ì„ë² ë”© ì°¨ì›
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % 10 == 0 or i == len(documents) - 1:
                    success_rate = ((i + 1 - failed_count) / (i + 1)) * 100
                    print(f"   ğŸ“Š ì§„í–‰ë¥ : {i + 1}/{len(documents)} ({((i + 1) / len(documents) * 100):.1f}%) - ì„±ê³µë¥ : {success_rate:.1f}%")
            
            # ì„ë² ë”© ì°¨ì› í™•ì¸
            if embeddings_list:
                dimension = len(embeddings_list[0])
                print(f"   ğŸ“ ì„ë² ë”© ì°¨ì›: {dimension}")
                print(f"   ğŸ“Š ì´ ì„ë² ë”© ìˆ˜: {len(embeddings_list)}")
                print(f"   âš ï¸ ì‹¤íŒ¨í•œ ì„ë² ë”© ìˆ˜: {failed_count}")
                
                if failed_count > 0:
                    print(f"   ğŸ’¡ {failed_count}ê°œì˜ ì„ë² ë”©ì´ ì‹¤íŒ¨í•˜ì—¬ ê¸°ë³¸ê°’(0)ìœ¼ë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                print("   âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ")
                return embeddings_list
            else:
                raise Exception("ëª¨ë“  ì„ë² ë”© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"   âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            
            # ì˜¤ë¥˜ ìœ í˜•ë³„ ì•ˆë‚´
            error_msg = str(e).lower()
            if "timeout" in error_msg:
                print("   ğŸ’¡ íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ë” ì‘ì€ ë°°ì¹˜ë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")
            elif "quota" in error_msg:
                print("   ğŸ’¡ API í• ë‹¹ëŸ‰ ì´ˆê³¼ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
            elif "invalid_grant" in error_msg or "bad request" in error_msg:
                print("   ğŸ’¡ API ì¸ì¦ ì˜¤ë¥˜ì…ë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
            return []
    
    def step4_create_hnsw_index(self, documents: List[Document], embeddings: List[np.ndarray]) -> bool:
        """
        5ë‹¨ê³„: HNSW ì¸ë±ìŠ¤ ìƒì„±
        FAISS HNSW ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ë²¡í„°ë¥¼ ì €ì¥
        """
        print("\nğŸ” 5ë‹¨ê³„: HNSW ì¸ë±ìŠ¤ ìƒì„±")
        print("-" * 40)
        
        try:
            if not embeddings:
                print("   âŒ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
            embeddings_array = np.array(embeddings, dtype=np.float32)
            dimension = embeddings_array.shape[1]
            
            print(f"   ğŸ“Š ì„ë² ë”© ë°°ì—´ í˜•íƒœ: {embeddings_array.shape}")
            print(f"   ğŸ”§ HNSW ì„¤ì •: M={self.hnsw_config['M']}, "
                  f"efConstruction={self.hnsw_config['efConstruction']}")
            
            # HNSW ì¸ë±ìŠ¤ ìƒì„±
            print("   ğŸ”¨ HNSW ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            index = faiss.IndexHNSWFlat(dimension, self.hnsw_config['M'])
            
            # HNSW íŒŒë¼ë¯¸í„° ì„¤ì •
            index.hnsw.efConstruction = self.hnsw_config['efConstruction']
            index.hnsw.efSearch = self.hnsw_config['efSearch']
            index.metric_type = self.hnsw_config['metric']
            
            # ë²¡í„°ë¥¼ ì¸ë±ìŠ¤ì— ì¶”ê°€
            print("   ğŸ“¥ ë²¡í„°ë¥¼ ì¸ë±ìŠ¤ì— ì¶”ê°€ ì¤‘...")
            index.add(embeddings_array)
            
            # LangChain FAISS ë˜í¼ ìƒì„±
            print("   ğŸ”— LangChain FAISS ë˜í¼ ìƒì„± ì¤‘...")
            self.faiss_index = FAISS(
                embedding_function=self.embeddings,
                index=index,
                docstore=self._create_docstore(documents),
                index_to_docstore_id={i: i for i in range(len(documents))}
            )
            
            # ì¸ë±ìŠ¤ ì €ì¥
            print("   ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì¤‘...")
            self.faiss_index.save_local(str(self.index_dir))
            
            # HNSW ì •ë³´ ì¶œë ¥
            print(f"   ğŸ“Š ì´ ë²¡í„° ìˆ˜: {index.ntotal}")
            print(f"   ğŸ”§ HNSW ë…¸ë“œ ìˆ˜: {index.hnsw.levels.size()}")
            print(f"   ğŸ”§ HNSW ìµœëŒ€ ë ˆë²¨: {index.hnsw.max_level}")
            print(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.index_dir}")
            
            print("   âœ… HNSW ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"   âŒ HNSW ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _create_docstore(self, documents: List[Document]):
        """ë¬¸ì„œ ì €ì¥ì†Œ ìƒì„±"""
        from langchain.docstore.document import Document as LangChainDocument
        
        docstore = {}
        for i, doc in enumerate(documents):
            docstore[i] = LangChainDocument(
                page_content=doc.page_content,
                metadata=doc.metadata
            )
        return docstore
    
    def step5_similarity_search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """
        6ë‹¨ê³„: ìœ ì‚¬ë„ ê²€ìƒ‰
        ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ HNSW ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰
        """
        print(f"\nğŸ” 6ë‹¨ê³„: ìœ ì‚¬ë„ ê²€ìƒ‰")
        print("-" * 40)
        
        if not self.faiss_index:
            print("   âŒ FAISS ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            print(f"   ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
            print(f"   ğŸ“Š ê²€ìƒ‰í•  ê²°ê³¼ ìˆ˜: {k}")
            
            # HNSW ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì„¤ì •
            if hasattr(self.faiss_index.index, 'hnsw'):
                self.faiss_index.index.hnsw.efSearch = self.hnsw_config['efSearch']
                print(f"   ğŸ”§ HNSW efSearch ì„¤ì •: {self.hnsw_config['efSearch']}")
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
            print("   ğŸ”„ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
            docs_and_scores = self.faiss_index.similarity_search_with_score(
                query, k=k
            )
            
            print(f"   âœ… {len(docs_and_scores)}ê°œ ê²°ê³¼ ë°œê²¬")
            
            # ê²°ê³¼ ì¶œë ¥
            for i, (doc, score) in enumerate(docs_and_scores, 1):
                print(f"\n   ğŸ“„ ê²°ê³¼ {i}:")
                print(f"      - ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}")
                print(f"      - í˜ì´ì§€: {doc.metadata.get('page', 'N/A')}")
                print(f"      - ì²­í¬ í¬ê¸°: {doc.metadata.get('chunk_size', 'N/A')}ì")
                print(f"      - ê¸°ìˆ ì  ë‚´ìš©: {'âœ…' if doc.metadata.get('has_technical_content', False) else 'âŒ'}")
                print(f"      - ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:100]}...")
            
            return docs_and_scores
            
        except Exception as e:
            print(f"   âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def load_existing_index(self) -> bool:
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            if (self.index_dir / "index.faiss").exists():
                print("ğŸ“‚ ê¸°ì¡´ FAISS HNSW ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
                self.faiss_index = FAISS.load_local(
                    str(self.index_dir),
                    self.embeddings
                )
                print("   âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                print("   âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
        except Exception as e:
            print(f"   âŒ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def build_complete_system(self) -> bool:
        """
        ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¶•
        ëª¨ë“  ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
        """
        print("\nğŸš€ ì „ì²´ FAISS HNSW ì‹œìŠ¤í…œ êµ¬ì¶• ì‹œì‘")
        print("=" * 80)
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
        if self.load_existing_index():
            print("âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚¬ìš© ê°€ëŠ¥")
            return True
        
        # 1ë‹¨ê³„: ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì´ë¯¸ ì™„ë£Œë¨)
        print("âœ… 1ë‹¨ê³„: ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 2ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ
        documents = self.step1_load_documents()
        if not documents:
            print("âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨")
            return False
        
        # 3ë‹¨ê³„: í† í¬ë‚˜ì´ì§• ë° ì²­í‚¹
        chunked_docs = self.step2_tokenize_and_chunk(documents)
        if not chunked_docs:
            print("âŒ í† í¬ë‚˜ì´ì§• ë° ì²­í‚¹ ì‹¤íŒ¨")
            return False
        
        # 4ë‹¨ê³„: ì„ë² ë”© ìƒì„±
        embeddings = self.step3_create_embeddings(chunked_docs)
        if not embeddings:
            print("âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return False
        
        # 5ë‹¨ê³„: HNSW ì¸ë±ìŠ¤ ìƒì„±
        success = self.step4_create_hnsw_index(chunked_docs, embeddings)
        
        if success:
            print("\nğŸ‰ ì „ì²´ FAISS HNSW ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!")
            self._print_system_statistics()
        
        return success
    
    def _print_system_statistics(self):
        """ì‹œìŠ¤í…œ í†µê³„ ì •ë³´ ì¶œë ¥"""
        if not self.faiss_index:
            return
        
        print("\nğŸ“Š ì‹œìŠ¤í…œ í†µê³„:")
        print("-" * 30)
        
        # ê¸°ë³¸ ì •ë³´
        print(f"   â€¢ ì„ë² ë”© ëª¨ë¸: {self.embedding_type}")
        print(f"   â€¢ ì¸ë±ìŠ¤ íƒ€ì…: FAISS HNSW")
        print(f"   â€¢ ì €ì¥ ìœ„ì¹˜: {self.index_dir}")
        
        # HNSW ì •ë³´
        if hasattr(self.faiss_index.index, 'hnsw'):
            print(f"   â€¢ HNSW ë…¸ë“œ ìˆ˜: {self.faiss_index.index.hnsw.levels.size()}")
            print(f"   â€¢ HNSW ìµœëŒ€ ë ˆë²¨: {self.faiss_index.index.hnsw.max_level}")
            print(f"   â€¢ HNSW efSearch: {self.faiss_index.index.hnsw.efSearch}")
        
        # ë¬¸ì„œ ì •ë³´
        docs = list(self.faiss_index.docstore.values())
        if docs:
            technical_docs = sum(1 for doc in docs if doc.metadata.get('has_technical_content', False))
            print(f"   â€¢ ì´ ë¬¸ì„œ ìˆ˜: {len(docs)}")
            print(f"   â€¢ ê¸°ìˆ ì  ë‚´ìš© ë¬¸ì„œ: {technical_docs}")
            print(f"   â€¢ í‰ê·  ë¬¸ì„œ ê¸¸ì´: {sum(len(doc.page_content) for doc in docs) // len(docs)}ì")
    
    def interactive_demo(self):
        """ëŒ€í™”í˜• ë°ëª¨"""
        print("\nğŸ“ êµìœ¡ìš© ëŒ€í™”í˜• ë°ëª¨")
        print("=" * 50)
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:")
        print("  - ê²€ìƒ‰ì–´ë§Œ ì…ë ¥: ì§ì ‘ ê²€ìƒ‰ (ì˜ˆ: 'RSI', 'ë³¼ë¦°ì €ë°´ë“œ')")
        print("  - 'search <ê²€ìƒ‰ì–´>': ëª…ì‹œì  ê²€ìƒ‰ ëª…ë ¹")
        print("  - 'stats': ì‹œìŠ¤í…œ í†µê³„")
        print("  - 'quit': ì¢…ë£Œ")
        print(f"\ní˜„ì¬ ì„ë² ë”© ëª¨ë¸: {self.embedding_type}")
        print()
        
        while True:
            try:
                command = input("ê²€ìƒ‰ ëª…ë ¹ì–´ ì…ë ¥: ").strip()
                
                if command.lower() == 'quit':
                    break
                elif command.lower() == 'stats':
                    self._print_system_statistics()
                elif command.startswith('search '):
                    # ëª…ì‹œì  search ëª…ë ¹ì–´ ì²˜ë¦¬
                    query = command[7:].strip()
                    if query:
                        results = self.step5_similarity_search(query)
                        if results:
                            self._save_search_results(results, query)
                    else:
                        print("   âŒ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                elif command:
                    # ë‹¨ìˆœ ê²€ìƒ‰ì–´ë¡œ ì²˜ë¦¬ (search ì ‘ë‘ì‚¬ ì—†ì´)
                    results = self.step5_similarity_search(command)
                    if results:
                        self._save_search_results(results, command)
                    else:
                        print("   âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    print("   âŒ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                
                print()
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ë°ëª¨ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def _save_search_results(self, results: List[Tuple[Document, float]], query: str):
        """ê²€ìƒ‰ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"educational_search_results_{timestamp}.json"
        
        try:
            search_data = {
                'query': query,
                'embedding_type': self.embedding_type,
                'timestamp': datetime.now().isoformat(),
                'total_results': len(results),
                'results': []
            }
            
            for doc, score in results:
                result_item = {
                    'score': float(score),
                    'page': doc.metadata.get('page', 'N/A'),
                    'chunk_id': doc.metadata.get('chunk_id', 'N/A'),
                    'chunk_size': doc.metadata.get('chunk_size', 'N/A'),
                    'has_technical_content': doc.metadata.get('has_technical_content', False),
                    'content': doc.page_content,
                    'metadata': doc.metadata
                }
                search_data['results'].append(result_item)
            
            output_path = Path("DocumentsLoader/search_results") / filename
            output_path.parent.mkdir(exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(search_data, f, ensure_ascii=False, indent=2)
            
            print(f"   âœ… ê²€ìƒ‰ ê²°ê³¼ ì €ì¥: {output_path}")
            
        except Exception as e:
            print(f"   âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„ë² ë”© íƒ€ì… ì„ íƒ
    embedding_type = "huggingface"
    if len(sys.argv) > 1:
        embedding_type = sys.argv[1].lower()
    
    if embedding_type not in ["huggingface", "gemini"]:
        print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© íƒ€ì…ì…ë‹ˆë‹¤. 'huggingface' ë˜ëŠ” 'gemini'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        return
    
    print(f"ğŸ“ êµìœ¡ìš© FAISS HNSW ì‹œìŠ¤í…œ - ì„ë² ë”© ëª¨ë¸: {embedding_type}")
    
    # êµìœ¡ìš© FAISS ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    educational_system = EducationalFAISSSystem(embedding_type=embedding_type)
    
    # ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¶•
    if educational_system.build_complete_system():
        print("\nğŸ‰ êµìœ¡ìš© FAISS HNSW ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!")
        
        # ëŒ€í™”í˜• ë°ëª¨ ì‹¤í–‰
        educational_system.interactive_demo()
        
    else:
        print("âŒ ì‹œìŠ¤í…œ êµ¬ì¶•ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 